{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"The 100 line AI agent that's actually useful <p>In 2024, SWE-bench &amp; SWE-agent helped kickstart the coding agent revolution.</p> <p>We now ask: What if the agent was 100x smaller, and still worked nearly as well?</p> <p><code>mini</code> is for</p> <ul> <li>Researchers who want to benchmark, fine-tune or RL without assumptions, bloat, or surprises</li> <li>Developers who like their tools like their scripts: short, sharp, and readable</li> <li>Engineers who want something trivial to sandbox &amp; to deploy anywhere</li> </ul> <p>Here's some details:</p> <ul> <li>Minimal: Just 100 lines of python (+100 total for env, model, script) \u2014 no fancy dependencies!</li> <li>Powerful: Resolves &gt;70% of GitHub issues in the SWE-bench verified benchmark (leaderboard).</li> <li>Convenient: Comes with UIs that turn this into your daily dev swiss army knife!</li> <li>Deployable: In addition to local envs, you can use docker, podman, singularity, apptainer, and more</li> <li>Cutting edge: Built by the Princeton &amp; Stanford team behind SWE-bench and SWE-agent.</li> </ul> Why use mini-SWE-agent for research? <p>SWE-agent jump-started the development of AI agents in 2024. Back then, we placed a lot of emphasis on tools and special interfaces for the agent. However, one year later, a lot of this is not needed at all to build a useful agent!</p> <p>In fact, mini-SWE-agent:</p> <ul> <li>Does not have any tools other than bash \u2014 it doesn't even use the tool-calling interface of the LMs.   This means that you can run it with literally any model.   When running in sandboxed environments you also don't need to take care of installing a single package \u2014 all it needs is bash.</li> <li>Has a completely linear history \u2014 every step of the agent just appends to the messages and that's it.   So there's no difference between the trajectory and the messages that you pass on to the LM.   Great for debugging &amp; fine-tuning.</li> <li>Executes actions with <code>subprocess.run</code> \u2014 every action is completely independent (as opposed to keeping a stateful shell session running). This makes it trivial to execute the actions in sandboxes (literally just switch out <code>subprocess.run</code> with <code>docker exec</code>) and to scale up effortlessly.   Seriously, this is a big deal, trust me.</li> </ul> <p>This makes it perfect as a baseline system and for a system that puts the language model (rather than the agent scaffold) in the middle of our attention. You can see the result on the SWE-bench (bash only) leaderboard, that evaluates the performance of different LMs with <code>mini</code>.</p> Why use mini-SWE-agent as a tool? <p>Some agents are overfitted research artifacts. Others are UI-heavy frontend monsters.</p> <p><code>mini</code> wants to be a hackable tool, not a black box.</p> <ul> <li>Simple enough to understand at a glance</li> <li>Convenient enough to use in daily workflows</li> <li>Flexible to extend</li> </ul> <p>Unlike other agents (including our own swe-agent), it is radically simpler, because it:</p> <ul> <li>Does not have any tools other than bash \u2014 it doesn't even use the tool-calling interface of the LMs.   Instead of implementing custom tools for every specific thing the agent might want to do, the focus is fully on the LM utilizing the shell to its full potential.   Want it to do something specific like opening a PR?   Just tell the LM to figure it out rather than spending time to implement it in the agent.</li> <li>Executes actions with <code>subprocess.run</code> \u2014 every action is completely independent (as opposed to keeping a stateful shell session running).   This is a big deal for the stability of the agent, trust me.</li> <li>Has a completely linear history \u2014 every step of the agent just appends to the messages that are passed to the LM in the next step and that's it.   This is great for debugging and understanding what the LM is prompted with.</li> </ul> Should I use mini-SWE-agent or swe-agent? <p>You should use <code>mini-swe-agent</code> if</p> <ul> <li>You want a quick command line tool that works locally</li> <li>You want an agent with a very simple control flow</li> <li>You want even faster, simpler &amp; more stable sandboxing &amp; benchmark evaluations</li> <li>You are doing FT or RL and don't want to overfit to a specific agent scaffold</li> </ul> <p>You should use <code>swe-agent</code> if</p> <ul> <li>You need specific tools or want to experiment with different tools</li> <li>You want to experiment with different history processors</li> <li>You want very powerful yaml configuration without touching code</li> </ul> <p>What you get with both</p> <ul> <li>Excellent performance on SWE-Bench</li> <li>A trajectory browser</li> </ul> Simple UI (<code>mini</code>)  Visual UI (<code>mini -v</code>)  Batch inference Trajectory browser Python bindings More in the docs <pre><code>agent = DefaultAgent(\n    LitellmModel(model_name=...),\n    LocalEnvironment(),\n)\nagent.run(\"Write a sudoku game\")</code></pre> <ul> <li>Quick start</li> <li><code>mini</code></li> <li>FAQ</li> <li>Global configuration</li> <li>Yaml configuration</li> </ul>"},{"location":"#continue-reading","title":"Continue reading:","text":"launch Installation &amp; Quick Start <p>Get started with mini-SWE-agent</p> flash_on Usage: Simple UI <p>Learn to use the <code>mini</code> command</p> visibility Usage: Visual UI <p>Try the visual interface with <code>mini -v</code></p> help FAQ <p>Common questions and answers</p> settings Configuration <p>Setup and customize your agent</p> fitness_center Power up <p>Start hacking the agent!</p>"},{"location":"#new-features","title":"\ud83d\udce3 New features","text":"<p>Please check the github release notes for the latest updates.</p>"},{"location":"#documentation-updates","title":"\ud83d\udce3 Documentation updates","text":"<ul> <li>Jul 27: More notes on local models</li> </ul> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"SECURITY/","title":"Security Policy","text":""},{"location":"SECURITY/#security-policy","title":"Security Policy","text":""},{"location":"SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>Please contact Kilian Lieret (kl5675@princeton.edu), John Yang (johnby@stanford.edu), Carlos E. Jimenez (carlosej@princeton.edu), and Ofir Press (ofirp@princeton.edu).</p>"},{"location":"_footer/","title":"footer","text":"bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"\u2764\ufe0f Contributing","text":"<p>We happily accept contributions!</p>"},{"location":"contributing/#areas-of-help","title":"Areas of help","text":"<ul> <li>Feedback on the <code>mini</code> and <code>mini -v</code> interfaces at this github issue or in our Slack channel.</li> <li>Documentation, examples, tutorials, etc. In particular, we're looking for<ul> <li>examples of how this library is used in the wild</li> <li>additional examples for the cookbook</li> </ul> </li> <li>Support for more models (anything where <code>litellm</code> doesn't work out of the box)</li> <li>Support for more environments &amp; deployments (e.g., run it as a github action, etc.)</li> <li>Take a look at the issues and look for issues marked <code>good-first-issue</code> or <code>help-wanted</code> (please read the guidelines below first)</li> </ul>"},{"location":"contributing/#design-architecture","title":"Design &amp; Architecture","text":"<ul> <li><code>mini-swe-agent</code> aims to stay minimalistic, hackable, and of high quality code.</li> <li>To extend features, we prefer to add a new version of the one of the four components (see cookbook), rather than making the existing components more complex.</li> <li>Components should be relatively self-contained, but if there are utilities that might be shared, add a <code>utils</code> folder (like this one). But keep it simple!</li> <li>If your component is a bit more specific, add it into an <code>extra</code> folder (like this one)</li> <li>Our target audience is anyone who doesn't shy away from modifying a bit of code (especially a run script) to get what they want.</li> <li>Therefore, not everything needs to be configurable with the config files, but it should be easy to create a run script that makes use of it.</li> <li>Many LMs write very verbose code -- please clean it up! Same goes for the tests. They should still be concise and readable.</li> <li>Please install <code>pre-commit</code> (<code>pip install pre-commit &amp;&amp; pre-commit install</code>) and run it before committing. This will enforce our style guide.</li> </ul>"},{"location":"contributing/#development-setup","title":"Development setup","text":"<p>Make sure to follow the dev setup instructions in quickstart.md.</p> <p>After that you can run <code>pytest</code> with <code>pytest -n auto</code> (this parallelizes the tests across all cores for speedup).</p> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#faq","title":"FAQ","text":""},{"location":"faq/#general","title":"General","text":"<p>Does mini-SWE-agent work on my system?</p> <p>mini-SWE-agent should work on any system that has a bash shell or uses a container runtime (e.g., docker, singularity, apptainer, etc.) to emulate one.</p> Should I use mini-SWE-agent or swe-agent? <p>You should use <code>mini-swe-agent</code> if</p> <ul> <li>You want a quick command line tool that works locally</li> <li>You want an agent with a very simple control flow</li> <li>You want even faster, simpler &amp; more stable sandboxing &amp; benchmark evaluations</li> <li>You are doing FT or RL and don't want to overfit to a specific agent scaffold</li> </ul> <p>You should use <code>swe-agent</code> if</p> <ul> <li>You need specific tools or want to experiment with different tools</li> <li>You want to experiment with different history processors</li> <li>You want very powerful yaml configuration without touching code</li> </ul> <p>What you get with both</p> <ul> <li>Excellent performance on SWE-Bench</li> <li>A trajectory browser</li> </ul> How is <code>mini</code> simpler than <code>swe-agent</code>? <p><code>mini</code> is simpler than <code>swe-agent</code> because it:</p> <ul> <li>Does not have any tools other than bash \u2014 it doesn't even use the tool-calling interface of the LMs.   This means you don't have to install anything in any environment you're running in. <code>bash</code> is all you need.</li> <li>Has a completely linear history \u2014 every step of the agent just appends to the messages and that's it.</li> <li>Executes actions with <code>subprocess.run</code> \u2014 every action is completely independent (as opposed to keeping a stateful shell session running).   This avoids so many issues, trust me.</li> </ul> What are the limitations of mini-SWE-agent? <p>mini-SWE-agent can be extended trivially in various ways, the following assumes the default setup. As reflected in the high SWE-bench scores, none of the following limitations are a problem in practice.</p> <ul> <li>No tools other than bash</li> <li>Actions are parsed from triple-backtick blocks (rather than assuming a function calling/tool calling format)</li> <li>By default, actions are executed as <code>subprocess.run</code>, i.e., every action is independent of the previous ones.   (meaning that the agent cannot change directories or export environment variables; however environment variables   can be set per-action). This avoids so many issues, trust me.</li> </ul> <p>If you want more flexibility with these items, you can use SWE-agent instead.</p> Where is global configuration stored? <p>The global configuration is stored in the <code>.env</code> file in the config directory. The location is printed when you run <code>mini --help</code>.</p> <p>The <code>.env</code> file is a simple key-value file that is read by the <code>dotenv</code> library.</p>"},{"location":"faq/#models","title":"Models","text":"<p>What models do you support?</p> <p>Currently, mini-SWE-agent supports all models that are supported by litellm or OpenRouter and we're open to extend the <code>models/</code> directory with more models should <code>litellm</code> not support them.</p> <p>How do I set the API key for a model?</p> <p>The API key can be stored either as an environment variable (note that enviroinment variables are not persistent unless you set them in your <code>~/.bashrc</code> or similar), or as a permanent key in the config file.</p> <p>To temporarily set the API key as an environment variable, you can use the following command:</p> <pre><code>export OPENAI_API_KEY=sk-test123\n</code></pre> <p>To permanently set the API key in the config file, you can use the following command:</p> <pre><code>mini-extra config set OPENAI_API_KEY sk-test123\n</code></pre> <p>Alternatively, you can directly edit the <code>.env</code> file in the config directory (the location is printed when you run <code>mini --help</code>).</p> <p>How can I set the default model?</p> <p>The default model is stored in the config/environment as <code>MSWEA_MODEL_NAME</code>. To permanently change it:</p> <pre><code>mini-extra config set MSWEA_MODEL_NAME anthropic/claude-sonnet-4-5-20250929\n</code></pre> <p>Alternatively, you can directly edit the <code>.env</code> file in the config directory (the location is printed when you run <code>mini --help</code>).</p>"},{"location":"faq/#minutia","title":"Minutia","text":"Why is not needed a running shell session such a big deal? <p>Most agents so far kept a running shell session. Every action from the agent was executed in this session. However, this is far from trivial:</p> <ol> <li>It's not obvious when a command has terminated. Essentially you're just pasting input into the shell session, and press enter\u2014but when do you stop reading output?    We've experimented with various heuristics (watching PIDs, watching for the shell to go back to the prompt, etc.) but all of them were flaky.    The <code>mini</code> agent doesn't need any of this!</li> <li>Particularly bad commands from the LM can kill the shell session. Then what?</li> <li>Interrupting a command running in a shell session can also mess up the shell itself and can in particular interfere with all the following outputs you want to extract.</li> </ol> <p><code>mini</code> is different: There is no running shell session. Every action is executed as a subprocess, that means every action is independent of the previous ones (it is literally a <code>subprocess.run</code>/<code>os.system</code>/<code>docker exec</code> call).</p> <p>This means that the agent cannot even change directories or export environment variables. But you don't need this! You can always prefix <code>cd /path/to/project</code> or <code>export FOO=bar</code> to every action (and in fact some LMs like Claude will do that even if you don't ask them to).</p> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"quickstart/","title":"Quick start","text":""},{"location":"quickstart/#quick-start","title":"Quick start","text":"<p>Installation Options</p> pipuv (isolated)pipx (isolated)From source/dev <p>Use pip to install <code>mini</code> in your current environment:</p> <pre><code>pip install mini-swe-agent\n</code></pre> <p>And try our command line interface</p> <pre><code>mini  # simple UI\nmini -v  # visual UI\nmini-extra  # extra utilities\n</code></pre> <p>Use <code>uv</code>/<code>uvx</code> (installation) to install &amp; run the <code>mini</code> agent in an isolated environment.</p> <p>Quickly install + run:</p> <pre><code>uvx mini-swe-agent  # simple UI\nuvx mini-swe-agent -v  # visual UI\nuvx --from mini-swe-agent mini-extra  # extra utilities\n</code></pre> <p>Permanently install</p> <pre><code>uv tool install mini-swe-agent\n# then\nmini  # simple UI\nmini -v  # visual UI\nmini-extra  # extra utilities\n</code></pre> <p>Use pipx (installation) to install &amp; run <code>mini</code> in an isolated environment.</p> <p>Quick install + run:</p> <pre><code># Simple UI\npipx run mini-swe-agent\n# Textual UI\npipx run mini-swe-agent -v\n# Extra utilities\npipx run --spec mini-swe-agent mini-extra\n</code></pre> <p>or for a persistent installation (recommended):</p> <pre><code>pipx install mini-swe-agent\n# then\nmini  # simple UI\nmini -v  # visual UI\nmini-extra  # extra utilities\n</code></pre> <p>If the invocation doesn't immediately work, you might need to run <code>pipx ensurepath</code>.</p> <p>For development or if you want to customize the agent:</p> <pre><code>git clone https://github.com/SWE-agent/mini-swe-agent.git\ncd mini-swe-agent\npip install -e .\n</code></pre> <p>Then run:</p> <pre><code>mini  # simple UI\nmini -v  # visual UI\nmini-extra  # extra utilities\n</code></pre> <p>Or pick a run script:</p> <pre><code>python src/minisweagent/run/hello_world.py\n</code></pre> <p>If you are planning to contribute, please also install the dev dependencies and <code>pre-commit</code> hooks:</p> <pre><code>pip install -e '.[dev]'\npip install pre-commit &amp;&amp; pre-commit install\n</code></pre> <p>To check your installation, you can run <code>pytest -n auto</code> in the root folder. This should run all tests in parallel (should take ~3min to run).</p> <p>Note that there are still some extra dependencies that are not installed by default (basically anything that is in an <code>.../extra/...</code> folder). If you truly want to get the maximal package, you can run <code>pip install -e '.[full]'</code></p> <p>Changelog</p> <p>Please see the github release notes for recent changes.</p> <p>Example Prompts</p> <p>Try mini-SWE-agent with these example prompts:</p> <ul> <li>Implement a Sudoku solver in python in the <code>sudoku</code> folder. Make sure the codebase is modular and well tested with pytest.</li> <li>Please run pytest on the current project, discover failing unittests and help me fix them. Always make sure to test the final solution.</li> <li>Help me document &amp; type my codebase by adding short docstrings and type hints.</li> </ul>"},{"location":"quickstart/#models","title":"Models","text":"<p>Models should be set up the first time you run <code>mini</code></p> <ul> <li>If you missed the setup wizard, just run <code>mini-extra config setup</code></li> <li>For more information, please check the model setup quickstart.</li> <li>If you want to use local models, please check this guide.</li> </ul> <p>Tip: Please always include the provider in the model name, e.g., <code>anthropic/claude-...</code>.</p> <p>Which model to use?</p> <p>We recommend using <code>anthropic/claude-sonnet-4-5-20250929</code> for most tasks. For openai models, we recommend using <code>openai/gpt-5</code> or <code>openai/gpt-5-mini</code>. You can check scores of different models at our SWE-bench (bash-only) leaderboard.</p>"},{"location":"advanced/control_flow/","title":"Control flow","text":""},{"location":"advanced/control_flow/#agent-control-flow","title":"Agent control flow","text":"<p>Understanding the default agent</p> <ul> <li>This guide shows the control flow of the default agent.</li> <li>After this, you're ready to remix &amp; extend mini</li> </ul> <p>The following diagram shows the control flow of the mini agent:</p> <p>And here is the code that implements it:</p> Default agent class <ul> <li>Read on GitHub</li> <li>API reference</li> </ul> <pre><code>\"\"\"Basic agent class. See https://mini-swe-agent.com/latest/advanced/control_flow/ for visual explanation.\"\"\"\n\nimport re\nimport subprocess\nfrom collections.abc import Callable\nfrom dataclasses import asdict, dataclass\n\nfrom jinja2 import StrictUndefined, Template\n\nfrom minisweagent import Environment, Model\n\n\n@dataclass\nclass AgentConfig:\n    # The default settings are the bare minimum to run the agent. Take a look at the config files for improved settings.\n    system_template: str = \"You are a helpful assistant that can do anything.\"\n    instance_template: str = (\n        \"Your task: {{task}}. Please reply with a single shell command in triple backticks. \"\n        \"To finish, the first line of the output of the shell command must be 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'.\"\n    )\n    timeout_template: str = (\n        \"The last command &lt;command&gt;{{action['action']}}&lt;/command&gt; timed out and has been killed.\\n\"\n        \"The output of the command was:\\n &lt;output&gt;\\n{{output}}\\n&lt;/output&gt;\\n\"\n        \"Please try another command and make sure to avoid those requiring interactive input.\"\n    )\n    format_error_template: str = \"Please always provide EXACTLY ONE action in triple backticks.\"\n    action_observation_template: str = \"Observation: {{output}}\"\n    step_limit: int = 0\n    cost_limit: float = 3.0\n\n\nclass NonTerminatingException(Exception):\n    \"\"\"Raised for conditions that can be handled by the agent.\"\"\"\n\n\nclass FormatError(NonTerminatingException):\n    \"\"\"Raised when the LM's output is not in the expected format.\"\"\"\n\n\nclass ExecutionTimeoutError(NonTerminatingException):\n    \"\"\"Raised when the action execution timed out.\"\"\"\n\n\nclass TerminatingException(Exception):\n    \"\"\"Raised for conditions that terminate the agent.\"\"\"\n\n\nclass Submitted(TerminatingException):\n    \"\"\"Raised when the LM declares that the agent has finished its task.\"\"\"\n\n\nclass LimitsExceeded(TerminatingException):\n    \"\"\"Raised when the agent has reached its cost or step limit.\"\"\"\n\n\nclass DefaultAgent:\n    def __init__(self, model: Model, env: Environment, *, config_class: Callable = AgentConfig, **kwargs):\n        self.config = config_class(**kwargs)\n        self.messages: list[dict] = []\n        self.model = model\n        self.env = env\n        self.extra_template_vars = {}\n\n    def render_template(self, template: str, **kwargs) -&gt; str:\n        template_vars = asdict(self.config) | self.env.get_template_vars() | self.model.get_template_vars()\n        return Template(template, undefined=StrictUndefined).render(\n            **kwargs, **template_vars, **self.extra_template_vars\n        )\n\n    def add_message(self, role: str, content: str, **kwargs):\n        self.messages.append({\"role\": role, \"content\": content, **kwargs})\n\n    def run(self, task: str, **kwargs) -&gt; tuple[str, str]:\n        \"\"\"Run step() until agent is finished. Return exit status &amp; message\"\"\"\n        self.extra_template_vars |= {\"task\": task, **kwargs}\n        self.messages = []\n        self.add_message(\"system\", self.render_template(self.config.system_template))\n        self.add_message(\"user\", self.render_template(self.config.instance_template))\n        while True:\n            try:\n                self.step()\n            except NonTerminatingException as e:\n                self.add_message(\"user\", str(e))\n            except TerminatingException as e:\n                self.add_message(\"user\", str(e))\n                return type(e).__name__, str(e)\n\n    def step(self) -&gt; dict:\n        \"\"\"Query the LM, execute the action, return the observation.\"\"\"\n        return self.get_observation(self.query())\n\n    def query(self) -&gt; dict:\n        \"\"\"Query the model and return the response.\"\"\"\n        if 0 &lt; self.config.step_limit &lt;= self.model.n_calls or 0 &lt; self.config.cost_limit &lt;= self.model.cost:\n            raise LimitsExceeded()\n        response = self.model.query(self.messages)\n        self.add_message(\"assistant\", **response)\n        return response\n\n    def get_observation(self, response: dict) -&gt; dict:\n        \"\"\"Execute the action and return the observation.\"\"\"\n        output = self.execute_action(self.parse_action(response))\n        observation = self.render_template(self.config.action_observation_template, output=output)\n        self.add_message(\"user\", observation)\n        return output\n\n    def parse_action(self, response: dict) -&gt; dict:\n        \"\"\"Parse the action from the message. Returns the action.\"\"\"\n        actions = re.findall(r\"```bash\\s*\\n(.*?)\\n```\", response[\"content\"], re.DOTALL)\n        if len(actions) == 1:\n            return {\"action\": actions[0].strip(), **response}\n        raise FormatError(self.render_template(self.config.format_error_template, actions=actions))\n\n    def execute_action(self, action: dict) -&gt; dict:\n        try:\n            output = self.env.execute(action[\"action\"])\n        except subprocess.TimeoutExpired as e:\n            output = e.output.decode(\"utf-8\", errors=\"replace\") if e.output else \"\"\n            raise ExecutionTimeoutError(\n                self.render_template(self.config.timeout_template, action=action, output=output)\n            )\n        except TimeoutError:\n            raise ExecutionTimeoutError(self.render_template(self.config.timeout_template, action=action, output=\"\"))\n        self.has_finished(output)\n        return output\n\n    def has_finished(self, output: dict[str, str]):\n        \"\"\"Raises Submitted exception with final output if the agent has finished its task.\"\"\"\n        lines = output.get(\"output\", \"\").lstrip().splitlines(keepends=True)\n        if lines and lines[0].strip() in [\"MINI_SWE_AGENT_FINAL_OUTPUT\", \"COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT\"]:\n            raise Submitted(\"\".join(lines[1:]))\n</code></pre> <p>Essentially, <code>DefaultAgent.run</code> calls <code>DefaultAgent.step</code> in a loop until the agent has finished its task.</p> <p>The <code>step</code> method is the core of the agent. It does the following:</p> <ol> <li>Queries the model for a response based on the current messages (<code>DefaultAgent.query</code>, calling <code>Model.query</code>)</li> <li>Parses the response to get the action, i.e., the shell command to execute (<code>DefaultAgent.parse_action</code>)</li> <li>Executes the action in the environment (<code>DefaultAgent.execute_action</code>, calling <code>Environment.execute</code>)</li> <li>Renders the observation message with <code>DefaultAgent.render_template</code></li> <li>Adds the observation to the messages</li> </ol> <p>The interesting bit is how we handle error conditions and the finish condition: This uses exceptions of two types: <code>TerminatingException</code> and <code>NonTerminatingException</code>.</p> <ul> <li><code>TerminatingException</code> is raised when the agent has finished its task or we hit a limit (cost, step limit, etc.)</li> <li> <p><code>NonTerminatingException</code> is raised when the agent has not finished its task, but we want to continue the loop.    In this case, all we need to do is to add a new message to the messages list, so that the LM can see the new state.    There are two typical cases that we handle this way:</p> <ol> <li><code>TimeoutError</code>: the action took too long to execute (we show partial output)</li> <li><code>FormatError</code>: the output from the LM contained zero or multiple actions (we show the error message)</li> </ol> </li> </ul> <p>The <code>DefaultAgent.run</code> method catches these exceptions and handles them by adding the corresponding message to the messages list and continuing the loop.</p> <pre><code>while True:\n    try:\n        self.step()\n    except NonTerminatingException as e:\n        self.add_message(\"user\", str(e))\n    except TerminatingException as e:\n        self.add_message(\"user\", str(e))\n        return type(e).__name__, str(e)\n</code></pre> <p>Using exceptions for the control flow is a lot easier than passing around flags and states, especially when extending or subclassing the agent.</p> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"advanced/cookbook/","title":"Python bindings","text":""},{"location":"advanced/cookbook/#cookbook","title":"Cookbook","text":"<p>Remixing &amp; extending mini</p> <ul> <li>This guide shows how to mix the different components of the <code>mini</code> agent to create your own custom version.</li> <li>You might want to first take a look at the control flow of the default agent first</li> </ul> <p>Development setup</p> <p>Make sure to follow the dev setup instructions in quickstart.md.</p> <p>We provide several different entry points to the agent, for example hello world, or the default when calling <code>mini</code>.</p> <p>Want to cook up your custom version and the config is not enough? Just follow the recipe below:</p> <ol> <li>What's the control flow you need? Pick an agent class (e.g., simplest example, with human in the loop)</li> <li>How should actions be executed? Pick an environment class (e.g., local, or docker)</li> <li>How is the LM queried? Pick a model class (e.g., litellm)</li> <li>How to invoke the agent? Bind them all together in a run script, possibly reading from a config (e.g., hello world, or <code>mini</code> entry point)</li> </ol> <p>We aim to keep all of these components very simple, but offer lots of choice between them -- enough to cover a broad range of things that you might want to do.</p> <p>You can override the default entry point by setting the <code>MSWEA_DEFAULT_RUN</code> environment variable to the import path of your run script.</p>"},{"location":"advanced/cookbook/#mix-match","title":"Mix &amp; match","text":""},{"location":"advanced/cookbook/#models","title":"Models","text":"Hello world (use automatic model selection)Hello world (Anthropic)Hello world (Litellm) <pre><code>from minisweagent.agents.default import DefaultAgent\nfrom minisweagent.models import get_model\nfrom minisweagent.environments.local import LocalEnvironment\n\nmodel_name = \"anthropic/claude-sonnet-4-5-20250929\"\n\nagent = DefaultAgent(\n    get_model(model_name=model_name),\n    LocalEnvironment(),\n)\nagent.run(task)\n</code></pre> <pre><code>from minisweagent.agents.default import DefaultAgent\nfrom minisweagent.models.anthropic_model import AnthropicModel\nfrom minisweagent.environments.local import LocalEnvironment\n\nmodel_name = \"anthropic/claude-sonnet-4-5-20250929\"\n\nagent = DefaultAgent(\n    AnthropicModel(model_name=model_name),\n    LocalEnvironment(),\n)\nagent.run(task)\n</code></pre> <pre><code>from minisweagent.agents.default import DefaultAgent\nfrom minisweagent.models.litellm_model import LitellmModel\nfrom minisweagent.environments.local import LocalEnvironment\n\nmodel_name = \"gpt-4o\"\n\nagent = DefaultAgent(\n    LitellmModel(model_name=model_name),\n    LocalEnvironment(),\n)\nagent.run(task)\n</code></pre>"},{"location":"advanced/cookbook/#environments","title":"Environments","text":"Hello world with local executionHello world with docker execution <pre><code>from minisweagent.environments.local import LocalEnvironment\n\nagent = DefaultAgent(\n    LitellmModel(model_name=model_name),\n    LocalEnvironment(),\n)\n</code></pre> <pre><code>from minisweagent.environments.docker import DockerEnvironment\n\nagent = DefaultAgent(\n    LitellmModel(model_name=model_name),\n    DockerEnvironment(),\n)\n</code></pre>"},{"location":"advanced/cookbook/#agents","title":"Agents","text":"Default agentHuman in the loopHuman in the loop (textual) <pre><code>from minisweagent.agents.default import DefaultAgent\nfrom minisweagent.models import get_model\nfrom minisweagent.environments.local import LocalEnvironment\n\nagent = DefaultAgent(\n    get_model(model_name=model_name),\n    LocalEnvironment(),\n)\n</code></pre> <pre><code>from minisweagent.agents.interactive import InteractiveAgent\nfrom minisweagent.models import get_model\nfrom minisweagent.environments.local import LocalEnvironment\n\nagent = InteractiveAgent(\n    LitellmModel(model_name=model_name),\n    LocalEnvironment(),\n)\n</code></pre> <pre><code>from minisweagent.agents.interactive_textual import TextualAgent\nfrom minisweagent.models import get_model\nfrom minisweagent.environments.local import LocalEnvironment\n\nagent = TextualAgent(\n    LitellmModel(model_name=model_name),\n    LocalEnvironment(),\n)\n</code></pre>"},{"location":"advanced/cookbook/#advanced","title":"Advanced","text":""},{"location":"advanced/cookbook/#customizing-execution","title":"Customizing execution","text":"<p>An agent that uses python function for some actions:</p> Subclassing the agentSubclassing the environment <pre><code>from minisweagent.agents.default import DefaultAgent\nimport shlex\n\ndef python_function(*args) -&gt; dict:\n    ...\n    return {\"output\": \"...\"}\n\nclass AgentWithPythonFunctions(DefaultAgent):\n    def execute_action(self, action: dict) -&gt; dict:\n        if action[\"action\"].startswith(\"python_function\"):\n            args = shlex.split(action[\"action\"].removeprefix(\"python_function\").strip())\n            return python_function(*args)\n        return super().execute_action(action)\n</code></pre> <pre><code>from minisweagent.agents.default import DefaultAgent\nimport shlex\n\ndef python_function(*args) -&gt; dict:\n    ...\n    return {\"output\": \"...\"}\n\nclass EnvironmentWithPythonFunctions(LocalEnvironment):\n    def execute(self, command: str, cwd: str = \"\") -&gt; dict:\n        if command.startswith(\"python_function\"):\n            args = shlex.split(command.removeprefix(\"python_function\").strip())\n            return python_function(*args)\n        return super().execute(command, cwd)\n\nagent = DefaultAgent(\n    LitellmModel(model_name=model_name),\n    EnvironmentWithPythonFunctions(),\n)\n</code></pre> <p>An agent that exits when the <code>submit</code> command is issued:</p> Subclassing the agentSubclassing the environment <pre><code>from minisweagent.agents.default import DefaultAgent, Submitted\n\nclass AgentQuitsOnSubmit(DefaultAgent):\n    def execute_action(self, action: dict) -&gt; dict:\n        if action[\"action\"] == \"submit\":\n            # The `Submitted` exception will be caught by the agent and\n            # the final output will be printed.\n            raise Submitted(\"The agent has finished its task.\")\n        return super().execute_action(action)\n</code></pre> <pre><code>from minisweagent.agents.default import DefaultAgent, Submitted\nfrom minisweagent.environments.local import LocalEnvironment\n\nclass EnvironmentQuitsOnSubmit(LocalEnvironment):\n    def execute(self, command: str, cwd: str = \"\") -&gt; dict:\n        if command == \"submit\":\n            raise Submitted(\"The agent has finished its task.\")\n        return super().execute(command, cwd)\n\nagent = DefaultAgent(\n    LitellmModel(model_name=model_name),\n    EnvironmentQuitsOnSubmit(),\n)\n</code></pre> <p>An agent that validates actions before execution (also an example of how to use an extended config class):</p> Subclassing the agentSubclassing the environment <pre><code>import re\nfrom dataclasses import dataclass\nfrom minisweagent.agents.default import (\n    DefaultAgent, NonTerminatingException, DefaultAgentConfig\n)\n\n@dataclass\nclass ValidatingAgentConfig(DefaultAgentConfig):\n    forbidden_patterns: list[str] = [\n        r\"rm -rf /\",\n        r\"sudo.*passwd\",\n        r\"mkfs\\.\",\n    ]\n\nclass ValidatingAgent(DefaultAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs, config_class=ValidatingAgentConfig)\n\n    def execute_action(self, action: dict) -&gt; dict:\n        for pattern in self.config.forbidden_patterns:\n            if re.search(pattern, action[\"action\"], re.IGNORECASE):\n                raise NonTerminatingException(\"Action blocked\")\n        return super().execute_action(action)\n</code></pre> <pre><code>import re\nfrom dataclasses import dataclass\nfrom minisweagent.agents.default import (\n    DefaultAgent, NonTerminatingException, DefaultAgentConfig\n)\nfrom minisweagent.environments.local import LocalEnvironment\n\n@dataclass\nclass EnvironmentWithForbiddenPatternsConfig(LocalEnvironmentConfig):\n    forbidden_patterns: list[str] = [\n        r\"rm -rf /\",\n        r\"sudo.*passwd\",\n        r\"mkfs\\.\",\n    ]\n\nclass EnvironmentWithForbiddenPatterns(LocalEnvironment):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs, config_class=EnvironmentWithForbiddenPatternsConfig)\n\n    def execute(self, command: str, cwd: str = \"\") -&gt; dict:\n        for pattern in self.config.forbidden_patterns:\n            if re.search(pattern, command, re.IGNORECASE):\n                raise NonTerminatingException(\"Action blocked\")\n        return super().execute(command, cwd)\n\nagent = DefaultAgent(\n    LitellmModel(model_name=model_name),\n    EnvironmentWithForbiddenPatterns(),\n)\n</code></pre>"},{"location":"advanced/cookbook/#running-mini-swe-agent-on-ray","title":"Running mini-swe-agent on Ray","text":"<p>This blog post describes how to parallelize mini-swe-agent runs with Ray.</p> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"advanced/environments/","title":"Environments","text":""},{"location":"advanced/environments/#environment-classes","title":"Environment classes","text":"<p>We support various environments for executing code through different backends.</p> <p>If you run <code>mini</code>, you will run in the <code>local</code> environment by default.</p> <p>However, particularly for evaluating on SWE-bench, we offer multiple environment backends that you can use.</p> <p>You can specify the environment class with the <code>--environment-class</code> flag or the <code>environment.environment_class</code> key in the agent config file.</p> <ul> <li> <p><code>local</code> (<code>LocalEnvironment</code>). Executes commands directly on the host machine using <code>subprocess.run</code>. No isolation. Directly works in your current python environment.</p> </li> <li> <p><code>docker</code> (<code>DockerEnvironment</code>). Executes commands with <code>docker exec</code>.</p> </li> <li> <p><code>singularity</code> (<code>SingularityEnvironment</code>) - Executes commands in Singularity/Apptainer containers. Good alternative to Docker in HPC environments where Docker is not available.</p> </li> </ul> <p>On top, there are a few more specialized environment classes that you can use:</p> <ul> <li> <p><code>swerex_docker</code> (<code>SwerexDockerEnvironment</code>) - Docker execution through SWE-ReX</p> </li> <li> <p><code>bubblewrap</code> (<code>BubblewrapEnvironment</code>) - Linux only. Uses bubblewrap for lightweight, unprivileged sandboxing. Experimental.</p> </li> </ul>"},{"location":"advanced/global_configuration/","title":"Global configuration","text":""},{"location":"advanced/global_configuration/#global-configuration","title":"Global configuration","text":"<p>Configuring mini</p> <ul> <li>This guide shows how to configure the <code>mini</code> agent's global settings (API keys, default model, etc.).   Basically anything that is set as environment variables or similar.</li> <li>You should already be familiar with the quickstart guide.</li> <li>For more agent specific settings, see the yaml configuration file guide.</li> </ul> <p>Setting up models</p> <p>Setting up models is also covered in the quickstart guide.</p>"},{"location":"advanced/global_configuration/#setting-global-configuration","title":"Setting global configuration","text":"<p>All global configuration can be either set as environment variables, or in the <code>.env</code> file (the exact location is printed when you run <code>mini</code>). Environment variables take precedence over variables set in the <code>.env</code> file.</p> <p>We provide several helper functions to update the global configuration.</p> <p>For example, to set the default model and API keys, you can run:</p> <pre><code>mini-extra config setup\n</code></pre> <p>or to update specific settings:</p> <pre><code>mini-extra config set KEY VALUE\n# e.g.,\nmini-extra config set MSWEA_MODEL_NAME \"anthropic/claude-sonnet-4-5-20250929\"\nmini-extra config set MSWEA_MODEL_API_KEY \"sk-...\"\n</code></pre> <p>or to unset a key:</p> <pre><code>mini-extra config unset KEY\n# e.g.,\nmini-extra config unset MSWEA_MODEL_API_KEY\n</code></pre> <p>You can also edit the <code>.env</code> file directly and we provide a helper function for that:</p> <pre><code>mini-extra config edit\n</code></pre> <p>To set environment variables (recommended for temporary experimentation or API keys):</p> <pre><code>export KEY=\"value\"\n# windows:\nsetx KEY \"value\"\n</code></pre>"},{"location":"advanced/global_configuration/#models-and-keys","title":"Models and keys","text":"<p>See also</p> <p>Read the quickstart guide first\u2014it already covers most of this.</p> <pre><code># Default model name\n# (default: not set)\nMSWEA_MODEL_NAME=\"anthropic/claude-sonnet-4-5-20250929\"\n\n# Default API key\n# (default: not set)\nMSWEA_MODEL_API_KEY=\"sk-...\"\n</code></pre> <p>To register extra models to litellm (see local models for more details), you can either specify the path in the agent file, or set</p> <pre><code>LITELLM_MODEL_REGISTRY_PATH=\"/path/to/your/model/registry.json\"\n</code></pre> <p>Global cost limits:</p> <pre><code># Global limit on number of model calls (0 = no limit)\n# (default: 0)\nMSWEA_GLOBAL_CALL_LIMIT=\"100\"\n\n# Global cost limit in dollars (0 = no limit)\n# (default: 0)\nMSWEA_GLOBAL_COST_LIMIT=\"10.00\"\n</code></pre>"},{"location":"advanced/global_configuration/#default-config-files","title":"Default config files","text":"<pre><code># Set a custom directory for agent config files in addition to the builtin ones\n# This allows to specify them by names\nMSWEA_CONFIG_DIR=\"/path/to/your/own/config/dir\"\n\n# Config path for mini run script\n# (default: package_dir / \"config\" / \"mini.yaml\")\nMSWEA_MINI_CONFIG_PATH=\"/path/to/your/own/config\"\n\n# Config path for GitHub issue script\n# (default: package_dir / \"config\" / \"github_issue.yaml\")\nMSWEA_GITHUB_CONFIG_PATH=\"/path/to/your/github/config.yaml\"\n\n# Custom style path for trajectory inspector\n# (default: package_dir / \"config\" / \"mini.tcss\")\nMSWEA_INSPECTOR_STYLE_PATH=\"/path/to/your/inspector/style.tcss\"\n\n# Custom style path for mini textual interface\n# (default: package_dir / \"config\" / \"mini.tcss\")\nMSWEA_MINI_STYLE_PATH=\"/path/to/your/mini/style.tcss\"\n</code></pre>"},{"location":"advanced/global_configuration/#settings-for-environments","title":"Settings for environments","text":"<pre><code># Path/name to the singularity/apptainer executable\n# (default: \"singularity\")\nMSWEA_SINGULARITY_EXECUTABLE=\"singularity\"\n\n# Path/name to the docker executable\n# (default: \"docker\")\nMSWEA_DOCKER_EXECUTABLE=\"docker\"\n\n# Path/name to the bubblewrap executable\n# (default: \"bwrap\")\nMSWEA_BUBBLEWRAP_EXECUTABLE=\"bwrap\"\n</code></pre>"},{"location":"advanced/global_configuration/#default-run-files","title":"Default run files","text":"<pre><code># Default run script entry point for the main CLI\n# (default: \"minisweagent.run.mini\")\nMSWEA_DEFAULT_RUN=\"minisweagent.run.mini\"\n\n# Set to true to use visual mode by default for the main CLI\n# (default: false)\nMSWEA_VISUAL_MODE_DEFAULT=\"false\"\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"advanced/yaml_configuration/","title":"Yaml config files","text":""},{"location":"advanced/yaml_configuration/#yaml-config-files","title":"Yaml config files","text":"<p>Agent configuration files</p> <ul> <li>This guide shows how to configure agent behavior using YAML configuration files.</li> <li>You should already be familiar with the quickstart guide.</li> <li>For global environment settings (API keys, default model, etc., basically anything that can be set as environment variables), see global configuration.</li> <li>Want more? See the cookbook for subclassing &amp; developing your own agent.</li> </ul>"},{"location":"advanced/yaml_configuration/#overall-structure","title":"Overall structure","text":"<p>Configuration files look like this:</p> Configuration file <pre><code>agent:\n  system_template: |\n    You are a helpful assistant that can interact with a computer.\n\n    Your response must contain exactly ONE bash code block with ONE command (or commands connected with &amp;&amp; or ||).\n    Include a THOUGHT section before your command where you explain your reasoning process.\n    Format your response as shown in &lt;format_example&gt;.\n\n    &lt;format_example&gt;\n    Your reasoning and analysis here. Explain why you want to perform the action.\n\n    ```bash\n    your_command_here\n    ```\n    &lt;/format_example&gt;\n\n    Failure to follow these rules will cause your response to be rejected.\n  instance_template: |\n    Please solve this issue: {{task}}\n\n    You can execute bash commands and edit files to implement the necessary changes.\n\n    ## Recommended Workflow\n\n    This workflows should be done step-by-step so that you can iterate on your changes and any possible problems.\n\n    1. Analyze the codebase by finding and reading relevant files\n    2. Create a script to reproduce the issue\n    3. Edit the source code to resolve the issue\n    4. Verify your fix works by running your script again\n    5. Test edge cases to ensure your fix is robust\n    6. Submit your changes and finish your work by issuing the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`.\n       Do not combine it with any other command. &lt;important&gt;After this command, you cannot continue working on this task.&lt;/important&gt;\n\n    ## Important Rules\n\n    1. Every response must contain exactly one action\n    2. The action must be enclosed in triple backticks\n    3. Directory or environment variable changes are not persistent. Every action is executed in a new subshell.\n       However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir &amp;&amp; ...` or write/load environment variables from files\n\n    &lt;system_information&gt;\n    {{system}} {{release}} {{version}} {{machine}}\n    &lt;/system_information&gt;\n\n    ## Formatting your response\n\n    Here is an example of a correct response:\n\n    &lt;example_response&gt;\n    THOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.\n\n    ```bash\n    ls -la\n    ```\n    &lt;/example_response&gt;\n\n    ## Useful command examples\n\n    ### Create a new file:\n\n    ```bash\n    cat &lt;&lt;'EOF' &gt; newfile.py\n    import numpy as np\n    hello = \"world\"\n    print(hello)\n    EOF\n    ```\n\n    ### Edit files with sed:\n\n    {%- if system == \"Darwin\" -%}\n    &lt;important&gt;\n    You are on MacOS. For all the below examples, you need to use `sed -i ''` instead of `sed -i`.\n    &lt;/important&gt;\n    {%- endif -%}\n\n    ```bash\n    # Replace all occurrences\n    sed -i 's/old_string/new_string/g' filename.py\n\n    # Replace only first occurrence\n    sed -i 's/old_string/new_string/' filename.py\n\n    # Replace first occurrence on line 1\n    sed -i '1s/old_string/new_string/' filename.py\n\n    # Replace all occurrences in lines 1-10\n    sed -i '1,10s/old_string/new_string/g' filename.py\n    ```\n\n    ### View file content:\n\n    ```bash\n    # View specific lines with numbers\n    nl -ba filename.py | sed -n '10,20p'\n    ```\n\n    ### Any other command you want to run\n\n    ```bash\n    anything\n    ```\n  action_observation_template: |\n    &lt;returncode&gt;{{output.returncode}}&lt;/returncode&gt;\n    {% if output.output | length &lt; 10000 -%}\n    &lt;output&gt;\n    {{ output.output -}}\n    &lt;/output&gt;\n    {%- else -%}\n    &lt;warning&gt;\n    The output of your last command was too long.\n    Please try a different command that produces less output.\n    If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.\n    If you're using grep or find and it produced too much output, you can use a more selective search pattern.\n    If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.\n    &lt;/warning&gt;\n    {%- set elided_chars = output.output | length - 10000 -%}\n    &lt;output_head&gt;\n    {{ output.output[:5000] }}\n    &lt;/output_head&gt;\n    &lt;elided_chars&gt;\n    {{ elided_chars }} characters elided\n    &lt;/elided_chars&gt;\n    &lt;output_tail&gt;\n    {{ output.output[-5000:] }}\n    &lt;/output_tail&gt;\n    {%- endif -%}\n  format_error_template: |\n    Please always provide EXACTLY ONE action in triple backticks, found {{actions|length}} actions.\n    If you want to end the task, please issue the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`\n    without any other command.\n    Else, please format your response exactly as follows:\n\n    &lt;response_example&gt;\n    Here are some thoughts about why you want to perform the action.\n\n    ```bash\n    &lt;action&gt;\n    ```\n    &lt;/response_example&gt;\n\n    Note: In rare cases, if you need to reference a similar format in your command, you might have\n    to proceed in two steps, first writing TRIPLEBACKTICKSBASH, then replacing them with ```bash.\n  step_limit: 0.\n  cost_limit: 3.\n  mode: confirm\nenvironment:\n  env:\n    PAGER: cat\n    MANPAGER: cat\n    LESS: -R\n    PIP_PROGRESS_BAR: 'off'\n    TQDM_DISABLE: '1'\nmodel:\n  model_kwargs:\n    temperature: 0.0\n    drop_params: true\n</code></pre> <p>We use the following top-level keys:</p> <ul> <li><code>agent</code>: Agent configuration (prompt templates, cost limits etc.)</li> <li><code>environment</code>: Environment configuration (if you want to run in a docker container, etc.)</li> <li><code>model</code>: Model configuration (model name, reasoning strength, etc.)</li> <li><code>run</code>: Run configuration (output file, etc.)</li> </ul>"},{"location":"advanced/yaml_configuration/#agent-configuration","title":"Agent configuration","text":"<p>Different agent classes might have slightly different configuration options. You can find the full list of options in the API reference.</p> <p>To use a different agent class, you can set the <code>agent_class</code> key to the name of the agent class you want to use or even to an import path (to use your own custom agent class even if it is not yet part of the mini-SWE-agent package).</p>"},{"location":"advanced/yaml_configuration/#prompt-templates","title":"Prompt templates","text":"<p>We use Jinja2 to render templates (e.g., the instance template). TL;DR: You include variables with double curly braces, e.g. <code>{{task}}</code>, but you can also do fairly complicated logic like this:</p> Example: Dealing with long observations <pre><code>&lt;returncode&gt;{{output.returncode}}&lt;/returncode&gt;\n{% if output.output | length &lt; 10000 -%}\n    &lt;output&gt;\n        {{ output.output -}}\n    &lt;/output&gt;\n{%- else -%}\n    &lt;warning&gt;\n        The output of your last command was too long.\n        Please try a different command that produces less output.\n        If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.\n        If you're using grep or find and it produced too much output, you can use a more selective search pattern.\n        If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.\n    &lt;/warning&gt;\n\n    {%- set elided_chars = output.output | length - 10000 -%}\n\n    &lt;output_head&gt;\n        {{ output.output[:5000] }}\n    &lt;/output_head&gt;\n\n    &lt;elided_chars&gt;\n        {{ elided_chars }} characters elided\n    &lt;/elided_chars&gt;\n\n    &lt;output_tail&gt;\n        {{ output.output[-5000:] }}\n    &lt;/output_tail&gt;\n{%- endif -%}\n</code></pre> <p>In all builtin agents, you can use the following variables:</p> <ul> <li>Environment variables (<code>LocalEnvironment</code> only, see discussion here)</li> <li>Agent config variables</li> <li>Environment config variables</li> <li>Explicitly passed variables (<code>observation</code>, <code>task</code> etc.) depending on the template</li> </ul>"},{"location":"advanced/yaml_configuration/#model-configuration","title":"Model configuration","text":"<p>See this guide for more details on model configuration.</p>"},{"location":"advanced/yaml_configuration/#environment-configuration","title":"Environment configuration","text":"<p>See this guide for more details on environment configuration.</p>"},{"location":"advanced/yaml_configuration/#run-configuration","title":"Run configuration","text":"<p>See the information in \"Usage\".</p> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"models/local_models/","title":"Local models","text":"<p>Local models</p> <ul> <li>This guide shows how to set up local models.</li> <li>You should already be familiar with the quickstart guide.</li> <li>You should also quickly skim the global configuration guide to understand   the global configuration and yaml configuration files guide.</li> </ul> <p>Examples</p> <ul> <li>Issue #303 has several examples of how to use local models.</li> <li>We also welcome concrete examples of how to use local models per pull request into this guide.</li> </ul>"},{"location":"models/local_models/#using-litellm","title":"Using litellm","text":"<p>Currently, all models are supported via <code>litellm</code> (but if you have specific needs, we're open to add more specific model classes in the <code>models</code> submodule).</p> <p>If you use local models, you most likely need to add some extra keywords to the <code>litellm</code> call. This is done with the <code>model_kwargs</code> dictionary which is directly passed to <code>litellm.completion</code>.</p> <p>In other words, this is how we invoke litellm:</p> <pre><code>litellm.completion(\n    model=model_name,\n    messages=messages,\n    **model_kwargs\n)\n</code></pre> <p>You can set <code>model_kwargs</code> in an agent config file like the following one:</p> Default configuration file <pre><code>agent:\n  system_template: |\n    You are a helpful assistant that can interact with a computer.\n\n    Your response must contain exactly ONE bash code block with ONE command (or commands connected with &amp;&amp; or ||).\n    Include a THOUGHT section before your command where you explain your reasoning process.\n    Format your response as shown in &lt;format_example&gt;.\n\n    &lt;format_example&gt;\n    Your reasoning and analysis here. Explain why you want to perform the action.\n\n    ```bash\n    your_command_here\n    ```\n    &lt;/format_example&gt;\n\n    Failure to follow these rules will cause your response to be rejected.\n  instance_template: |\n    Please solve this issue: {{task}}\n\n    You can execute bash commands and edit files to implement the necessary changes.\n\n    ## Recommended Workflow\n\n    This workflows should be done step-by-step so that you can iterate on your changes and any possible problems.\n\n    1. Analyze the codebase by finding and reading relevant files\n    2. Create a script to reproduce the issue\n    3. Edit the source code to resolve the issue\n    4. Verify your fix works by running your script again\n    5. Test edge cases to ensure your fix is robust\n    6. Submit your changes and finish your work by issuing the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`.\n       Do not combine it with any other command. &lt;important&gt;After this command, you cannot continue working on this task.&lt;/important&gt;\n\n    ## Important Rules\n\n    1. Every response must contain exactly one action\n    2. The action must be enclosed in triple backticks\n    3. Directory or environment variable changes are not persistent. Every action is executed in a new subshell.\n       However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir &amp;&amp; ...` or write/load environment variables from files\n\n    &lt;system_information&gt;\n    {{system}} {{release}} {{version}} {{machine}}\n    &lt;/system_information&gt;\n\n    ## Formatting your response\n\n    Here is an example of a correct response:\n\n    &lt;example_response&gt;\n    THOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.\n\n    ```bash\n    ls -la\n    ```\n    &lt;/example_response&gt;\n\n    ## Useful command examples\n\n    ### Create a new file:\n\n    ```bash\n    cat &lt;&lt;'EOF' &gt; newfile.py\n    import numpy as np\n    hello = \"world\"\n    print(hello)\n    EOF\n    ```\n\n    ### Edit files with sed:\n\n    {%- if system == \"Darwin\" -%}\n    &lt;important&gt;\n    You are on MacOS. For all the below examples, you need to use `sed -i ''` instead of `sed -i`.\n    &lt;/important&gt;\n    {%- endif -%}\n\n    ```bash\n    # Replace all occurrences\n    sed -i 's/old_string/new_string/g' filename.py\n\n    # Replace only first occurrence\n    sed -i 's/old_string/new_string/' filename.py\n\n    # Replace first occurrence on line 1\n    sed -i '1s/old_string/new_string/' filename.py\n\n    # Replace all occurrences in lines 1-10\n    sed -i '1,10s/old_string/new_string/g' filename.py\n    ```\n\n    ### View file content:\n\n    ```bash\n    # View specific lines with numbers\n    nl -ba filename.py | sed -n '10,20p'\n    ```\n\n    ### Any other command you want to run\n\n    ```bash\n    anything\n    ```\n  action_observation_template: |\n    &lt;returncode&gt;{{output.returncode}}&lt;/returncode&gt;\n    {% if output.output | length &lt; 10000 -%}\n    &lt;output&gt;\n    {{ output.output -}}\n    &lt;/output&gt;\n    {%- else -%}\n    &lt;warning&gt;\n    The output of your last command was too long.\n    Please try a different command that produces less output.\n    If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.\n    If you're using grep or find and it produced too much output, you can use a more selective search pattern.\n    If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.\n    &lt;/warning&gt;\n    {%- set elided_chars = output.output | length - 10000 -%}\n    &lt;output_head&gt;\n    {{ output.output[:5000] }}\n    &lt;/output_head&gt;\n    &lt;elided_chars&gt;\n    {{ elided_chars }} characters elided\n    &lt;/elided_chars&gt;\n    &lt;output_tail&gt;\n    {{ output.output[-5000:] }}\n    &lt;/output_tail&gt;\n    {%- endif -%}\n  format_error_template: |\n    Please always provide EXACTLY ONE action in triple backticks, found {{actions|length}} actions.\n    If you want to end the task, please issue the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`\n    without any other command.\n    Else, please format your response exactly as follows:\n\n    &lt;response_example&gt;\n    Here are some thoughts about why you want to perform the action.\n\n    ```bash\n    &lt;action&gt;\n    ```\n    &lt;/response_example&gt;\n\n    Note: In rare cases, if you need to reference a similar format in your command, you might have\n    to proceed in two steps, first writing TRIPLEBACKTICKSBASH, then replacing them with ```bash.\n  step_limit: 0.\n  cost_limit: 3.\n  mode: confirm\nenvironment:\n  env:\n    PAGER: cat\n    MANPAGER: cat\n    LESS: -R\n    PIP_PROGRESS_BAR: 'off'\n    TQDM_DISABLE: '1'\nmodel:\n  model_kwargs:\n    temperature: 0.0\n    drop_params: true\n</code></pre> <p>In the last section, you can add</p> <pre><code>model:\n  model_name: \"my-local-model\"\n  model_kwargs:\n    custom_llm_provider: \"openai\"\n    api_base: \"https://...\"\n    ...\n  ...\n</code></pre> <p>Updating the default <code>mini</code> configuration file</p> <p>You can set the <code>MSWEA_MINI_CONFIG_PATH</code> setting to set path to the default <code>mini</code> configuration file. This will allow you to override the default configuration file with your own. See the global configuration guide for more details.</p> <p>If this is not enough, our model class should be simple to modify:</p> Complete model class <ul> <li>Read on GitHub</li> <li>API reference</li> </ul> <pre><code>import json\nimport logging\nimport os\nfrom dataclasses import asdict, dataclass, field\nfrom pathlib import Path\nfrom typing import Any, Literal\n\nimport litellm\nfrom tenacity import (\n    before_sleep_log,\n    retry,\n    retry_if_not_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom minisweagent.models import GLOBAL_MODEL_STATS\nfrom minisweagent.models.utils.cache_control import set_cache_control\n\nlogger = logging.getLogger(\"litellm_model\")\n\n\n@dataclass\nclass LitellmModelConfig:\n    model_name: str\n    model_kwargs: dict[str, Any] = field(default_factory=dict)\n    litellm_model_registry: Path | str | None = os.getenv(\"LITELLM_MODEL_REGISTRY_PATH\")\n    set_cache_control: Literal[\"default_end\"] | None = None\n    \"\"\"Set explicit cache control markers, for example for Anthropic models\"\"\"\n\n\nclass LitellmModel:\n    def __init__(self, *, config_class: type = LitellmModelConfig, **kwargs):\n        self.config = config_class(**kwargs)\n        self.cost = 0.0\n        self.n_calls = 0\n        if self.config.litellm_model_registry and Path(self.config.litellm_model_registry).is_file():\n            litellm.utils.register_model(json.loads(Path(self.config.litellm_model_registry).read_text()))\n\n    @retry(\n        stop=stop_after_attempt(10),\n        wait=wait_exponential(multiplier=1, min=4, max=60),\n        before_sleep=before_sleep_log(logger, logging.WARNING),\n        retry=retry_if_not_exception_type(\n            (\n                litellm.exceptions.UnsupportedParamsError,\n                litellm.exceptions.NotFoundError,\n                litellm.exceptions.PermissionDeniedError,\n                litellm.exceptions.ContextWindowExceededError,\n                litellm.exceptions.APIError,\n                litellm.exceptions.AuthenticationError,\n                KeyboardInterrupt,\n            )\n        ),\n    )\n    def _query(self, messages: list[dict[str, str]], **kwargs):\n        try:\n            return litellm.completion(\n                model=self.config.model_name, messages=messages, **(self.config.model_kwargs | kwargs)\n            )\n        except litellm.exceptions.AuthenticationError as e:\n            e.message += \" You can permanently set your API key with `mini-extra config set KEY VALUE`.\"\n            raise e\n\n    def query(self, messages: list[dict[str, str]], **kwargs) -&gt; dict:\n        if self.config.set_cache_control:\n            messages = set_cache_control(messages, mode=self.config.set_cache_control)\n        response = self._query(messages, **kwargs)\n        try:\n            cost = litellm.cost_calculator.completion_cost(response)\n        except Exception as e:\n            logger.critical(\n                f\"Error calculating cost for model {self.config.model_name}: {e}. \"\n                \"Please check the 'Updating the model registry' section in the documentation at \"\n                \"https://klieret.short.gy/litellm-model-registry Still stuck? Please open a github issue for help!\"\n            )\n            raise\n        self.n_calls += 1\n        assert cost &gt;= 0.0, f\"Cost is negative: {cost}\"\n        self.cost += cost\n        GLOBAL_MODEL_STATS.add(cost)\n        return {\n            \"content\": response.choices[0].message.content or \"\",  # type: ignore\n            \"extra\": {\n                \"response\": response.model_dump(),\n            },\n        }\n\n    def get_template_vars(self) -&gt; dict[str, Any]:\n        return asdict(self.config) | {\"n_model_calls\": self.n_calls, \"model_cost\": self.cost}\n</code></pre> <p>The other part that you most likely need to figure out are costs. There are two ways to do this with <code>litellm</code>:</p> <ol> <li>You set up a litellm proxy server (which gives you a lot of control over all the LM calls)</li> <li>You update the model registry (next section)</li> </ol>"},{"location":"models/local_models/#updating-the-model-registry","title":"Updating the model registry","text":"<p>LiteLLM get its cost and model metadata from this file. You can override or add data from this file if it's outdated or missing your desired model by including a custom registry file.</p> <p>The model registry JSON file should follow LiteLLM's format:</p> <pre><code>{\n  \"my-custom-model\": {\n    \"max_tokens\": 4096,\n    \"input_cost_per_token\": 0.0001,\n    \"output_cost_per_token\": 0.0002,\n    \"litellm_provider\": \"openai\",\n    \"mode\": \"chat\"\n  },\n  \"my-local-model\": {\n    \"max_tokens\": 8192,\n    \"input_cost_per_token\": 0.0,\n    \"output_cost_per_token\": 0.0,\n    \"litellm_provider\": \"ollama\",\n    \"mode\": \"chat\"\n  }\n}\n</code></pre> <p>Model names</p> <p>Model names are case sensitive. Please make sure you have an exact match.</p> <p>There are two ways of setting the path to the model registry:</p> <ol> <li>Set <code>LITELLM_MODEL_REGISTRY_PATH</code> (e.g., <code>mini-extra config set LITELLM_MODEL_REGISTRY_PATH /path/to/model_registry.json</code>)</li> <li>Set <code>litellm_model_registry</code> in the agent config file</li> </ol> <pre><code>model:\n  litellm_model_registry: \"/path/to/model_registry.json\"\n  ...\n...\n</code></pre>"},{"location":"models/local_models/#concrete-examples","title":"Concrete examples","text":"<p>Help us fill this section!</p> <p>We welcome concrete examples of how to use local models per pull request into this guide. Please add your example here.</p> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"models/quickstart/","title":"Quick start","text":""},{"location":"models/quickstart/#model-setup-quickstart","title":"Model setup quickstart","text":"<p>Setup</p> <ul> <li>In most cases, you can simply run <code>mini-extra config setup</code> to set up your default model and API keys.   This should be run the first time you run <code>mini</code>.</li> <li>By default we support all models using <code>litellm</code>.</li> <li>We also offer support for models via Openrouter and Portkey.</li> </ul>"},{"location":"models/quickstart/#setting-api-keys","title":"Setting API keys","text":"<p>There are several ways to set your API keys:</p> <ul> <li>Recommended: Run our setup script: <code>mini-extra config setup</code>. This should also run automatically the first time you run <code>mini</code>.</li> <li>Use <code>mini-extra config set ANTHROPIC_API_KEY &lt;your-api-key&gt;</code> to put the key in the <code>mini</code> config file.</li> <li>Export your key as an environment variable: <code>export ANTHROPIC_API_KEY=&lt;your-api-key&gt;</code> (this is not persistent if you restart your shell, unless you add it to your shell config, like <code>~/.bashrc</code> or <code>~/.zshrc</code>).</li> <li>If you only use a single model, you can also set <code>MSWEA_MODEL_API_KEY</code> (as environment variable or in the config file). This takes precedence over all other keys.</li> <li>If you run several agents in parallel, see our note about rotating anthropic keys here.</li> </ul> All the API key names <p>We use <code>litellm</code> to support most models. Here's a list of all the API key names available in <code>litellm</code>:</p> <pre><code>ALEPH_ALPHA_API_KEY\nALEPHALPHA_API_KEY\nANTHROPIC_API_KEY\nANYSCALE_API_KEY\nAZURE_AI_API_KEY\nAZURE_API_KEY\nAZURE_OPENAI_API_KEY\nBASETEN_API_KEY\nCEREBRAS_API_KEY\nCLARIFAI_API_KEY\nCLOUDFLARE_API_KEY\nCO_API_KEY\nCODESTRAL_API_KEY\nCOHERE_API_KEY\nDATABRICKS_API_KEY\nDEEPINFRA_API_KEY\nDEEPSEEK_API_KEY\nFEATHERLESS_AI_API_KEY\nFIREWORKS_AI_API_KEY\nFIREWORKS_API_KEY\nFIREWORKSAI_API_KEY\nGEMINI_API_KEY\nGROQ_API_KEY\nHUGGINGFACE_API_KEY\nINFINITY_API_KEY\nMARITALK_API_KEY\nMISTRAL_API_KEY\nNEBIUS_API_KEY\nNLP_CLOUD_API_KEY\nNOVITA_API_KEY\nNVIDIA_NIM_API_KEY\nOLLAMA_API_KEY\nOPENAI_API_KEY\nOPENAI_LIKE_API_KEY\nOPENROUTER_API_KEY\nOR_API_KEY\nPALM_API_KEY\nPERPLEXITYAI_API_KEY\nPREDIBASE_API_KEY\nPROVIDER_API_KEY\nREPLICATE_API_KEY\nTOGETHERAI_API_KEY\nVOLCENGINE_API_KEY\nVOYAGE_API_KEY\nWATSONX_API_KEY\nWX_API_KEY\nXAI_API_KEY\nXINFERENCE_API_KEY\n</code></pre> <p>In addition, Portkey models use the <code>PORTKEY_API_KEY</code> environment variable.</p>"},{"location":"models/quickstart/#selecting-a-model","title":"Selecting a model","text":"<p>Model names and providers.</p> <p>We support most models using <code>litellm</code>. You can find a list of their supported models here. Please always include the provider in the model name, e.g., <code>anthropic/claude-...</code>.</p> <ul> <li>Recommended: <code>mini-extra config setup</code> (should be run the first time you run <code>mini</code>) can set the default model for you</li> <li>All command line interfaces allow you to set the model name with <code>-m</code> or <code>--model</code>.</li> <li>In addition, you can set the default model with <code>mini-extra config set MSWEA_MODEL_NAME &lt;model-name&gt;</code>, by editing the global config file (shortcut: <code>mini-extra config edit</code>), or by setting the <code>MSWEA_MODEL_NAME</code> environment variable.</li> <li>You can also set your model in a config file (key <code>model_name</code> under <code>model</code>).</li> <li>If you want to use local models, please check this guide.</li> </ul> <p>Popular models</p> <p>Here's a few examples of popular models:</p> <pre><code>anthropic/claude-sonnet-4-5-20250929\nopenai/gpt-5\nopenai/gpt-5-mini\ngemini/gemini-2.5-pro\ndeepseek/deepseek-chat\n</code></pre> List of all supported models <p>Here's a list of all model names supported by <code>litellm</code> as of Aug 29th 2025. For even more recent models, check the <code>model_prices_and_context_window.json</code> file from litellm.</p> <pre><code>1024-x-1024/50-steps/bedrock/amazon.nova-canvas-v1:0\n1024-x-1024/50-steps/stability.stable-diffusion-xl-v1\n1024-x-1024/dall-e-2\n1024-x-1024/max-steps/stability.stable-diffusion-xl-v1\n256-x-256/dall-e-2\n512-x-512/50-steps/stability.stable-diffusion-xl-v0\n512-x-512/dall-e-2\n512-x-512/max-steps/stability.stable-diffusion-xl-v0\nai21.j2-mid-v1\nai21.j2-ultra-v1\nai21.jamba-1-5-large-v1:0\nai21.jamba-1-5-mini-v1:0\nai21.jamba-instruct-v1:0\naiml/dall-e-2\naiml/dall-e-3\naiml/flux-pro\naiml/flux-pro/v1.1\naiml/flux-pro/v1.1-ultra\naiml/flux-realism\naiml/flux/dev\naiml/flux/kontext-max/text-to-image\naiml/flux/kontext-pro/text-to-image\naiml/flux/schnell\namazon.nova-lite-v1:0\namazon.nova-micro-v1:0\namazon.nova-pro-v1:0\namazon.rerank-v1:0\namazon.titan-embed-image-v1\namazon.titan-embed-text-v1\namazon.titan-embed-text-v2:0\namazon.titan-text-express-v1\namazon.titan-text-lite-v1\namazon.titan-text-premier-v1:0\nanthropic.claude-3-5-haiku-20241022-v1:0\nanthropic.claude-3-5-sonnet-20240620-v1:0\nanthropic.claude-3-5-sonnet-20241022-v2:0\nanthropic.claude-3-7-sonnet-20250219-v1:0\nanthropic.claude-3-haiku-20240307-v1:0\nanthropic.claude-3-opus-20240229-v1:0\nanthropic.claude-3-sonnet-20240229-v1:0\nanthropic.claude-instant-v1\nanthropic.claude-opus-4-1-20250805-v1:0\nanthropic.claude-opus-4-20250514-v1:0\nanthropic.claude-sonnet-4-20250514-v1:0\nanthropic.claude-sonnet-4-5-20250929-v1:0\nanthropic.claude-v1\nanthropic.claude-v2\nanthropic.claude-v2:1\nanyscale/HuggingFaceH4/zephyr-7b-beta\nanyscale/codellama/CodeLlama-34b-Instruct-hf\nanyscale/codellama/CodeLlama-70b-Instruct-hf\nanyscale/google/gemma-7b-it\nanyscale/meta-llama/Llama-2-13b-chat-hf\nanyscale/meta-llama/Llama-2-70b-chat-hf\nanyscale/meta-llama/Llama-2-7b-chat-hf\nanyscale/meta-llama/Meta-Llama-3-70B-Instruct\nanyscale/meta-llama/Meta-Llama-3-8B-Instruct\nanyscale/mistralai/Mistral-7B-Instruct-v0.1\nanyscale/mistralai/Mixtral-8x22B-Instruct-v0.1\nanyscale/mistralai/Mixtral-8x7B-Instruct-v0.1\napac.amazon.nova-lite-v1:0\napac.amazon.nova-micro-v1:0\napac.amazon.nova-pro-v1:0\napac.anthropic.claude-3-5-sonnet-20240620-v1:0\napac.anthropic.claude-3-5-sonnet-20241022-v2:0\napac.anthropic.claude-3-haiku-20240307-v1:0\napac.anthropic.claude-3-sonnet-20240229-v1:0\napac.anthropic.claude-sonnet-4-20250514-v1:0\napac.anthropic.claude-sonnet-4-5-20250929-v1:0\nassemblyai/best\nassemblyai/nano\nazure/ada\nazure/codex-mini\nazure/command-r-plus\nazure/computer-use-preview\nazure/eu/gpt-4o-2024-08-06\nazure/eu/gpt-4o-2024-11-20\nazure/eu/gpt-4o-mini-2024-07-18\nazure/eu/gpt-4o-mini-realtime-preview-2024-12-17\nazure/eu/gpt-4o-realtime-preview-2024-10-01\nazure/eu/gpt-4o-realtime-preview-2024-12-17\nazure/eu/o1-2024-12-17\nazure/eu/o1-mini-2024-09-12\nazure/eu/o1-preview-2024-09-12\nazure/eu/o3-mini-2025-01-31\nazure/global-standard/gpt-4o-2024-08-06\nazure/global-standard/gpt-4o-2024-11-20\nazure/global-standard/gpt-4o-mini\nazure/global/gpt-4o-2024-08-06\nazure/global/gpt-4o-2024-11-20\nazure/gpt-3.5-turbo\nazure/gpt-3.5-turbo-0125\nazure/gpt-3.5-turbo-instruct-0914\nazure/gpt-35-turbo\nazure/gpt-35-turbo-0125\nazure/gpt-35-turbo-0301\nazure/gpt-35-turbo-0613\nazure/gpt-35-turbo-1106\nazure/gpt-35-turbo-16k\nazure/gpt-35-turbo-16k-0613\nazure/gpt-35-turbo-instruct\nazure/gpt-35-turbo-instruct-0914\nazure/gpt-4\nazure/gpt-4-0125-preview\nazure/gpt-4-0613\nazure/gpt-4-1106-preview\nazure/gpt-4-32k\nazure/gpt-4-32k-0613\nazure/gpt-4-turbo\nazure/gpt-4-turbo-2024-04-09\nazure/gpt-4-turbo-vision-preview\nazure/gpt-4.1\nazure/gpt-4.1-2025-04-14\nazure/gpt-4.1-mini\nazure/gpt-4.1-mini-2025-04-14\nazure/gpt-4.1-nano\nazure/gpt-4.1-nano-2025-04-14\nazure/gpt-4.5-preview\nazure/gpt-4o\nazure/gpt-4o-2024-05-13\nazure/gpt-4o-2024-08-06\nazure/gpt-4o-2024-11-20\nazure/gpt-4o-audio-preview-2024-12-17\nazure/gpt-4o-mini\nazure/gpt-4o-mini-2024-07-18\nazure/gpt-4o-mini-audio-preview-2024-12-17\nazure/gpt-4o-mini-realtime-preview-2024-12-17\nazure/gpt-4o-mini-transcribe\nazure/gpt-4o-mini-tts\nazure/gpt-4o-realtime-preview-2024-10-01\nazure/gpt-4o-realtime-preview-2024-12-17\nazure/gpt-4o-transcribe\nazure/gpt-5\nazure/gpt-5-2025-08-07\nazure/gpt-5-chat\nazure/gpt-5-chat-latest\nazure/gpt-5-mini\nazure/gpt-5-mini-2025-08-07\nazure/gpt-5-nano\nazure/gpt-5-nano-2025-08-07\nazure/gpt-image-1\nazure/hd/1024-x-1024/dall-e-3\nazure/hd/1024-x-1792/dall-e-3\nazure/hd/1792-x-1024/dall-e-3\nazure/high/1024-x-1024/gpt-image-1\nazure/high/1024-x-1536/gpt-image-1\nazure/high/1536-x-1024/gpt-image-1\nazure/low/1024-x-1024/gpt-image-1\nazure/low/1024-x-1536/gpt-image-1\nazure/low/1536-x-1024/gpt-image-1\nazure/medium/1024-x-1024/gpt-image-1\nazure/medium/1024-x-1536/gpt-image-1\nazure/medium/1536-x-1024/gpt-image-1\nazure/mistral-large-2402\nazure/mistral-large-latest\nazure/o1\nazure/o1-2024-12-17\nazure/o1-mini\nazure/o1-mini-2024-09-12\nazure/o1-preview\nazure/o1-preview-2024-09-12\nazure/o3\nazure/o3-2025-04-16\nazure/o3-deep-research\nazure/o3-mini\nazure/o3-mini-2025-01-31\nazure/o3-pro\nazure/o3-pro-2025-06-10\nazure/o4-mini\nazure/o4-mini-2025-04-16\nazure/standard/1024-x-1024/dall-e-2\nazure/standard/1024-x-1024/dall-e-3\nazure/standard/1024-x-1792/dall-e-3\nazure/standard/1792-x-1024/dall-e-3\nazure/text-embedding-3-large\nazure/text-embedding-3-small\nazure/text-embedding-ada-002\nazure/tts-1\nazure/tts-1-hd\nazure/us/gpt-4o-2024-08-06\nazure/us/gpt-4o-2024-11-20\nazure/us/gpt-4o-mini-2024-07-18\nazure/us/gpt-4o-mini-realtime-preview-2024-12-17\nazure/us/gpt-4o-realtime-preview-2024-10-01\nazure/us/gpt-4o-realtime-preview-2024-12-17\nazure/us/o1-2024-12-17\nazure/us/o1-mini-2024-09-12\nazure/us/o1-preview-2024-09-12\nazure/us/o3-mini-2025-01-31\nazure/whisper-1\nazure_ai/Cohere-embed-v3-english\nazure_ai/Cohere-embed-v3-multilingual\nazure_ai/FLUX-1.1-pro\nazure_ai/FLUX.1-Kontext-pro\nazure_ai/Llama-3.2-11B-Vision-Instruct\nazure_ai/Llama-3.2-90B-Vision-Instruct\nazure_ai/Llama-3.3-70B-Instruct\nazure_ai/Llama-4-Maverick-17B-128E-Instruct-FP8\nazure_ai/Llama-4-Scout-17B-16E-Instruct\nazure_ai/Meta-Llama-3-70B-Instruct\nazure_ai/Meta-Llama-3.1-405B-Instruct\nazure_ai/Meta-Llama-3.1-70B-Instruct\nazure_ai/Meta-Llama-3.1-8B-Instruct\nazure_ai/Phi-3-medium-128k-instruct\nazure_ai/Phi-3-medium-4k-instruct\nazure_ai/Phi-3-mini-128k-instruct\nazure_ai/Phi-3-mini-4k-instruct\nazure_ai/Phi-3-small-128k-instruct\nazure_ai/Phi-3-small-8k-instruct\nazure_ai/Phi-3.5-MoE-instruct\nazure_ai/Phi-3.5-mini-instruct\nazure_ai/Phi-3.5-vision-instruct\nazure_ai/Phi-4\nazure_ai/Phi-4-mini-instruct\nazure_ai/Phi-4-multimodal-instruct\nazure_ai/cohere-rerank-v3-english\nazure_ai/cohere-rerank-v3-multilingual\nazure_ai/cohere-rerank-v3.5\nazure_ai/deepseek-r1\nazure_ai/deepseek-v3\nazure_ai/deepseek-v3-0324\nazure_ai/embed-v-4-0\nazure_ai/global/grok-3\nazure_ai/global/grok-3-mini\nazure_ai/grok-3\nazure_ai/grok-3-mini\nazure_ai/jais-30b-chat\nazure_ai/jamba-instruct\nazure_ai/ministral-3b\nazure_ai/mistral-large\nazure_ai/mistral-large-2407\nazure_ai/mistral-large-latest\nazure_ai/mistral-medium-2505\nazure_ai/mistral-nemo\nazure_ai/mistral-small\nazure_ai/mistral-small-2503\nbabbage-002\nbedrock/*/1-month-commitment/cohere.command-light-text-v14\nbedrock/*/1-month-commitment/cohere.command-text-v14\nbedrock/*/6-month-commitment/cohere.command-light-text-v14\nbedrock/*/6-month-commitment/cohere.command-text-v14\nbedrock/ap-northeast-1/1-month-commitment/anthropic.claude-instant-v1\nbedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v1\nbedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v2\nbedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v2:1\nbedrock/ap-northeast-1/6-month-commitment/anthropic.claude-instant-v1\nbedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v1\nbedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v2\nbedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v2:1\nbedrock/ap-northeast-1/anthropic.claude-instant-v1\nbedrock/ap-northeast-1/anthropic.claude-v1\nbedrock/ap-northeast-1/anthropic.claude-v2\nbedrock/ap-northeast-1/anthropic.claude-v2:1\nbedrock/ap-south-1/meta.llama3-70b-instruct-v1:0\nbedrock/ap-south-1/meta.llama3-8b-instruct-v1:0\nbedrock/ca-central-1/meta.llama3-70b-instruct-v1:0\nbedrock/ca-central-1/meta.llama3-8b-instruct-v1:0\nbedrock/eu-central-1/1-month-commitment/anthropic.claude-instant-v1\nbedrock/eu-central-1/1-month-commitment/anthropic.claude-v1\nbedrock/eu-central-1/1-month-commitment/anthropic.claude-v2\nbedrock/eu-central-1/1-month-commitment/anthropic.claude-v2:1\nbedrock/eu-central-1/6-month-commitment/anthropic.claude-instant-v1\nbedrock/eu-central-1/6-month-commitment/anthropic.claude-v1\nbedrock/eu-central-1/6-month-commitment/anthropic.claude-v2\nbedrock/eu-central-1/6-month-commitment/anthropic.claude-v2:1\nbedrock/eu-central-1/anthropic.claude-instant-v1\nbedrock/eu-central-1/anthropic.claude-v1\nbedrock/eu-central-1/anthropic.claude-v2\nbedrock/eu-central-1/anthropic.claude-v2:1\nbedrock/eu-west-1/meta.llama3-70b-instruct-v1:0\nbedrock/eu-west-1/meta.llama3-8b-instruct-v1:0\nbedrock/eu-west-2/meta.llama3-70b-instruct-v1:0\nbedrock/eu-west-2/meta.llama3-8b-instruct-v1:0\nbedrock/eu-west-3/mistral.mistral-7b-instruct-v0:2\nbedrock/eu-west-3/mistral.mistral-large-2402-v1:0\nbedrock/eu-west-3/mistral.mixtral-8x7b-instruct-v0:1\nbedrock/invoke/anthropic.claude-3-5-sonnet-20240620-v1:0\nbedrock/sa-east-1/meta.llama3-70b-instruct-v1:0\nbedrock/sa-east-1/meta.llama3-8b-instruct-v1:0\nbedrock/us-east-1/1-month-commitment/anthropic.claude-instant-v1\nbedrock/us-east-1/1-month-commitment/anthropic.claude-v1\nbedrock/us-east-1/1-month-commitment/anthropic.claude-v2\nbedrock/us-east-1/1-month-commitment/anthropic.claude-v2:1\nbedrock/us-east-1/6-month-commitment/anthropic.claude-instant-v1\nbedrock/us-east-1/6-month-commitment/anthropic.claude-v1\nbedrock/us-east-1/6-month-commitment/anthropic.claude-v2\nbedrock/us-east-1/6-month-commitment/anthropic.claude-v2:1\nbedrock/us-east-1/anthropic.claude-instant-v1\nbedrock/us-east-1/anthropic.claude-v1\nbedrock/us-east-1/anthropic.claude-v2\nbedrock/us-east-1/anthropic.claude-v2:1\nbedrock/us-east-1/meta.llama3-70b-instruct-v1:0\nbedrock/us-east-1/meta.llama3-8b-instruct-v1:0\nbedrock/us-east-1/mistral.mistral-7b-instruct-v0:2\nbedrock/us-east-1/mistral.mistral-large-2402-v1:0\nbedrock/us-east-1/mistral.mixtral-8x7b-instruct-v0:1\nbedrock/us-gov-east-1/amazon.nova-pro-v1:0\nbedrock/us-gov-east-1/amazon.titan-embed-text-v1\nbedrock/us-gov-east-1/amazon.titan-embed-text-v2:0\nbedrock/us-gov-east-1/amazon.titan-text-express-v1\nbedrock/us-gov-east-1/amazon.titan-text-lite-v1\nbedrock/us-gov-east-1/amazon.titan-text-premier-v1:0\nbedrock/us-gov-east-1/anthropic.claude-3-5-sonnet-20240620-v1:0\nbedrock/us-gov-east-1/anthropic.claude-3-haiku-20240307-v1:0\nbedrock/us-gov-east-1/meta.llama3-70b-instruct-v1:0\nbedrock/us-gov-east-1/meta.llama3-8b-instruct-v1:0\nbedrock/us-gov-west-1/amazon.nova-pro-v1:0\nbedrock/us-gov-west-1/amazon.titan-embed-text-v1\nbedrock/us-gov-west-1/amazon.titan-embed-text-v2:0\nbedrock/us-gov-west-1/amazon.titan-text-express-v1\nbedrock/us-gov-west-1/amazon.titan-text-lite-v1\nbedrock/us-gov-west-1/amazon.titan-text-premier-v1:0\nbedrock/us-gov-west-1/anthropic.claude-3-5-sonnet-20240620-v1:0\nbedrock/us-gov-west-1/anthropic.claude-3-haiku-20240307-v1:0\nbedrock/us-gov-west-1/meta.llama3-70b-instruct-v1:0\nbedrock/us-gov-west-1/meta.llama3-8b-instruct-v1:0\nbedrock/us-west-1/meta.llama3-70b-instruct-v1:0\nbedrock/us-west-1/meta.llama3-8b-instruct-v1:0\nbedrock/us-west-2/1-month-commitment/anthropic.claude-instant-v1\nbedrock/us-west-2/1-month-commitment/anthropic.claude-v1\nbedrock/us-west-2/1-month-commitment/anthropic.claude-v2\nbedrock/us-west-2/1-month-commitment/anthropic.claude-v2:1\nbedrock/us-west-2/6-month-commitment/anthropic.claude-instant-v1\nbedrock/us-west-2/6-month-commitment/anthropic.claude-v1\nbedrock/us-west-2/6-month-commitment/anthropic.claude-v2\nbedrock/us-west-2/6-month-commitment/anthropic.claude-v2:1\nbedrock/us-west-2/anthropic.claude-instant-v1\nbedrock/us-west-2/anthropic.claude-v1\nbedrock/us-west-2/anthropic.claude-v2\nbedrock/us-west-2/anthropic.claude-v2:1\nbedrock/us-west-2/mistral.mistral-7b-instruct-v0:2\nbedrock/us-west-2/mistral.mistral-large-2402-v1:0\nbedrock/us-west-2/mistral.mixtral-8x7b-instruct-v0:1\ncerebras/llama-3.3-70b\ncerebras/llama3.1-70b\ncerebras/llama3.1-8b\ncerebras/openai/gpt-oss-120b\ncerebras/openai/gpt-oss-20b\ncerebras/qwen-3-32b\nchat-bison\nchat-bison-32k\nchat-bison-32k@002\nchat-bison@001\nchat-bison@002\nchatdolphin\nchatgpt-4o-latest\nclaude-3-5-haiku-20241022\nclaude-3-5-haiku-latest\nclaude-3-5-sonnet-20240620\nclaude-3-5-sonnet-20241022\nclaude-3-5-sonnet-latest\nclaude-3-7-sonnet-20250219\nclaude-3-7-sonnet-latest\nclaude-3-haiku-20240307\nclaude-3-opus-20240229\nclaude-3-opus-latest\nclaude-4-opus-20250514\nclaude-4-sonnet-20250514\nclaude-opus-4-1\nclaude-opus-4-1-20250805\nclaude-opus-4-20250514\nclaude-sonnet-4-20250514\nclaude-sonnet-4-5-20250929\ncloudflare/@cf/meta/llama-2-7b-chat-fp16\ncloudflare/@cf/meta/llama-2-7b-chat-int8\ncloudflare/@cf/mistral/mistral-7b-instruct-v0.1\ncloudflare/@hf/thebloke/codellama-7b-instruct-awq\ncode-bison\ncode-bison-32k@002\ncode-bison32k\ncode-bison@001\ncode-bison@002\ncode-gecko\ncode-gecko-latest\ncode-gecko@001\ncode-gecko@002\ncodechat-bison\ncodechat-bison-32k\ncodechat-bison-32k@002\ncodechat-bison@001\ncodechat-bison@002\ncodechat-bison@latest\ncodestral/codestral-2405\ncodestral/codestral-latest\ncodex-mini-latest\ncohere.command-light-text-v14\ncohere.command-r-plus-v1:0\ncohere.command-r-v1:0\ncohere.command-text-v14\ncohere.embed-english-v3\ncohere.embed-multilingual-v3\ncohere.rerank-v3-5:0\ncommand\ncommand-a-03-2025\ncommand-light\ncommand-nightly\ncommand-r\ncommand-r-08-2024\ncommand-r-plus\ncommand-r-plus-08-2024\ncommand-r7b-12-2024\ncomputer-use-preview\ndashscope/qwen-max\ndashscope/qwen-plus-latest\ndashscope/qwen-turbo-latest\ndashscope/qwen3-30b-a3b\ndatabricks/databricks-bge-large-en\ndatabricks/databricks-claude-3-7-sonnet\ndatabricks/databricks-gte-large-en\ndatabricks/databricks-llama-2-70b-chat\ndatabricks/databricks-llama-4-maverick\ndatabricks/databricks-meta-llama-3-1-405b-instruct\ndatabricks/databricks-meta-llama-3-3-70b-instruct\ndatabricks/databricks-meta-llama-3-70b-instruct\ndatabricks/databricks-mixtral-8x7b-instruct\ndatabricks/databricks-mpt-30b-instruct\ndatabricks/databricks-mpt-7b-instruct\ndavinci-002\ndeepgram/base\ndeepgram/base-conversationalai\ndeepgram/base-finance\ndeepgram/base-general\ndeepgram/base-meeting\ndeepgram/base-phonecall\ndeepgram/base-video\ndeepgram/base-voicemail\ndeepgram/enhanced\ndeepgram/enhanced-finance\ndeepgram/enhanced-general\ndeepgram/enhanced-meeting\ndeepgram/enhanced-phonecall\ndeepgram/nova\ndeepgram/nova-2\ndeepgram/nova-2-atc\ndeepgram/nova-2-automotive\ndeepgram/nova-2-conversationalai\ndeepgram/nova-2-drivethru\ndeepgram/nova-2-finance\ndeepgram/nova-2-general\ndeepgram/nova-2-meeting\ndeepgram/nova-2-phonecall\ndeepgram/nova-2-video\ndeepgram/nova-2-voicemail\ndeepgram/nova-3\ndeepgram/nova-3-general\ndeepgram/nova-3-medical\ndeepgram/nova-general\ndeepgram/nova-phonecall\ndeepgram/whisper\ndeepgram/whisper-base\ndeepgram/whisper-large\ndeepgram/whisper-medium\ndeepgram/whisper-small\ndeepgram/whisper-tiny\ndeepinfra/Austism/chronos-hermes-13b-v2\ndeepinfra/Gryphe/MythoMax-L2-13b\ndeepinfra/Gryphe/MythoMax-L2-13b-turbo\ndeepinfra/KoboldAI/LLaMA2-13B-Tiefighter\ndeepinfra/NousResearch/Hermes-3-Llama-3.1-405B\ndeepinfra/NousResearch/Hermes-3-Llama-3.1-70B\ndeepinfra/NovaSky-AI/Sky-T1-32B-Preview\ndeepinfra/Phind/Phind-CodeLlama-34B-v2\ndeepinfra/Qwen/QVQ-72B-Preview\ndeepinfra/Qwen/QwQ-32B\ndeepinfra/Qwen/QwQ-32B-Preview\ndeepinfra/Qwen/Qwen2-72B-Instruct\ndeepinfra/Qwen/Qwen2-7B-Instruct\ndeepinfra/Qwen/Qwen2.5-72B-Instruct\ndeepinfra/Qwen/Qwen2.5-7B-Instruct\ndeepinfra/Qwen/Qwen2.5-Coder-32B-Instruct\ndeepinfra/Qwen/Qwen2.5-Coder-7B\ndeepinfra/Qwen/Qwen2.5-VL-32B-Instruct\ndeepinfra/Qwen/Qwen3-14B\ndeepinfra/Qwen/Qwen3-235B-A22B\ndeepinfra/Qwen/Qwen3-235B-A22B-Instruct-2507\ndeepinfra/Qwen/Qwen3-235B-A22B-Thinking-2507\ndeepinfra/Qwen/Qwen3-30B-A3B\ndeepinfra/Qwen/Qwen3-32B\ndeepinfra/Qwen/Qwen3-Coder-480B-A35B-Instruct\ndeepinfra/Qwen/Qwen3-Coder-480B-A35B-Instruct-Turbo\ndeepinfra/Sao10K/L3-70B-Euryale-v2.1\ndeepinfra/Sao10K/L3-8B-Lunaris-v1\ndeepinfra/Sao10K/L3-8B-Lunaris-v1-Turbo\ndeepinfra/Sao10K/L3.1-70B-Euryale-v2.2\ndeepinfra/Sao10K/L3.3-70B-Euryale-v2.3\ndeepinfra/allenai/olmOCR-7B-0725-FP8\ndeepinfra/anthropic/claude-3-7-sonnet-latest\ndeepinfra/anthropic/claude-4-opus\ndeepinfra/anthropic/claude-4-sonnet\ndeepinfra/bigcode/starcoder2-15b-instruct-v0.1\ndeepinfra/cognitivecomputations/dolphin-2.6-mixtral-8x7b\ndeepinfra/cognitivecomputations/dolphin-2.9.1-llama-3-70b\ndeepinfra/deepinfra/airoboros-70b\ndeepinfra/deepseek-ai/DeepSeek-Prover-V2-671B\ndeepinfra/deepseek-ai/DeepSeek-R1\ndeepinfra/deepseek-ai/DeepSeek-R1-0528\ndeepinfra/deepseek-ai/DeepSeek-R1-0528-Turbo\ndeepinfra/deepseek-ai/DeepSeek-R1-Distill-Llama-70B\ndeepinfra/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\ndeepinfra/deepseek-ai/DeepSeek-R1-Turbo\ndeepinfra/deepseek-ai/DeepSeek-V3\ndeepinfra/deepseek-ai/DeepSeek-V3-0324\ndeepinfra/deepseek-ai/DeepSeek-V3-0324-Turbo\ndeepinfra/deepseek-ai/DeepSeek-V3.1\ndeepinfra/google/codegemma-7b-it\ndeepinfra/google/gemini-1.5-flash\ndeepinfra/google/gemini-1.5-flash-8b\ndeepinfra/google/gemini-2.0-flash-001\ndeepinfra/google/gemini-2.5-flash\ndeepinfra/google/gemini-2.5-pro\ndeepinfra/google/gemma-1.1-7b-it\ndeepinfra/google/gemma-2-27b-it\ndeepinfra/google/gemma-2-9b-it\ndeepinfra/google/gemma-3-12b-it\ndeepinfra/google/gemma-3-27b-it\ndeepinfra/google/gemma-3-4b-it\ndeepinfra/lizpreciatior/lzlv_70b_fp16_hf\ndeepinfra/mattshumer/Reflection-Llama-3.1-70B\ndeepinfra/meta-llama/Llama-2-13b-chat-hf\ndeepinfra/meta-llama/Llama-2-70b-chat-hf\ndeepinfra/meta-llama/Llama-3.2-11B-Vision-Instruct\ndeepinfra/meta-llama/Llama-3.2-1B-Instruct\ndeepinfra/meta-llama/Llama-3.2-3B-Instruct\ndeepinfra/meta-llama/Llama-3.2-90B-Vision-Instruct\ndeepinfra/meta-llama/Llama-3.3-70B-Instruct\ndeepinfra/meta-llama/Llama-3.3-70B-Instruct-Turbo\ndeepinfra/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\ndeepinfra/meta-llama/Llama-4-Maverick-17B-128E-Instruct-Turbo\ndeepinfra/meta-llama/Llama-4-Scout-17B-16E-Instruct\ndeepinfra/meta-llama/Llama-Guard-3-8B\ndeepinfra/meta-llama/Llama-Guard-4-12B\ndeepinfra/meta-llama/Meta-Llama-3-70B-Instruct\ndeepinfra/meta-llama/Meta-Llama-3-8B-Instruct\ndeepinfra/meta-llama/Meta-Llama-3.1-405B-Instruct\ndeepinfra/meta-llama/Meta-Llama-3.1-70B-Instruct\ndeepinfra/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\ndeepinfra/meta-llama/Meta-Llama-3.1-8B-Instruct\ndeepinfra/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\ndeepinfra/microsoft/Phi-3-medium-4k-instruct\ndeepinfra/microsoft/Phi-4-multimodal-instruct\ndeepinfra/microsoft/WizardLM-2-7B\ndeepinfra/microsoft/WizardLM-2-8x22B\ndeepinfra/microsoft/phi-4\ndeepinfra/microsoft/phi-4-reasoning-plus\ndeepinfra/mistralai/Devstral-Small-2505\ndeepinfra/mistralai/Devstral-Small-2507\ndeepinfra/mistralai/Mistral-7B-Instruct-v0.1\ndeepinfra/mistralai/Mistral-7B-Instruct-v0.2\ndeepinfra/mistralai/Mistral-7B-Instruct-v0.3\ndeepinfra/mistralai/Mistral-Nemo-Instruct-2407\ndeepinfra/mistralai/Mistral-Small-24B-Instruct-2501\ndeepinfra/mistralai/Mistral-Small-3.1-24B-Instruct-2503\ndeepinfra/mistralai/Mistral-Small-3.2-24B-Instruct-2506\ndeepinfra/mistralai/Mixtral-8x22B-Instruct-v0.1\ndeepinfra/mistralai/Mixtral-8x7B-Instruct-v0.1\ndeepinfra/moonshotai/Kimi-K2-Instruct\ndeepinfra/nvidia/Llama-3.1-Nemotron-70B-Instruct\ndeepinfra/nvidia/Nemotron-4-340B-Instruct\ndeepinfra/openai/gpt-oss-120b\ndeepinfra/openai/gpt-oss-20b\ndeepinfra/openbmb/MiniCPM-Llama3-V-2_5\ndeepinfra/openchat/openchat-3.6-8b\ndeepinfra/openchat/openchat_3.5\ndeepinfra/zai-org/GLM-4.5\ndeepinfra/zai-org/GLM-4.5-Air\ndeepseek/deepseek-chat\ndeepseek/deepseek-coder\ndeepseek/deepseek-r1\ndeepseek/deepseek-reasoner\ndeepseek/deepseek-v3\ndolphin\nelevenlabs/scribe_v1\nelevenlabs/scribe_v1_experimental\nembed-english-light-v2.0\nembed-english-light-v3.0\nembed-english-v2.0\nembed-english-v3.0\nembed-multilingual-v2.0\nembed-multilingual-v3.0\neu.amazon.nova-lite-v1:0\neu.amazon.nova-micro-v1:0\neu.amazon.nova-pro-v1:0\neu.anthropic.claude-3-5-haiku-20241022-v1:0\neu.anthropic.claude-3-5-sonnet-20240620-v1:0\neu.anthropic.claude-3-5-sonnet-20241022-v2:0\neu.anthropic.claude-3-7-sonnet-20250219-v1:0\neu.anthropic.claude-3-haiku-20240307-v1:0\neu.anthropic.claude-3-opus-20240229-v1:0\neu.anthropic.claude-3-sonnet-20240229-v1:0\neu.anthropic.claude-opus-4-1-20250805-v1:0\neu.anthropic.claude-opus-4-20250514-v1:0\neu.anthropic.claude-sonnet-4-20250514-v1:0\neu.anthropic.claude-sonnet-4-5-20250929-v1:0\neu.meta.llama3-2-1b-instruct-v1:0\neu.meta.llama3-2-3b-instruct-v1:0\neu.mistral.pixtral-large-2502-v1:0\nfeatherless_ai/featherless-ai/Qwerky-72B\nfeatherless_ai/featherless-ai/Qwerky-QwQ-32B\nfireworks-ai-4.1b-to-16b\nfireworks-ai-56b-to-176b\nfireworks-ai-above-16b\nfireworks-ai-default\nfireworks-ai-embedding-150m-to-350m\nfireworks-ai-embedding-up-to-150m\nfireworks-ai-moe-up-to-56b\nfireworks-ai-up-to-4b\nfireworks_ai/WhereIsAI/UAE-Large-V1\nfireworks_ai/accounts/fireworks/models/deepseek-coder-v2-instruct\nfireworks_ai/accounts/fireworks/models/deepseek-r1\nfireworks_ai/accounts/fireworks/models/deepseek-r1-0528\nfireworks_ai/accounts/fireworks/models/deepseek-r1-basic\nfireworks_ai/accounts/fireworks/models/deepseek-v3\nfireworks_ai/accounts/fireworks/models/deepseek-v3-0324\nfireworks_ai/accounts/fireworks/models/deepseek-v3p1\nfireworks_ai/accounts/fireworks/models/firefunction-v2\nfireworks_ai/accounts/fireworks/models/glm-4p5\nfireworks_ai/accounts/fireworks/models/glm-4p5-air\nfireworks_ai/accounts/fireworks/models/gpt-oss-120b\nfireworks_ai/accounts/fireworks/models/gpt-oss-20b\nfireworks_ai/accounts/fireworks/models/kimi-k2-instruct\nfireworks_ai/accounts/fireworks/models/llama-v3p1-405b-instruct\nfireworks_ai/accounts/fireworks/models/llama-v3p1-8b-instruct\nfireworks_ai/accounts/fireworks/models/llama-v3p2-11b-vision-instruct\nfireworks_ai/accounts/fireworks/models/llama-v3p2-1b-instruct\nfireworks_ai/accounts/fireworks/models/llama-v3p2-3b-instruct\nfireworks_ai/accounts/fireworks/models/llama-v3p2-90b-vision-instruct\nfireworks_ai/accounts/fireworks/models/llama4-maverick-instruct-basic\nfireworks_ai/accounts/fireworks/models/llama4-scout-instruct-basic\nfireworks_ai/accounts/fireworks/models/mixtral-8x22b-instruct-hf\nfireworks_ai/accounts/fireworks/models/qwen2-72b-instruct\nfireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct\nfireworks_ai/accounts/fireworks/models/yi-large\nfireworks_ai/nomic-ai/nomic-embed-text-v1\nfireworks_ai/nomic-ai/nomic-embed-text-v1.5\nfireworks_ai/thenlper/gte-base\nfireworks_ai/thenlper/gte-large\nfriendliai/meta-llama-3.1-70b-instruct\nfriendliai/meta-llama-3.1-8b-instruct\nft:babbage-002\nft:davinci-002\nft:gpt-3.5-turbo\nft:gpt-3.5-turbo-0125\nft:gpt-3.5-turbo-0613\nft:gpt-3.5-turbo-1106\nft:gpt-4-0613\nft:gpt-4o-2024-08-06\nft:gpt-4o-2024-11-20\nft:gpt-4o-mini-2024-07-18\ngemini-1.0-pro\ngemini-1.0-pro-001\ngemini-1.0-pro-002\ngemini-1.0-pro-vision\ngemini-1.0-pro-vision-001\ngemini-1.0-ultra\ngemini-1.0-ultra-001\ngemini-1.5-flash\ngemini-1.5-flash-001\ngemini-1.5-flash-002\ngemini-1.5-flash-exp-0827\ngemini-1.5-flash-preview-0514\ngemini-1.5-pro\ngemini-1.5-pro-001\ngemini-1.5-pro-002\ngemini-1.5-pro-preview-0215\ngemini-1.5-pro-preview-0409\ngemini-1.5-pro-preview-0514\ngemini-2.0-flash\ngemini-2.0-flash-001\ngemini-2.0-flash-exp\ngemini-2.0-flash-lite\ngemini-2.0-flash-lite-001\ngemini-2.0-flash-live-preview-04-09\ngemini-2.0-flash-preview-image-generation\ngemini-2.0-flash-thinking-exp\ngemini-2.0-flash-thinking-exp-01-21\ngemini-2.0-pro-exp-02-05\ngemini-2.5-flash\ngemini-2.5-flash-image-preview\ngemini-2.5-flash-lite\ngemini-2.5-flash-lite-preview-06-17\ngemini-2.5-flash-preview-04-17\ngemini-2.5-flash-preview-05-20\ngemini-2.5-pro\ngemini-2.5-pro-exp-03-25\ngemini-2.5-pro-preview-03-25\ngemini-2.5-pro-preview-05-06\ngemini-2.5-pro-preview-06-05\ngemini-2.5-pro-preview-tts\ngemini-embedding-001\ngemini-flash-experimental\ngemini-pro\ngemini-pro-experimental\ngemini-pro-vision\ngemini/gemini-1.5-flash\ngemini/gemini-1.5-flash-001\ngemini/gemini-1.5-flash-002\ngemini/gemini-1.5-flash-8b\ngemini/gemini-1.5-flash-8b-exp-0827\ngemini/gemini-1.5-flash-8b-exp-0924\ngemini/gemini-1.5-flash-exp-0827\ngemini/gemini-1.5-flash-latest\ngemini/gemini-1.5-pro\ngemini/gemini-1.5-pro-001\ngemini/gemini-1.5-pro-002\ngemini/gemini-1.5-pro-exp-0801\ngemini/gemini-1.5-pro-exp-0827\ngemini/gemini-1.5-pro-latest\ngemini/gemini-2.0-flash\ngemini/gemini-2.0-flash-001\ngemini/gemini-2.0-flash-exp\ngemini/gemini-2.0-flash-lite\ngemini/gemini-2.0-flash-lite-preview-02-05\ngemini/gemini-2.0-flash-live-001\ngemini/gemini-2.0-flash-preview-image-generation\ngemini/gemini-2.0-flash-thinking-exp\ngemini/gemini-2.0-flash-thinking-exp-01-21\ngemini/gemini-2.0-pro-exp-02-05\ngemini/gemini-2.5-flash\ngemini/gemini-2.5-flash-image-preview\ngemini/gemini-2.5-flash-lite\ngemini/gemini-2.5-flash-lite-preview-06-17\ngemini/gemini-2.5-flash-preview-04-17\ngemini/gemini-2.5-flash-preview-05-20\ngemini/gemini-2.5-flash-preview-tts\ngemini/gemini-2.5-pro\ngemini/gemini-2.5-pro-exp-03-25\ngemini/gemini-2.5-pro-preview-03-25\ngemini/gemini-2.5-pro-preview-05-06\ngemini/gemini-2.5-pro-preview-06-05\ngemini/gemini-2.5-pro-preview-tts\ngemini/gemini-exp-1114\ngemini/gemini-exp-1206\ngemini/gemini-gemma-2-27b-it\ngemini/gemini-gemma-2-9b-it\ngemini/gemini-pro\ngemini/gemini-pro-vision\ngemini/gemma-3-27b-it\ngemini/imagen-3.0-fast-generate-001\ngemini/imagen-3.0-generate-001\ngemini/imagen-3.0-generate-002\ngemini/imagen-4.0-fast-generate-001\ngemini/imagen-4.0-generate-001\ngemini/imagen-4.0-ultra-generate-001\ngemini/learnlm-1.5-pro-experimental\ngpt-3.5-turbo\ngpt-3.5-turbo-0125\ngpt-3.5-turbo-0301\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-1106\ngpt-3.5-turbo-16k\ngpt-3.5-turbo-16k-0613\ngpt-3.5-turbo-instruct\ngpt-3.5-turbo-instruct-0914\ngpt-4\ngpt-4-0125-preview\ngpt-4-0314\ngpt-4-0613\ngpt-4-1106-preview\ngpt-4-1106-vision-preview\ngpt-4-32k\ngpt-4-32k-0314\ngpt-4-32k-0613\ngpt-4-turbo\ngpt-4-turbo-2024-04-09\ngpt-4-turbo-preview\ngpt-4-vision-preview\ngpt-4.1\ngpt-4.1-2025-04-14\ngpt-4.1-mini\ngpt-4.1-mini-2025-04-14\ngpt-4.1-nano\ngpt-4.1-nano-2025-04-14\ngpt-4.5-preview\ngpt-4.5-preview-2025-02-27\ngpt-4o\ngpt-4o-2024-05-13\ngpt-4o-2024-08-06\ngpt-4o-2024-11-20\ngpt-4o-audio-preview\ngpt-4o-audio-preview-2024-10-01\ngpt-4o-audio-preview-2024-12-17\ngpt-4o-audio-preview-2025-06-03\ngpt-4o-mini\ngpt-4o-mini-2024-07-18\ngpt-4o-mini-audio-preview\ngpt-4o-mini-audio-preview-2024-12-17\ngpt-4o-mini-realtime-preview\ngpt-4o-mini-realtime-preview-2024-12-17\ngpt-4o-mini-search-preview\ngpt-4o-mini-search-preview-2025-03-11\ngpt-4o-mini-transcribe\ngpt-4o-mini-tts\ngpt-4o-realtime-preview\ngpt-4o-realtime-preview-2024-10-01\ngpt-4o-realtime-preview-2024-12-17\ngpt-4o-realtime-preview-2025-06-03\ngpt-4o-search-preview\ngpt-4o-search-preview-2025-03-11\ngpt-4o-transcribe\ngpt-5\ngpt-5-2025-08-07\ngpt-5-chat\ngpt-5-chat-latest\ngpt-5-mini\ngpt-5-mini-2025-08-07\ngpt-5-nano\ngpt-5-nano-2025-08-07\ngpt-image-1\ngradient_ai/alibaba-qwen3-32b\ngradient_ai/anthropic-claude-3-opus\ngradient_ai/anthropic-claude-3.5-haiku\ngradient_ai/anthropic-claude-3.5-sonnet\ngradient_ai/anthropic-claude-3.7-sonnet\ngradient_ai/deepseek-r1-distill-llama-70b\ngradient_ai/llama3-8b-instruct\ngradient_ai/llama3.3-70b-instruct\ngradient_ai/mistral-nemo-instruct-2407\ngradient_ai/openai-gpt-4o\ngradient_ai/openai-gpt-4o-mini\ngradient_ai/openai-o3\ngradient_ai/openai-o3-mini\ngroq/deepseek-r1-distill-llama-70b\ngroq/distil-whisper-large-v3-en\ngroq/gemma-7b-it\ngroq/gemma2-9b-it\ngroq/llama-3.1-405b-reasoning\ngroq/llama-3.1-70b-versatile\ngroq/llama-3.1-8b-instant\ngroq/llama-3.2-11b-text-preview\ngroq/llama-3.2-11b-vision-preview\ngroq/llama-3.2-1b-preview\ngroq/llama-3.2-3b-preview\ngroq/llama-3.2-90b-text-preview\ngroq/llama-3.2-90b-vision-preview\ngroq/llama-3.3-70b-specdec\ngroq/llama-3.3-70b-versatile\ngroq/llama-guard-3-8b\ngroq/llama2-70b-4096\ngroq/llama3-70b-8192\ngroq/llama3-8b-8192\ngroq/llama3-groq-70b-8192-tool-use-preview\ngroq/llama3-groq-8b-8192-tool-use-preview\ngroq/meta-llama/llama-4-maverick-17b-128e-instruct\ngroq/meta-llama/llama-4-scout-17b-16e-instruct\ngroq/mistral-saba-24b\ngroq/mixtral-8x7b-32768\ngroq/moonshotai/kimi-k2-instruct\ngroq/openai/gpt-oss-120b\ngroq/openai/gpt-oss-20b\ngroq/playai-tts\ngroq/qwen/qwen3-32b\ngroq/whisper-large-v3\ngroq/whisper-large-v3-turbo\nhd/1024-x-1024/dall-e-3\nhd/1024-x-1792/dall-e-3\nhd/1792-x-1024/dall-e-3\nhigh/1024-x-1024/gpt-image-1\nhigh/1024-x-1536/gpt-image-1\nhigh/1536-x-1024/gpt-image-1\nhyperbolic/NousResearch/Hermes-3-Llama-3.1-70B\nhyperbolic/Qwen/QwQ-32B\nhyperbolic/Qwen/Qwen2.5-72B-Instruct\nhyperbolic/Qwen/Qwen2.5-Coder-32B-Instruct\nhyperbolic/Qwen/Qwen3-235B-A22B\nhyperbolic/deepseek-ai/DeepSeek-R1\nhyperbolic/deepseek-ai/DeepSeek-R1-0528\nhyperbolic/deepseek-ai/DeepSeek-V3\nhyperbolic/deepseek-ai/DeepSeek-V3-0324\nhyperbolic/meta-llama/Llama-3.2-3B-Instruct\nhyperbolic/meta-llama/Llama-3.3-70B-Instruct\nhyperbolic/meta-llama/Meta-Llama-3-70B-Instruct\nhyperbolic/meta-llama/Meta-Llama-3.1-405B-Instruct\nhyperbolic/meta-llama/Meta-Llama-3.1-70B-Instruct\nhyperbolic/meta-llama/Meta-Llama-3.1-8B-Instruct\nhyperbolic/moonshotai/Kimi-K2-Instruct\nj2-light\nj2-mid\nj2-ultra\njamba-1.5\njamba-1.5-large\njamba-1.5-large@001\njamba-1.5-mini\njamba-1.5-mini@001\njamba-large-1.6\njamba-large-1.7\njamba-mini-1.6\njamba-mini-1.7\njina-reranker-v2-base-multilingual\nlambda_ai/deepseek-llama3.3-70b\nlambda_ai/deepseek-r1-0528\nlambda_ai/deepseek-r1-671b\nlambda_ai/deepseek-v3-0324\nlambda_ai/hermes3-405b\nlambda_ai/hermes3-70b\nlambda_ai/hermes3-8b\nlambda_ai/lfm-40b\nlambda_ai/lfm-7b\nlambda_ai/llama-4-maverick-17b-128e-instruct-fp8\nlambda_ai/llama-4-scout-17b-16e-instruct\nlambda_ai/llama3.1-405b-instruct-fp8\nlambda_ai/llama3.1-70b-instruct-fp8\nlambda_ai/llama3.1-8b-instruct\nlambda_ai/llama3.1-nemotron-70b-instruct-fp8\nlambda_ai/llama3.2-11b-vision-instruct\nlambda_ai/llama3.2-3b-instruct\nlambda_ai/llama3.3-70b-instruct-fp8\nlambda_ai/qwen25-coder-32b-instruct\nlambda_ai/qwen3-32b-fp8\nlow/1024-x-1024/gpt-image-1\nlow/1024-x-1536/gpt-image-1\nlow/1536-x-1024/gpt-image-1\nluminous-base\nluminous-base-control\nluminous-extended\nluminous-extended-control\nluminous-supreme\nluminous-supreme-control\nmax-x-max/50-steps/stability.stable-diffusion-xl-v0\nmax-x-max/max-steps/stability.stable-diffusion-xl-v0\nmedium/1024-x-1024/gpt-image-1\nmedium/1024-x-1536/gpt-image-1\nmedium/1536-x-1024/gpt-image-1\nmedlm-large\nmedlm-medium\nmeta.llama2-13b-chat-v1\nmeta.llama2-70b-chat-v1\nmeta.llama3-1-405b-instruct-v1:0\nmeta.llama3-1-70b-instruct-v1:0\nmeta.llama3-1-8b-instruct-v1:0\nmeta.llama3-2-11b-instruct-v1:0\nmeta.llama3-2-1b-instruct-v1:0\nmeta.llama3-2-3b-instruct-v1:0\nmeta.llama3-2-90b-instruct-v1:0\nmeta.llama3-3-70b-instruct-v1:0\nmeta.llama3-70b-instruct-v1:0\nmeta.llama3-8b-instruct-v1:0\nmeta.llama4-maverick-17b-instruct-v1:0\nmeta.llama4-scout-17b-instruct-v1:0\nmeta_llama/Llama-3.3-70B-Instruct\nmeta_llama/Llama-3.3-8B-Instruct\nmeta_llama/Llama-4-Maverick-17B-128E-Instruct-FP8\nmeta_llama/Llama-4-Scout-17B-16E-Instruct-FP8\nmistral.mistral-7b-instruct-v0:2\nmistral.mistral-large-2402-v1:0\nmistral.mistral-large-2407-v1:0\nmistral.mistral-small-2402-v1:0\nmistral.mixtral-8x7b-instruct-v0:1\nmistral/codestral-2405\nmistral/codestral-latest\nmistral/codestral-mamba-latest\nmistral/devstral-medium-2507\nmistral/devstral-small-2505\nmistral/devstral-small-2507\nmistral/magistral-medium-2506\nmistral/magistral-medium-latest\nmistral/magistral-small-2506\nmistral/magistral-small-latest\nmistral/mistral-embed\nmistral/mistral-large-2402\nmistral/mistral-large-2407\nmistral/mistral-large-2411\nmistral/mistral-large-latest\nmistral/mistral-medium\nmistral/mistral-medium-2312\nmistral/mistral-medium-2505\nmistral/mistral-medium-latest\nmistral/mistral-small\nmistral/mistral-small-latest\nmistral/mistral-tiny\nmistral/open-codestral-mamba\nmistral/open-mistral-7b\nmistral/open-mistral-nemo\nmistral/open-mistral-nemo-2407\nmistral/open-mixtral-8x22b\nmistral/open-mixtral-8x7b\nmistral/pixtral-12b-2409\nmistral/pixtral-large-2411\nmistral/pixtral-large-latest\nmoonshot/kimi-k2-0711-preview\nmoonshot/kimi-latest\nmoonshot/kimi-latest-128k\nmoonshot/kimi-latest-32k\nmoonshot/kimi-latest-8k\nmoonshot/kimi-thinking-preview\nmoonshot/moonshot-v1-128k\nmoonshot/moonshot-v1-128k-0430\nmoonshot/moonshot-v1-128k-vision-preview\nmoonshot/moonshot-v1-32k\nmoonshot/moonshot-v1-32k-0430\nmoonshot/moonshot-v1-32k-vision-preview\nmoonshot/moonshot-v1-8k\nmoonshot/moonshot-v1-8k-0430\nmoonshot/moonshot-v1-8k-vision-preview\nmoonshot/moonshot-v1-auto\nmorph/morph-v3-fast\nmorph/morph-v3-large\nmultimodalembedding\nmultimodalembedding@001\nnscale/Qwen/QwQ-32B\nnscale/Qwen/Qwen2.5-Coder-32B-Instruct\nnscale/Qwen/Qwen2.5-Coder-3B-Instruct\nnscale/Qwen/Qwen2.5-Coder-7B-Instruct\nnscale/black-forest-labs/FLUX.1-schnell\nnscale/deepseek-ai/DeepSeek-R1-Distill-Llama-70B\nnscale/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\nnscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\nnscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\nnscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\nnscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\nnscale/meta-llama/Llama-3.1-8B-Instruct\nnscale/meta-llama/Llama-3.3-70B-Instruct\nnscale/meta-llama/Llama-4-Scout-17B-16E-Instruct\nnscale/mistralai/mixtral-8x22b-instruct-v0.1\nnscale/stabilityai/stable-diffusion-xl-base-1.0\no1\no1-2024-12-17\no1-mini\no1-mini-2024-09-12\no1-preview\no1-preview-2024-09-12\no1-pro\no1-pro-2025-03-19\no3\no3-2025-04-16\no3-deep-research\no3-deep-research-2025-06-26\no3-mini\no3-mini-2025-01-31\no3-pro\no3-pro-2025-06-10\no4-mini\no4-mini-2025-04-16\no4-mini-deep-research\no4-mini-deep-research-2025-06-26\noci/meta.llama-3.1-405b-instruct\noci/meta.llama-3.2-90b-vision-instruct\noci/meta.llama-3.3-70b-instruct\noci/meta.llama-4-maverick-17b-128e-instruct-fp8\noci/meta.llama-4-scout-17b-16e-instruct\noci/xai.grok-3\noci/xai.grok-3-fast\noci/xai.grok-3-mini\noci/xai.grok-3-mini-fast\noci/xai.grok-4\nollama/codegeex4\nollama/codegemma\nollama/codellama\nollama/deepseek-coder-v2-base\nollama/deepseek-coder-v2-instruct\nollama/deepseek-coder-v2-lite-base\nollama/deepseek-coder-v2-lite-instruct\nollama/internlm2_5-20b-chat\nollama/llama2\nollama/llama2-uncensored\nollama/llama2:13b\nollama/llama2:70b\nollama/llama2:7b\nollama/llama3\nollama/llama3.1\nollama/llama3:70b\nollama/llama3:8b\nollama/mistral\nollama/mistral-7B-Instruct-v0.1\nollama/mistral-7B-Instruct-v0.2\nollama/mistral-large-instruct-2407\nollama/mixtral-8x22B-Instruct-v0.1\nollama/mixtral-8x7B-Instruct-v0.1\nollama/orca-mini\nollama/vicuna\nomni-moderation-2024-09-26\nomni-moderation-latest\nomni-moderation-latest-intents\nopenai.gpt-oss-120b-1:0\nopenai.gpt-oss-20b-1:0\nopenrouter/anthropic/claude-2\nopenrouter/anthropic/claude-3-5-haiku\nopenrouter/anthropic/claude-3-5-haiku-20241022\nopenrouter/anthropic/claude-3-haiku\nopenrouter/anthropic/claude-3-haiku-20240307\nopenrouter/anthropic/claude-3-opus\nopenrouter/anthropic/claude-3-sonnet\nopenrouter/anthropic/claude-3.5-sonnet\nopenrouter/anthropic/claude-3.5-sonnet:beta\nopenrouter/anthropic/claude-3.7-sonnet\nopenrouter/anthropic/claude-3.7-sonnet:beta\nopenrouter/anthropic/claude-instant-v1\nopenrouter/anthropic/claude-opus-4\nopenrouter/anthropic/claude-opus-4.1\nopenrouter/anthropic/claude-sonnet-4\nopenrouter/bytedance/ui-tars-1.5-7b\nopenrouter/cognitivecomputations/dolphin-mixtral-8x7b\nopenrouter/cohere/command-r-plus\nopenrouter/databricks/dbrx-instruct\nopenrouter/deepseek/deepseek-chat\nopenrouter/deepseek/deepseek-chat-v3-0324\nopenrouter/deepseek/deepseek-chat-v3.1\nopenrouter/deepseek/deepseek-coder\nopenrouter/deepseek/deepseek-r1\nopenrouter/deepseek/deepseek-r1-0528\nopenrouter/fireworks/firellava-13b\nopenrouter/google/gemini-2.0-flash-001\nopenrouter/google/gemini-2.5-flash\nopenrouter/google/gemini-2.5-pro\nopenrouter/google/gemini-pro-1.5\nopenrouter/google/gemini-pro-vision\nopenrouter/google/palm-2-chat-bison\nopenrouter/google/palm-2-codechat-bison\nopenrouter/gryphe/mythomax-l2-13b\nopenrouter/jondurbin/airoboros-l2-70b-2.1\nopenrouter/mancer/weaver\nopenrouter/meta-llama/codellama-34b-instruct\nopenrouter/meta-llama/llama-2-13b-chat\nopenrouter/meta-llama/llama-2-70b-chat\nopenrouter/meta-llama/llama-3-70b-instruct\nopenrouter/meta-llama/llama-3-70b-instruct:nitro\nopenrouter/meta-llama/llama-3-8b-instruct:extended\nopenrouter/meta-llama/llama-3-8b-instruct:free\nopenrouter/microsoft/wizardlm-2-8x22b:nitro\nopenrouter/mistralai/mistral-7b-instruct\nopenrouter/mistralai/mistral-7b-instruct:free\nopenrouter/mistralai/mistral-large\nopenrouter/mistralai/mistral-small-3.1-24b-instruct\nopenrouter/mistralai/mistral-small-3.2-24b-instruct\nopenrouter/mistralai/mixtral-8x22b-instruct\nopenrouter/nousresearch/nous-hermes-llama2-13b\nopenrouter/openai/gpt-3.5-turbo\nopenrouter/openai/gpt-3.5-turbo-16k\nopenrouter/openai/gpt-4\nopenrouter/openai/gpt-4-vision-preview\nopenrouter/openai/gpt-4o\nopenrouter/openai/gpt-4o-2024-05-13\nopenrouter/openai/gpt-5-chat\nopenrouter/openai/gpt-5-mini\nopenrouter/openai/gpt-5-nano\nopenrouter/openai/gpt-oss-120b\nopenrouter/openai/gpt-oss-20b\nopenrouter/openai/o1\nopenrouter/openai/o1-mini\nopenrouter/openai/o1-mini-2024-09-12\nopenrouter/openai/o1-preview\nopenrouter/openai/o1-preview-2024-09-12\nopenrouter/openai/o3-mini\nopenrouter/openai/o3-mini-high\nopenrouter/pygmalionai/mythalion-13b\nopenrouter/qwen/qwen-2.5-coder-32b-instruct\nopenrouter/qwen/qwen-vl-plus\nopenrouter/qwen/qwen3-coder\nopenrouter/switchpoint/router\nopenrouter/undi95/remm-slerp-l2-13b\nopenrouter/x-ai/grok-4\npalm/chat-bison\npalm/chat-bison-001\npalm/text-bison\npalm/text-bison-001\npalm/text-bison-safety-off\npalm/text-bison-safety-recitation-off\nperplexity/codellama-34b-instruct\nperplexity/codellama-70b-instruct\nperplexity/llama-2-70b-chat\nperplexity/llama-3.1-70b-instruct\nperplexity/llama-3.1-8b-instruct\nperplexity/llama-3.1-sonar-huge-128k-online\nperplexity/llama-3.1-sonar-large-128k-chat\nperplexity/llama-3.1-sonar-large-128k-online\nperplexity/llama-3.1-sonar-small-128k-chat\nperplexity/llama-3.1-sonar-small-128k-online\nperplexity/mistral-7b-instruct\nperplexity/mixtral-8x7b-instruct\nperplexity/pplx-70b-chat\nperplexity/pplx-70b-online\nperplexity/pplx-7b-chat\nperplexity/pplx-7b-online\nperplexity/sonar\nperplexity/sonar-deep-research\nperplexity/sonar-medium-chat\nperplexity/sonar-medium-online\nperplexity/sonar-pro\nperplexity/sonar-reasoning\nperplexity/sonar-reasoning-pro\nperplexity/sonar-small-chat\nperplexity/sonar-small-online\nrecraft/recraftv2\nrecraft/recraftv3\nreplicate/meta/llama-2-13b\nreplicate/meta/llama-2-13b-chat\nreplicate/meta/llama-2-70b\nreplicate/meta/llama-2-70b-chat\nreplicate/meta/llama-2-7b\nreplicate/meta/llama-2-7b-chat\nreplicate/meta/llama-3-70b\nreplicate/meta/llama-3-70b-instruct\nreplicate/meta/llama-3-8b\nreplicate/meta/llama-3-8b-instruct\nreplicate/mistralai/mistral-7b-instruct-v0.2\nreplicate/mistralai/mistral-7b-v0.1\nreplicate/mistralai/mixtral-8x7b-instruct-v0.1\nrerank-english-v2.0\nrerank-english-v3.0\nrerank-multilingual-v2.0\nrerank-multilingual-v3.0\nrerank-v3.5\nsagemaker/meta-textgeneration-llama-2-13b\nsagemaker/meta-textgeneration-llama-2-13b-f\nsagemaker/meta-textgeneration-llama-2-70b\nsagemaker/meta-textgeneration-llama-2-70b-b-f\nsagemaker/meta-textgeneration-llama-2-7b\nsagemaker/meta-textgeneration-llama-2-7b-f\nsambanova/DeepSeek-R1\nsambanova/DeepSeek-R1-Distill-Llama-70B\nsambanova/DeepSeek-V3-0324\nsambanova/Llama-4-Maverick-17B-128E-Instruct\nsambanova/Llama-4-Scout-17B-16E-Instruct\nsambanova/Meta-Llama-3.1-405B-Instruct\nsambanova/Meta-Llama-3.1-8B-Instruct\nsambanova/Meta-Llama-3.2-1B-Instruct\nsambanova/Meta-Llama-3.2-3B-Instruct\nsambanova/Meta-Llama-3.3-70B-Instruct\nsambanova/Meta-Llama-Guard-3-8B\nsambanova/QwQ-32B\nsambanova/Qwen2-Audio-7B-Instruct\nsambanova/Qwen3-32B\nsample_spec\nsnowflake/claude-3-5-sonnet\nsnowflake/deepseek-r1\nsnowflake/gemma-7b\nsnowflake/jamba-1.5-large\nsnowflake/jamba-1.5-mini\nsnowflake/jamba-instruct\nsnowflake/llama2-70b-chat\nsnowflake/llama3-70b\nsnowflake/llama3-8b\nsnowflake/llama3.1-405b\nsnowflake/llama3.1-70b\nsnowflake/llama3.1-8b\nsnowflake/llama3.2-1b\nsnowflake/llama3.2-3b\nsnowflake/llama3.3-70b\nsnowflake/mistral-7b\nsnowflake/mistral-large\nsnowflake/mistral-large2\nsnowflake/mixtral-8x7b\nsnowflake/reka-core\nsnowflake/reka-flash\nsnowflake/snowflake-arctic\nsnowflake/snowflake-llama-3.1-405b\nsnowflake/snowflake-llama-3.3-70b\nstability.sd3-5-large-v1:0\nstability.sd3-large-v1:0\nstability.stable-image-core-v1:0\nstability.stable-image-core-v1:1\nstability.stable-image-ultra-v1:0\nstability.stable-image-ultra-v1:1\nstandard/1024-x-1024/dall-e-3\nstandard/1024-x-1792/dall-e-3\nstandard/1792-x-1024/dall-e-3\ntext-bison\ntext-bison32k\ntext-bison32k@002\ntext-bison@001\ntext-bison@002\ntext-completion-codestral/codestral-2405\ntext-completion-codestral/codestral-latest\ntext-embedding-004\ntext-embedding-005\ntext-embedding-3-large\ntext-embedding-3-small\ntext-embedding-ada-002\ntext-embedding-ada-002-v2\ntext-embedding-large-exp-03-07\ntext-embedding-preview-0409\ntext-moderation-007\ntext-moderation-latest\ntext-moderation-stable\ntext-multilingual-embedding-002\ntext-multilingual-embedding-preview-0409\ntext-unicorn\ntext-unicorn@001\ntextembedding-gecko\ntextembedding-gecko-multilingual\ntextembedding-gecko-multilingual@001\ntextembedding-gecko@001\ntextembedding-gecko@003\ntogether-ai-21.1b-41b\ntogether-ai-4.1b-8b\ntogether-ai-41.1b-80b\ntogether-ai-8.1b-21b\ntogether-ai-81.1b-110b\ntogether-ai-embedding-151m-to-350m\ntogether-ai-embedding-up-to-150m\ntogether-ai-up-to-4b\ntogether_ai/OpenAI/gpt-oss-20B\ntogether_ai/Qwen/Qwen2.5-72B-Instruct-Turbo\ntogether_ai/Qwen/Qwen2.5-7B-Instruct-Turbo\ntogether_ai/Qwen/Qwen3-235B-A22B-Instruct-2507-tput\ntogether_ai/Qwen/Qwen3-235B-A22B-Thinking-2507\ntogether_ai/Qwen/Qwen3-235B-A22B-fp8-tput\ntogether_ai/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\ntogether_ai/deepseek-ai/DeepSeek-R1\ntogether_ai/deepseek-ai/DeepSeek-R1-0528-tput\ntogether_ai/deepseek-ai/DeepSeek-V3\ntogether_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo\ntogether_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\ntogether_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\ntogether_ai/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\ntogether_ai/meta-llama/Llama-4-Scout-17B-16E-Instruct\ntogether_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\ntogether_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\ntogether_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\ntogether_ai/mistralai/Mistral-7B-Instruct-v0.1\ntogether_ai/mistralai/Mistral-Small-24B-Instruct-2501\ntogether_ai/mistralai/Mixtral-8x7B-Instruct-v0.1\ntogether_ai/moonshotai/Kimi-K2-Instruct\ntogether_ai/openai/gpt-oss-120b\ntogether_ai/togethercomputer/CodeLlama-34b-Instruct\ntogether_ai/zai-org/GLM-4.5-Air-FP8\ntts-1\ntts-1-hd\nus.amazon.nova-lite-v1:0\nus.amazon.nova-micro-v1:0\nus.amazon.nova-premier-v1:0\nus.amazon.nova-pro-v1:0\nus.anthropic.claude-3-5-haiku-20241022-v1:0\nus.anthropic.claude-3-5-sonnet-20240620-v1:0\nus.anthropic.claude-3-5-sonnet-20241022-v2:0\nus.anthropic.claude-3-7-sonnet-20250219-v1:0\nus.anthropic.claude-3-haiku-20240307-v1:0\nus.anthropic.claude-3-opus-20240229-v1:0\nus.anthropic.claude-3-sonnet-20240229-v1:0\nus.anthropic.claude-opus-4-1-20250805-v1:0\nus.anthropic.claude-opus-4-20250514-v1:0\nus.anthropic.claude-sonnet-4-20250514-v1:0\nus.anthropic.claude-sonnet-4-5-20250929-v1:0\nus.deepseek.r1-v1:0\nus.meta.llama3-1-405b-instruct-v1:0\nus.meta.llama3-1-70b-instruct-v1:0\nus.meta.llama3-1-8b-instruct-v1:0\nus.meta.llama3-2-11b-instruct-v1:0\nus.meta.llama3-2-1b-instruct-v1:0\nus.meta.llama3-2-3b-instruct-v1:0\nus.meta.llama3-2-90b-instruct-v1:0\nus.meta.llama3-3-70b-instruct-v1:0\nus.meta.llama4-maverick-17b-instruct-v1:0\nus.meta.llama4-scout-17b-instruct-v1:0\nus.mistral.pixtral-large-2502-v1:0\nv0/v0-1.0-md\nv0/v0-1.5-lg\nv0/v0-1.5-md\nvertex_ai/claude-3-5-haiku\nvertex_ai/claude-3-5-haiku@20241022\nvertex_ai/claude-3-5-sonnet\nvertex_ai/claude-3-5-sonnet-v2\nvertex_ai/claude-3-5-sonnet-v2@20241022\nvertex_ai/claude-3-5-sonnet@20240620\nvertex_ai/claude-3-7-sonnet@20250219\nvertex_ai/claude-3-haiku\nvertex_ai/claude-3-haiku@20240307\nvertex_ai/claude-3-opus\nvertex_ai/claude-3-opus@20240229\nvertex_ai/claude-3-sonnet\nvertex_ai/claude-3-sonnet@20240229\nvertex_ai/claude-opus-4\nvertex_ai/claude-opus-4-1\nvertex_ai/claude-opus-4-1@20250805\nvertex_ai/claude-opus-4@20250514\nvertex_ai/claude-sonnet-4\nvertex_ai/claude-sonnet-4@20250514\nvertex_ai/codestral-2501\nvertex_ai/codestral@2405\nvertex_ai/codestral@latest\nvertex_ai/deepseek-ai/deepseek-r1-0528-maas\nvertex_ai/imagegeneration@006\nvertex_ai/imagen-3.0-fast-generate-001\nvertex_ai/imagen-3.0-generate-001\nvertex_ai/imagen-3.0-generate-002\nvertex_ai/imagen-4.0-fast-generate-001\nvertex_ai/imagen-4.0-generate-001\nvertex_ai/imagen-4.0-ultra-generate-001\nvertex_ai/jamba-1.5\nvertex_ai/jamba-1.5-large\nvertex_ai/jamba-1.5-large@001\nvertex_ai/jamba-1.5-mini\nvertex_ai/jamba-1.5-mini@001\nvertex_ai/meta/llama-3.1-405b-instruct-maas\nvertex_ai/meta/llama-3.1-70b-instruct-maas\nvertex_ai/meta/llama-3.1-8b-instruct-maas\nvertex_ai/meta/llama-3.2-90b-vision-instruct-maas\nvertex_ai/meta/llama-4-maverick-17b-128e-instruct-maas\nvertex_ai/meta/llama-4-maverick-17b-16e-instruct-maas\nvertex_ai/meta/llama-4-scout-17b-128e-instruct-maas\nvertex_ai/meta/llama-4-scout-17b-16e-instruct-maas\nvertex_ai/meta/llama3-405b-instruct-maas\nvertex_ai/meta/llama3-70b-instruct-maas\nvertex_ai/meta/llama3-8b-instruct-maas\nvertex_ai/mistral-large-2411\nvertex_ai/mistral-large@2407\nvertex_ai/mistral-large@2411-001\nvertex_ai/mistral-large@latest\nvertex_ai/mistral-nemo@2407\nvertex_ai/mistral-nemo@latest\nvertex_ai/mistral-small-2503\nvertex_ai/mistral-small-2503@001\nvertex_ai/qwen/qwen3-235b-a22b-instruct-2507-maas\nvertex_ai/qwen/qwen3-coder-480b-a35b-instruct-maas\nvoyage/rerank-2\nvoyage/rerank-2-lite\nvoyage/voyage-2\nvoyage/voyage-3\nvoyage/voyage-3-large\nvoyage/voyage-3-lite\nvoyage/voyage-code-2\nvoyage/voyage-code-3\nvoyage/voyage-context-3\nvoyage/voyage-finance-2\nvoyage/voyage-large-2\nvoyage/voyage-law-2\nvoyage/voyage-lite-01\nvoyage/voyage-lite-02-instruct\nvoyage/voyage-multimodal-3\nwatsonx/ibm/granite-3-8b-instruct\nwatsonx/mistralai/mistral-large\nwhisper-1\nxai/grok-2\nxai/grok-2-1212\nxai/grok-2-latest\nxai/grok-2-vision\nxai/grok-2-vision-1212\nxai/grok-2-vision-latest\nxai/grok-3\nxai/grok-3-beta\nxai/grok-3-fast-beta\nxai/grok-3-fast-latest\nxai/grok-3-latest\nxai/grok-3-mini\nxai/grok-3-mini-beta\nxai/grok-3-mini-fast\nxai/grok-3-mini-fast-beta\nxai/grok-3-mini-fast-latest\nxai/grok-3-mini-latest\nxai/grok-4\nxai/grok-4-0709\nxai/grok-4-latest\nxai/grok-beta\nxai/grok-code-fast\nxai/grok-code-fast-1\nxai/grok-code-fast-1-0825\nxai/grok-vision-beta\n</code></pre> <p>To find the corresponding API key, check the previous section.</p>"},{"location":"models/quickstart/#extra-model-settings","title":"Extra model settings","text":"<p>To configure reasoning efforts or similar settings, you need to edit the agent config file. In newer versions, the location of the config file is printed when you run <code>mini</code> (\"agent config\" in the output).</p> <p>Here's a few examples:</p> TemperatureGPT-5 reasoning effortOpenRouterLocal models <p><code>litellm</code> allows to set model-specific settings with the <code>model_kwargs</code> key:</p> <pre><code>model:\nmodel_name: \"anthropic/claude-sonnet-4-5-20250929\"\n  model_kwargs:\n    temperature: 0.0\n</code></pre> <p>Note that temperature isn't supported by all models.</p> <p><code>litellm</code> allows to set model-specific settings with the <code>model_kwargs</code> key:</p> <pre><code>model:\n  model_name: \"gpt-5-mini\"\n  model_kwargs:\n    drop_params: true\n    reasoning_effort: \"high\"\n    verbosity: \"medium\"\n</code></pre> <p>Here, <code>drop_params</code> is used to drop any parameters that are not supported by the model.</p> <p>This example explicitly sets the model class to <code>openrouter</code> (see the next section for more details). It also explicitly sets the providers to disable switching between them (this is useful if you need very consistent cost behavior, e.g., for benchmarking, but it's not recommended if you're just interested in getting low latency and good prices).</p> <pre><code>model:\n    model_name: \"moonshotai/kimi-k2-0905\"\n    model_class: \"openrouter\"\n    model_kwargs:\n        temperature: 0.0\n        provider:\n          allow_fallbacks: false\n          only: [\"Moonshot AI\"]\n</code></pre> <p>Using <code>litellm</code> with local models:</p> <pre><code>model:\n  model_name: \"my-local-model\"\n  model_kwargs:\n    custom_llm_provider: \"openai\"\n    api_base: \"https://...\"\n    ...\n</code></pre> <p>See this guide for more details on local models. In particular, you need to configure token costs for local models.</p>"},{"location":"models/quickstart/#model-classes","title":"Model classes","text":"<p>We support the various models through different backends. By default (if you only specify the model name), we pick the best backend for you. This will almost always default to <code>litellm</code> (with Anthropic models being a special case as they need to have explicit cache breakpoint handling).</p> <p>However, there are a few other backends that you can use and specify with the <code>--model-class</code> flag or the <code>model.model_class</code> key in the agent config file (see previous section).</p> <p>For example:</p> Openrouter modelPortkey model <pre><code>mini -m \"moonshotai/kimi-k2-0905\" --model-class openrouter\n</code></pre> <pre><code>mini -m \"claude-sonnet-4-5-20250929\" --model-class portkey\n</code></pre> <ul> <li> <p><code>litellm</code> (<code>LitellmModel</code>) - Default and recommended. Supports most models through litellm. Works with OpenAI, Anthropic, Google, and many other providers.</p> </li> <li> <p><code>anthropic</code> (<code>AnthropicModel</code>) - Wrapper around <code>LitellmModel</code> for Anthropic models that adds cache breakpoint handling. Will be used by default if no <code>model_class</code> is specified and the model name contains \"anthropic\", \"claude\", etc.</p> </li> <li> <p><code>openrouter</code> (<code>OpenRouterModel</code>) - Direct integration with OpenRouter API for accessing various models through a single endpoint.</p> </li> <li> <p><code>portkey</code> (<code>PortkeyModel</code>) - Integration with Portkey for accessing various models with enhanced observability, caching, and routing features. Note that this still uses <code>litellm</code> to calculate costs.</p> </li> </ul> <p>On top, there's a few more exotic model classes that you can use:</p> <ul> <li><code>deterministic</code> (<code>DeterministicModel</code>) - Returns predefined responses for testing and development purposes.</li> <li><code>minisweagent.models.extra.roulette.RouletteModel</code> and <code>minisweagent.models.extra.roulette.InterleavingModel</code> (<code>RouletteModel</code> and <code>InterleavingModel</code>) - Randomly selects or interleaves multiple configured models for each query. See this blog post for more details.</li> </ul> <p>As with the last two, you can also specify any import path to your own custom model class (even if it is not yet part of the mini-SWE-agent package).</p> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"models/troubleshooting/","title":"Troubleshooting","text":""},{"location":"models/troubleshooting/#model-trouble-shooting","title":"Model trouble shooting","text":"<p>This section has examples of common error messages and how to fix them.</p>"},{"location":"models/troubleshooting/#litellm","title":"Litellm","text":"<p><code>litellm</code> is the default model class and is used to support most models.</p>"},{"location":"models/troubleshooting/#invalid-api-key","title":"Invalid API key","text":"<pre><code>AuthenticationError: litellm.AuthenticationError: geminiException - {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"API key not valid. Please pass a valid API key.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n        \"reason\": \"API_KEY_INVALID\",\n        \"domain\": \"googleapis.com\",\n        \"metadata\": {\n          \"service\": \"generativelanguage.googleapis.com\"\n        }\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.LocalizedMessage\",\n        \"locale\": \"en-US\",\n        \"message\": \"API key not valid. Please pass a valid API key.\"\n      }\n    ]\n  }\n}\n You can permanently set your API key with `mini-extra config set KEY VALUE`.\n</code></pre> <p>Double check your API key and make sure it is correct. You can take a look at all your API keys with <code>mini-extra config edit</code>.</p>"},{"location":"models/troubleshooting/#weird-authentication-error","title":"\"Weird\" authentication error","text":"<p>If you fail to authenticate but don't see the previous error message, it might be that you forgot to include the provider in the model name.</p> <p>For example, this:</p> <pre><code>  File \"/Users/.../.virtualenvs/openai/lib/python3.12/site-packages/google/auth/_default.py\", line 685, in default\n    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\ngoogle.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see\nhttps://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n</code></pre> <p>happens if you forgot to prefix your gemini model with <code>gemini/</code>.</p>"},{"location":"models/troubleshooting/#error-during-cost-calculation","title":"Error during cost calculation","text":"<pre><code>Exception: This model isn't mapped yet. model=together_ai/qwen/qwen3-coder-480b-a35b-instruct-fp8, custom_llm_provider=together_ai.\nAdd it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n</code></pre> <p><code>litellm</code> doesn't know about the cost of your model. Take a look at the model registry section of the local models guide to add it.</p> <p>Another common mistake is to not include any or the correct provider in the model name (e.g., <code>gemini-2.0-flash</code> instead of <code>gemini/gemini-2.0-flash</code>).</p>"},{"location":"models/troubleshooting/#temperature-not-supported","title":"Temperature not supported","text":"<p>Some models (like <code>gpt-5</code>, <code>o3</code> etc.) do not support temperature, however our default config specifies <code>temperature: 0.0</code>. You need to switch to a config file that does not specify temperature, e.g., <code>mini_no_temp.yaml</code>.</p> <p>To do this, add <code>-c mini_no_temp</code> to your <code>mini</code> command.</p> <p>We are working on a better solution for this (see this issue).</p>"},{"location":"models/troubleshooting/#portkey","title":"Portkey","text":""},{"location":"models/troubleshooting/#error-during-cost-calculation_1","title":"Error during cost calculation","text":"<p>We use <code>litellm</code> to calculate costs for Portkey models because Portkey doesn't seem to provide per-request cost information without very inconvenient APIs.</p> <p>This can lead to errors likethis:</p> <pre><code>  File \"/opt/miniconda3/envs/clash/lib/python3.10/site-packages/minisweagent/models/portkey_model.py\", line 85, in query\n    cost = litellm.cost_calculator.completion_cost(response)\n  File \"/opt/miniconda3/envs/clash/lib/python3.10/site-packages/litellm/cost_calculator.py\", line 973, in completion_cost\n    raise e\n  File \"/opt/miniconda3/envs/clash/lib/python3.10/site-packages/litellm/cost_calculator.py\", line 966, in completion_cost\n    raise e\n  File \"/opt/miniconda3/envs/clash/lib/python3.10/site-packages/litellm/cost_calculator.py\", line 928, in completion_cost\n    ) = cost_per_token(\n  File \"/opt/miniconda3/envs/clash/lib/python3.10/site-packages/litellm/cost_calculator.py\", line 218, in cost_per_token\n    _, custom_llm_provider, _, _ = litellm.get_llm_provider(model=model)\n  File \"/opt/miniconda3/envs/clash/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 395, in get_llm_provider\n    raise e\n  File \"/opt/miniconda3/envs/clash/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 372, in get_llm_provider\n    raise litellm.exceptions.BadRequestError(  # type: ignore\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=grok-code-fast-1\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n</code></pre> <p>In this case, the issue is simply that the portkey model name doesn't match the litellm model name (and very specifically here doesn't include the provider).</p> <p>To fix this, you can manually set the litellm model name to the portkey model name with the <code>litellm_model_name_override</code> key. For example:</p> <pre><code>model:\n  model_name: \"grok-code-fast-1\"  # the portkey model name\n  model_class: \"portkey\"  # make sure to use the portkey model class\n  litellm_model_name_override: \"xai/grok-code-fast-1\"  # the litellm model name for cost information\n  ...\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/","title":"API Reference","text":""},{"location":"reference/#api-reference","title":"API Reference","text":"<p>This section provides detailed documentation for all classes and modules in mini-SWE-agent.</p>"},{"location":"reference/#agents","title":"Agents","text":"<ul> <li>DefaultAgent - The minimal default agent implementation</li> <li>InteractiveAgent - Agent with human-in-the-loop functionality</li> <li>TextualAgent - Agent with interactive TUI using Textual</li> </ul>"},{"location":"reference/#models","title":"Models","text":"<ul> <li>LitellmModel - Wrapper for LiteLLM models (supports most LLM providers)</li> <li>AnthropicModel - Specialized interface for Anthropic models</li> <li>DeterministicModel - Deterministic models for testing</li> <li>Model Utilities - Convenience functions for model selection and configuration</li> </ul>"},{"location":"reference/#environments","title":"Environments","text":"<ul> <li>LocalEnvironment - Execute commands in the local environment</li> <li>DockerEnvironment - Execute commands in Docker containers</li> <li>SwerexDockerEnvironment - Extended Docker environment with SWE-Rex integration</li> </ul>"},{"location":"reference/#run-scripts","title":"Run Scripts","text":"<p>Entry points and command-line interfaces:</p> <ul> <li>Hello World - Simple example usage</li> <li>mini - Interactive local execution</li> <li>GitHub Issue - GitHub issue solver</li> <li>SWE-bench - SWE-bench evaluation script</li> </ul> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/agents/default/","title":"DefaultAgent","text":""},{"location":"reference/agents/default/#defaultagent","title":"DefaultAgent","text":"<p>DefaultAgent class</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>\"\"\"Basic agent class. See https://mini-swe-agent.com/latest/advanced/control_flow/ for visual explanation.\"\"\"\n\nimport re\nimport subprocess\nfrom collections.abc import Callable\nfrom dataclasses import asdict, dataclass\n\nfrom jinja2 import StrictUndefined, Template\n\nfrom minisweagent import Environment, Model\n\n\n@dataclass\nclass AgentConfig:\n    # The default settings are the bare minimum to run the agent. Take a look at the config files for improved settings.\n    system_template: str = \"You are a helpful assistant that can do anything.\"\n    instance_template: str = (\n        \"Your task: {{task}}. Please reply with a single shell command in triple backticks. \"\n        \"To finish, the first line of the output of the shell command must be 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'.\"\n    )\n    timeout_template: str = (\n        \"The last command &lt;command&gt;{{action['action']}}&lt;/command&gt; timed out and has been killed.\\n\"\n        \"The output of the command was:\\n &lt;output&gt;\\n{{output}}\\n&lt;/output&gt;\\n\"\n        \"Please try another command and make sure to avoid those requiring interactive input.\"\n    )\n    format_error_template: str = \"Please always provide EXACTLY ONE action in triple backticks.\"\n    action_observation_template: str = \"Observation: {{output}}\"\n    step_limit: int = 0\n    cost_limit: float = 3.0\n\n\nclass NonTerminatingException(Exception):\n    \"\"\"Raised for conditions that can be handled by the agent.\"\"\"\n\n\nclass FormatError(NonTerminatingException):\n    \"\"\"Raised when the LM's output is not in the expected format.\"\"\"\n\n\nclass ExecutionTimeoutError(NonTerminatingException):\n    \"\"\"Raised when the action execution timed out.\"\"\"\n\n\nclass TerminatingException(Exception):\n    \"\"\"Raised for conditions that terminate the agent.\"\"\"\n\n\nclass Submitted(TerminatingException):\n    \"\"\"Raised when the LM declares that the agent has finished its task.\"\"\"\n\n\nclass LimitsExceeded(TerminatingException):\n    \"\"\"Raised when the agent has reached its cost or step limit.\"\"\"\n\n\nclass DefaultAgent:\n    def __init__(self, model: Model, env: Environment, *, config_class: Callable = AgentConfig, **kwargs):\n        self.config = config_class(**kwargs)\n        self.messages: list[dict] = []\n        self.model = model\n        self.env = env\n        self.extra_template_vars = {}\n\n    def render_template(self, template: str, **kwargs) -&gt; str:\n        template_vars = asdict(self.config) | self.env.get_template_vars() | self.model.get_template_vars()\n        return Template(template, undefined=StrictUndefined).render(\n            **kwargs, **template_vars, **self.extra_template_vars\n        )\n\n    def add_message(self, role: str, content: str, **kwargs):\n        self.messages.append({\"role\": role, \"content\": content, **kwargs})\n\n    def run(self, task: str, **kwargs) -&gt; tuple[str, str]:\n        \"\"\"Run step() until agent is finished. Return exit status &amp; message\"\"\"\n        self.extra_template_vars |= {\"task\": task, **kwargs}\n        self.messages = []\n        self.add_message(\"system\", self.render_template(self.config.system_template))\n        self.add_message(\"user\", self.render_template(self.config.instance_template))\n        while True:\n            try:\n                self.step()\n            except NonTerminatingException as e:\n                self.add_message(\"user\", str(e))\n            except TerminatingException as e:\n                self.add_message(\"user\", str(e))\n                return type(e).__name__, str(e)\n\n    def step(self) -&gt; dict:\n        \"\"\"Query the LM, execute the action, return the observation.\"\"\"\n        return self.get_observation(self.query())\n\n    def query(self) -&gt; dict:\n        \"\"\"Query the model and return the response.\"\"\"\n        if 0 &lt; self.config.step_limit &lt;= self.model.n_calls or 0 &lt; self.config.cost_limit &lt;= self.model.cost:\n            raise LimitsExceeded()\n        response = self.model.query(self.messages)\n        self.add_message(\"assistant\", **response)\n        return response\n\n    def get_observation(self, response: dict) -&gt; dict:\n        \"\"\"Execute the action and return the observation.\"\"\"\n        output = self.execute_action(self.parse_action(response))\n        observation = self.render_template(self.config.action_observation_template, output=output)\n        self.add_message(\"user\", observation)\n        return output\n\n    def parse_action(self, response: dict) -&gt; dict:\n        \"\"\"Parse the action from the message. Returns the action.\"\"\"\n        actions = re.findall(r\"```bash\\s*\\n(.*?)\\n```\", response[\"content\"], re.DOTALL)\n        if len(actions) == 1:\n            return {\"action\": actions[0].strip(), **response}\n        raise FormatError(self.render_template(self.config.format_error_template, actions=actions))\n\n    def execute_action(self, action: dict) -&gt; dict:\n        try:\n            output = self.env.execute(action[\"action\"])\n        except subprocess.TimeoutExpired as e:\n            output = e.output.decode(\"utf-8\", errors=\"replace\") if e.output else \"\"\n            raise ExecutionTimeoutError(\n                self.render_template(self.config.timeout_template, action=action, output=output)\n            )\n        except TimeoutError:\n            raise ExecutionTimeoutError(self.render_template(self.config.timeout_template, action=action, output=\"\"))\n        self.has_finished(output)\n        return output\n\n    def has_finished(self, output: dict[str, str]):\n        \"\"\"Raises Submitted exception with final output if the agent has finished its task.\"\"\"\n        lines = output.get(\"output\", \"\").lstrip().splitlines(keepends=True)\n        if lines and lines[0].strip() in [\"MINI_SWE_AGENT_FINAL_OUTPUT\", \"COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT\"]:\n            raise Submitted(\"\".join(lines[1:]))\n</code></pre> <p>Understanding the control flow</p> <p>Check out the control flow guide for a visual explanation of the agent's control flow following this picture:</p> <p></p> <p></p> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/agents/default/#minisweagent.agents.default.AgentConfig","title":"minisweagent.agents.default.AgentConfig  <code>dataclass</code>","text":"<pre><code>AgentConfig(\n    system_template: str = \"You are a helpful assistant that can do anything.\",\n    instance_template: str = \"Your task: {{task}}. Please reply with a single shell command in triple backticks. To finish, the first line of the output of the shell command must be 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'.\",\n    timeout_template: str = \"The last command &lt;command&gt;{{action['action']}}&lt;/command&gt; timed out and has been killed.\\nThe output of the command was:\\n &lt;output&gt;\\n{{output}}\\n&lt;/output&gt;\\nPlease try another command and make sure to avoid those requiring interactive input.\",\n    format_error_template: str = \"Please always provide EXACTLY ONE action in triple backticks.\",\n    action_observation_template: str = \"Observation: {{output}}\",\n    step_limit: int = 0,\n    cost_limit: float = 3.0,\n)\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.AgentConfig.system_template","title":"system_template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>system_template: str = (\n    \"You are a helpful assistant that can do anything.\"\n)\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.AgentConfig.instance_template","title":"instance_template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>instance_template: str = \"Your task: {{task}}. Please reply with a single shell command in triple backticks. To finish, the first line of the output of the shell command must be 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'.\"\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.AgentConfig.timeout_template","title":"timeout_template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout_template: str = \"The last command &lt;command&gt;{{action['action']}}&lt;/command&gt; timed out and has been killed.\\nThe output of the command was:\\n &lt;output&gt;\\n{{output}}\\n&lt;/output&gt;\\nPlease try another command and make sure to avoid those requiring interactive input.\"\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.AgentConfig.format_error_template","title":"format_error_template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>format_error_template: str = \"Please always provide EXACTLY ONE action in triple backticks.\"\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.AgentConfig.action_observation_template","title":"action_observation_template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>action_observation_template: str = \"Observation: {{output}}\"\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.AgentConfig.step_limit","title":"step_limit  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>step_limit: int = 0\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.AgentConfig.cost_limit","title":"cost_limit  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cost_limit: float = 3.0\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent","title":"minisweagent.agents.default.DefaultAgent","text":"<pre><code>DefaultAgent(\n    model: Model,\n    env: Environment,\n    *,\n    config_class: Callable = AgentConfig,\n    **kwargs,\n)\n</code></pre> Source code in <code>src/minisweagent/agents/default.py</code> <pre><code>def __init__(self, model: Model, env: Environment, *, config_class: Callable = AgentConfig, **kwargs):\n    self.config = config_class(**kwargs)\n    self.messages: list[dict] = []\n    self.model = model\n    self.env = env\n    self.extra_template_vars = {}\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config_class(**kwargs)\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.messages","title":"messages  <code>instance-attribute</code>","text":"<pre><code>messages: list[dict] = []\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.env","title":"env  <code>instance-attribute</code>","text":"<pre><code>env = env\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.extra_template_vars","title":"extra_template_vars  <code>instance-attribute</code>","text":"<pre><code>extra_template_vars = {}\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.render_template","title":"render_template","text":"<pre><code>render_template(template: str, **kwargs) -&gt; str\n</code></pre> Source code in <code>src/minisweagent/agents/default.py</code> <pre><code>def render_template(self, template: str, **kwargs) -&gt; str:\n    template_vars = asdict(self.config) | self.env.get_template_vars() | self.model.get_template_vars()\n    return Template(template, undefined=StrictUndefined).render(\n        **kwargs, **template_vars, **self.extra_template_vars\n    )\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.add_message","title":"add_message","text":"<pre><code>add_message(role: str, content: str, **kwargs)\n</code></pre> Source code in <code>src/minisweagent/agents/default.py</code> <pre><code>def add_message(self, role: str, content: str, **kwargs):\n    self.messages.append({\"role\": role, \"content\": content, **kwargs})\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.run","title":"run","text":"<pre><code>run(task: str, **kwargs) -&gt; tuple[str, str]\n</code></pre> <p>Run step() until agent is finished. Return exit status &amp; message</p> Source code in <code>src/minisweagent/agents/default.py</code> <pre><code>def run(self, task: str, **kwargs) -&gt; tuple[str, str]:\n    \"\"\"Run step() until agent is finished. Return exit status &amp; message\"\"\"\n    self.extra_template_vars |= {\"task\": task, **kwargs}\n    self.messages = []\n    self.add_message(\"system\", self.render_template(self.config.system_template))\n    self.add_message(\"user\", self.render_template(self.config.instance_template))\n    while True:\n        try:\n            self.step()\n        except NonTerminatingException as e:\n            self.add_message(\"user\", str(e))\n        except TerminatingException as e:\n            self.add_message(\"user\", str(e))\n            return type(e).__name__, str(e)\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.step","title":"step","text":"<pre><code>step() -&gt; dict\n</code></pre> <p>Query the LM, execute the action, return the observation.</p> Source code in <code>src/minisweagent/agents/default.py</code> <pre><code>def step(self) -&gt; dict:\n    \"\"\"Query the LM, execute the action, return the observation.\"\"\"\n    return self.get_observation(self.query())\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.query","title":"query","text":"<pre><code>query() -&gt; dict\n</code></pre> <p>Query the model and return the response.</p> Source code in <code>src/minisweagent/agents/default.py</code> <pre><code>def query(self) -&gt; dict:\n    \"\"\"Query the model and return the response.\"\"\"\n    if 0 &lt; self.config.step_limit &lt;= self.model.n_calls or 0 &lt; self.config.cost_limit &lt;= self.model.cost:\n        raise LimitsExceeded()\n    response = self.model.query(self.messages)\n    self.add_message(\"assistant\", **response)\n    return response\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.get_observation","title":"get_observation","text":"<pre><code>get_observation(response: dict) -&gt; dict\n</code></pre> <p>Execute the action and return the observation.</p> Source code in <code>src/minisweagent/agents/default.py</code> <pre><code>def get_observation(self, response: dict) -&gt; dict:\n    \"\"\"Execute the action and return the observation.\"\"\"\n    output = self.execute_action(self.parse_action(response))\n    observation = self.render_template(self.config.action_observation_template, output=output)\n    self.add_message(\"user\", observation)\n    return output\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.parse_action","title":"parse_action","text":"<pre><code>parse_action(response: dict) -&gt; dict\n</code></pre> <p>Parse the action from the message. Returns the action.</p> Source code in <code>src/minisweagent/agents/default.py</code> <pre><code>def parse_action(self, response: dict) -&gt; dict:\n    \"\"\"Parse the action from the message. Returns the action.\"\"\"\n    actions = re.findall(r\"```bash\\s*\\n(.*?)\\n```\", response[\"content\"], re.DOTALL)\n    if len(actions) == 1:\n        return {\"action\": actions[0].strip(), **response}\n    raise FormatError(self.render_template(self.config.format_error_template, actions=actions))\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.execute_action","title":"execute_action","text":"<pre><code>execute_action(action: dict) -&gt; dict\n</code></pre> Source code in <code>src/minisweagent/agents/default.py</code> <pre><code>def execute_action(self, action: dict) -&gt; dict:\n    try:\n        output = self.env.execute(action[\"action\"])\n    except subprocess.TimeoutExpired as e:\n        output = e.output.decode(\"utf-8\", errors=\"replace\") if e.output else \"\"\n        raise ExecutionTimeoutError(\n            self.render_template(self.config.timeout_template, action=action, output=output)\n        )\n    except TimeoutError:\n        raise ExecutionTimeoutError(self.render_template(self.config.timeout_template, action=action, output=\"\"))\n    self.has_finished(output)\n    return output\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.DefaultAgent.has_finished","title":"has_finished","text":"<pre><code>has_finished(output: dict[str, str])\n</code></pre> <p>Raises Submitted exception with final output if the agent has finished its task.</p> Source code in <code>src/minisweagent/agents/default.py</code> <pre><code>def has_finished(self, output: dict[str, str]):\n    \"\"\"Raises Submitted exception with final output if the agent has finished its task.\"\"\"\n    lines = output.get(\"output\", \"\").lstrip().splitlines(keepends=True)\n    if lines and lines[0].strip() in [\"MINI_SWE_AGENT_FINAL_OUTPUT\", \"COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT\"]:\n        raise Submitted(\"\".join(lines[1:]))\n</code></pre>"},{"location":"reference/agents/default/#minisweagent.agents.default.NonTerminatingException","title":"minisweagent.agents.default.NonTerminatingException","text":"<p>               Bases: <code>Exception</code></p> <p>Raised for conditions that can be handled by the agent.</p>"},{"location":"reference/agents/default/#minisweagent.agents.default.TerminatingException","title":"minisweagent.agents.default.TerminatingException","text":"<p>               Bases: <code>Exception</code></p> <p>Raised for conditions that terminate the agent.</p>"},{"location":"reference/agents/interactive/","title":"InteractiveAgent","text":""},{"location":"reference/agents/interactive/#interactive","title":"Interactive","text":"<p>InteractiveAgent class</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>\"\"\"A small generalization of the default agent that puts the user in the loop.\n\nThere are three modes:\n- human: commands issued by the user are executed immediately\n- confirm: commands issued by the LM but not whitelisted are confirmed by the user\n- yolo: commands issued by the LM are executed immediately without confirmation\n\"\"\"\n\nimport re\nfrom dataclasses import dataclass, field\nfrom typing import Literal\n\nfrom prompt_toolkit.history import FileHistory\nfrom prompt_toolkit.shortcuts import PromptSession\nfrom rich.console import Console\nfrom rich.rule import Rule\n\nfrom minisweagent import global_config_dir\nfrom minisweagent.agents.default import AgentConfig, DefaultAgent, LimitsExceeded, NonTerminatingException, Submitted\n\nconsole = Console(highlight=False)\nprompt_session = PromptSession(history=FileHistory(global_config_dir / \"interactive_history.txt\"))\n\n\n@dataclass\nclass InteractiveAgentConfig(AgentConfig):\n    mode: Literal[\"human\", \"confirm\", \"yolo\"] = \"confirm\"\n    \"\"\"Whether to confirm actions.\"\"\"\n    whitelist_actions: list[str] = field(default_factory=list)\n    \"\"\"Never confirm actions that match these regular expressions.\"\"\"\n    confirm_exit: bool = True\n    \"\"\"If the agent wants to finish, do we ask for confirmation from user?\"\"\"\n\n\nclass InteractiveAgent(DefaultAgent):\n    _MODE_COMMANDS_MAPPING = {\"/u\": \"human\", \"/c\": \"confirm\", \"/y\": \"yolo\"}\n\n    def __init__(self, *args, config_class=InteractiveAgentConfig, **kwargs):\n        super().__init__(*args, config_class=config_class, **kwargs)\n        self.cost_last_confirmed = 0.0\n\n    def add_message(self, role: str, content: str, **kwargs):\n        # Extend supermethod to print messages\n        super().add_message(role, content, **kwargs)\n        if role == \"assistant\":\n            console.print(\n                f\"\\n[red][bold]mini-swe-agent[/bold] (step [bold]{self.model.n_calls}[/bold], [bold]${self.model.cost:.2f}[/bold]):[/red]\\n\",\n                end=\"\",\n                highlight=False,\n            )\n        else:\n            console.print(f\"\\n[bold green]{role.capitalize()}[/bold green]:\\n\", end=\"\", highlight=False)\n        console.print(content, highlight=False, markup=False)\n\n    def query(self) -&gt; dict:\n        # Extend supermethod to handle human mode\n        if self.config.mode == \"human\":\n            match command := self._prompt_and_handle_special(\"[bold yellow]&gt;[/bold yellow] \"):\n                case \"/y\" | \"/c\":  # Just go to the super query, which queries the LM for the next action\n                    pass\n                case _:\n                    msg = {\"content\": f\"\\n```bash\\n{command}\\n```\"}\n                    self.add_message(\"assistant\", msg[\"content\"])\n                    return msg\n        try:\n            with console.status(\"Waiting for the LM to respond...\"):\n                return super().query()\n        except LimitsExceeded:\n            console.print(\n                f\"Limits exceeded. Limits: {self.config.step_limit} steps, ${self.config.cost_limit}.\\n\"\n                f\"Current spend: {self.model.n_calls} steps, ${self.model.cost:.2f}.\"\n            )\n            self.config.step_limit = int(input(\"New step limit: \"))\n            self.config.cost_limit = float(input(\"New cost limit: \"))\n            return super().query()\n\n    def step(self) -&gt; dict:\n        # Override the step method to handle user interruption\n        try:\n            console.print(Rule())\n            return super().step()\n        except KeyboardInterrupt:\n            # We always add a message about the interrupt and then just proceed to the next step\n            interruption_message = self._prompt_and_handle_special(\n                \"\\n\\n[bold yellow]Interrupted.[/bold yellow] \"\n                \"[green]Type a comment/command[/green] (/h for available commands)\"\n                \"\\n[bold yellow]&gt;[/bold yellow] \"\n            ).strip()\n            if not interruption_message or interruption_message in self._MODE_COMMANDS_MAPPING:\n                interruption_message = \"Temporary interruption caught.\"\n            raise NonTerminatingException(f\"Interrupted by user: {interruption_message}\")\n\n    def execute_action(self, action: dict) -&gt; dict:\n        # Override the execute_action method to handle user confirmation\n        if self.should_ask_confirmation(action[\"action\"]):\n            self.ask_confirmation()\n        return super().execute_action(action)\n\n    def should_ask_confirmation(self, action: str) -&gt; bool:\n        return self.config.mode == \"confirm\" and not any(re.match(r, action) for r in self.config.whitelist_actions)\n\n    def ask_confirmation(self) -&gt; None:\n        prompt = (\n            \"[bold yellow]Execute?[/bold yellow] [green][bold]Enter[/bold] to confirm[/green], \"\n            \"or [green]Type a comment/command[/green] (/h for available commands)\\n\"\n            \"[bold yellow]&gt;[/bold yellow] \"\n        )\n        match user_input := self._prompt_and_handle_special(prompt).strip():\n            case \"\" | \"/y\":\n                pass  # confirmed, do nothing\n            case \"/u\":  # Skip execution action and get back to query\n                raise NonTerminatingException(\"Command not executed. Switching to human mode\")\n            case _:\n                raise NonTerminatingException(\n                    f\"Command not executed. The user rejected your command with the following message: {user_input}\"\n                )\n\n    def _prompt_and_handle_special(self, prompt: str) -&gt; str:\n        \"\"\"Prompts the user, takes care of /h (followed by requery) and sets the mode. Returns the user input.\"\"\"\n        console.print(prompt, end=\"\")\n        user_input = prompt_session.prompt(\"\")\n        if user_input == \"/h\":\n            console.print(\n                f\"Current mode: [bold green]{self.config.mode}[/bold green]\\n\"\n                f\"[bold green]/y[/bold green] to switch to [bold yellow]yolo[/bold yellow] mode (execute LM commands without confirmation)\\n\"\n                f\"[bold green]/c[/bold green] to switch to [bold yellow]confirmation[/bold yellow] mode (ask for confirmation before executing LM commands)\\n\"\n                f\"[bold green]/u[/bold green] to switch to [bold yellow]human[/bold yellow] mode (execute commands issued by the user)\\n\"\n            )\n            return self._prompt_and_handle_special(prompt)\n        if user_input in self._MODE_COMMANDS_MAPPING:\n            if self.config.mode == self._MODE_COMMANDS_MAPPING[user_input]:\n                return self._prompt_and_handle_special(\n                    f\"[bold red]Already in {self.config.mode} mode.[/bold red]\\n{prompt}\"\n                )\n            self.config.mode = self._MODE_COMMANDS_MAPPING[user_input]\n            console.print(f\"Switched to [bold green]{self.config.mode}[/bold green] mode.\")\n            return user_input\n        return user_input\n\n    def has_finished(self, output: dict[str, str]):\n        try:\n            return super().has_finished(output)\n        except Submitted as e:\n            if self.config.confirm_exit:\n                console.print(\n                    \"[bold green]Agent wants to finish.[/bold green] \"\n                    \"[green]Type a comment to give it a new task or press enter to quit.\\n\"\n                    \"[bold yellow]&gt;[/bold yellow] \",\n                    end=\"\",\n                )\n                if new_task := self._prompt_and_handle_special(\"\").strip():\n                    raise NonTerminatingException(f\"The user added a new task: {new_task}\")\n            raise e\n</code></pre> <p>See also</p> <ul> <li>This agent subclass builds on top of the default agent, make sure to read that first.</li> <li>This class powers the <code>mini</code> command line tool, see usage for more details.</li> </ul> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive","title":"minisweagent.agents.interactive","text":"<p>A small generalization of the default agent that puts the user in the loop.</p> <p>There are three modes: - human: commands issued by the user are executed immediately - confirm: commands issued by the LM but not whitelisted are confirmed by the user - yolo: commands issued by the LM are executed immediately without confirmation</p>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.console","title":"console  <code>module-attribute</code>","text":"<pre><code>console = Console(highlight=False)\n</code></pre>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.prompt_session","title":"prompt_session  <code>module-attribute</code>","text":"<pre><code>prompt_session = PromptSession(\n    history=FileHistory(\n        global_config_dir / \"interactive_history.txt\"\n    )\n)\n</code></pre>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgentConfig","title":"InteractiveAgentConfig  <code>dataclass</code>","text":"<pre><code>InteractiveAgentConfig(\n    system_template: str = \"You are a helpful assistant that can do anything.\",\n    instance_template: str = \"Your task: {{task}}. Please reply with a single shell command in triple backticks. To finish, the first line of the output of the shell command must be 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'.\",\n    timeout_template: str = \"The last command &lt;command&gt;{{action['action']}}&lt;/command&gt; timed out and has been killed.\\nThe output of the command was:\\n &lt;output&gt;\\n{{output}}\\n&lt;/output&gt;\\nPlease try another command and make sure to avoid those requiring interactive input.\",\n    format_error_template: str = \"Please always provide EXACTLY ONE action in triple backticks.\",\n    action_observation_template: str = \"Observation: {{output}}\",\n    step_limit: int = 0,\n    cost_limit: float = 3.0,\n    mode: Literal[\"human\", \"confirm\", \"yolo\"] = \"confirm\",\n    whitelist_actions: list[str] = list(),\n    confirm_exit: bool = True,\n)\n</code></pre> <p>               Bases: <code>AgentConfig</code></p>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgentConfig.mode","title":"mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mode: Literal['human', 'confirm', 'yolo'] = 'confirm'\n</code></pre> <p>Whether to confirm actions.</p>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgentConfig.whitelist_actions","title":"whitelist_actions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>whitelist_actions: list[str] = field(default_factory=list)\n</code></pre> <p>Never confirm actions that match these regular expressions.</p>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgentConfig.confirm_exit","title":"confirm_exit  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>confirm_exit: bool = True\n</code></pre> <p>If the agent wants to finish, do we ask for confirmation from user?</p>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgent","title":"InteractiveAgent","text":"<pre><code>InteractiveAgent(\n    *args, config_class=InteractiveAgentConfig, **kwargs\n)\n</code></pre> <p>               Bases: <code>DefaultAgent</code></p> Source code in <code>src/minisweagent/agents/interactive.py</code> <pre><code>def __init__(self, *args, config_class=InteractiveAgentConfig, **kwargs):\n    super().__init__(*args, config_class=config_class, **kwargs)\n    self.cost_last_confirmed = 0.0\n</code></pre>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgent.cost_last_confirmed","title":"cost_last_confirmed  <code>instance-attribute</code>","text":"<pre><code>cost_last_confirmed = 0.0\n</code></pre>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgent.add_message","title":"add_message","text":"<pre><code>add_message(role: str, content: str, **kwargs)\n</code></pre> Source code in <code>src/minisweagent/agents/interactive.py</code> <pre><code>def add_message(self, role: str, content: str, **kwargs):\n    # Extend supermethod to print messages\n    super().add_message(role, content, **kwargs)\n    if role == \"assistant\":\n        console.print(\n            f\"\\n[red][bold]mini-swe-agent[/bold] (step [bold]{self.model.n_calls}[/bold], [bold]${self.model.cost:.2f}[/bold]):[/red]\\n\",\n            end=\"\",\n            highlight=False,\n        )\n    else:\n        console.print(f\"\\n[bold green]{role.capitalize()}[/bold green]:\\n\", end=\"\", highlight=False)\n    console.print(content, highlight=False, markup=False)\n</code></pre>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgent.query","title":"query","text":"<pre><code>query() -&gt; dict\n</code></pre> Source code in <code>src/minisweagent/agents/interactive.py</code> <pre><code>def query(self) -&gt; dict:\n    # Extend supermethod to handle human mode\n    if self.config.mode == \"human\":\n        match command := self._prompt_and_handle_special(\"[bold yellow]&gt;[/bold yellow] \"):\n            case \"/y\" | \"/c\":  # Just go to the super query, which queries the LM for the next action\n                pass\n            case _:\n                msg = {\"content\": f\"\\n```bash\\n{command}\\n```\"}\n                self.add_message(\"assistant\", msg[\"content\"])\n                return msg\n    try:\n        with console.status(\"Waiting for the LM to respond...\"):\n            return super().query()\n    except LimitsExceeded:\n        console.print(\n            f\"Limits exceeded. Limits: {self.config.step_limit} steps, ${self.config.cost_limit}.\\n\"\n            f\"Current spend: {self.model.n_calls} steps, ${self.model.cost:.2f}.\"\n        )\n        self.config.step_limit = int(input(\"New step limit: \"))\n        self.config.cost_limit = float(input(\"New cost limit: \"))\n        return super().query()\n</code></pre>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgent.step","title":"step","text":"<pre><code>step() -&gt; dict\n</code></pre> Source code in <code>src/minisweagent/agents/interactive.py</code> <pre><code>def step(self) -&gt; dict:\n    # Override the step method to handle user interruption\n    try:\n        console.print(Rule())\n        return super().step()\n    except KeyboardInterrupt:\n        # We always add a message about the interrupt and then just proceed to the next step\n        interruption_message = self._prompt_and_handle_special(\n            \"\\n\\n[bold yellow]Interrupted.[/bold yellow] \"\n            \"[green]Type a comment/command[/green] (/h for available commands)\"\n            \"\\n[bold yellow]&gt;[/bold yellow] \"\n        ).strip()\n        if not interruption_message or interruption_message in self._MODE_COMMANDS_MAPPING:\n            interruption_message = \"Temporary interruption caught.\"\n        raise NonTerminatingException(f\"Interrupted by user: {interruption_message}\")\n</code></pre>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgent.execute_action","title":"execute_action","text":"<pre><code>execute_action(action: dict) -&gt; dict\n</code></pre> Source code in <code>src/minisweagent/agents/interactive.py</code> <pre><code>def execute_action(self, action: dict) -&gt; dict:\n    # Override the execute_action method to handle user confirmation\n    if self.should_ask_confirmation(action[\"action\"]):\n        self.ask_confirmation()\n    return super().execute_action(action)\n</code></pre>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgent.should_ask_confirmation","title":"should_ask_confirmation","text":"<pre><code>should_ask_confirmation(action: str) -&gt; bool\n</code></pre> Source code in <code>src/minisweagent/agents/interactive.py</code> <pre><code>def should_ask_confirmation(self, action: str) -&gt; bool:\n    return self.config.mode == \"confirm\" and not any(re.match(r, action) for r in self.config.whitelist_actions)\n</code></pre>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgent.ask_confirmation","title":"ask_confirmation","text":"<pre><code>ask_confirmation() -&gt; None\n</code></pre> Source code in <code>src/minisweagent/agents/interactive.py</code> <pre><code>def ask_confirmation(self) -&gt; None:\n    prompt = (\n        \"[bold yellow]Execute?[/bold yellow] [green][bold]Enter[/bold] to confirm[/green], \"\n        \"or [green]Type a comment/command[/green] (/h for available commands)\\n\"\n        \"[bold yellow]&gt;[/bold yellow] \"\n    )\n    match user_input := self._prompt_and_handle_special(prompt).strip():\n        case \"\" | \"/y\":\n            pass  # confirmed, do nothing\n        case \"/u\":  # Skip execution action and get back to query\n            raise NonTerminatingException(\"Command not executed. Switching to human mode\")\n        case _:\n            raise NonTerminatingException(\n                f\"Command not executed. The user rejected your command with the following message: {user_input}\"\n            )\n</code></pre>"},{"location":"reference/agents/interactive/#minisweagent.agents.interactive.InteractiveAgent.has_finished","title":"has_finished","text":"<pre><code>has_finished(output: dict[str, str])\n</code></pre> Source code in <code>src/minisweagent/agents/interactive.py</code> <pre><code>def has_finished(self, output: dict[str, str]):\n    try:\n        return super().has_finished(output)\n    except Submitted as e:\n        if self.config.confirm_exit:\n            console.print(\n                \"[bold green]Agent wants to finish.[/bold green] \"\n                \"[green]Type a comment to give it a new task or press enter to quit.\\n\"\n                \"[bold yellow]&gt;[/bold yellow] \",\n                end=\"\",\n            )\n            if new_task := self._prompt_and_handle_special(\"\").strip():\n                raise NonTerminatingException(f\"The user added a new task: {new_task}\")\n        raise e\n</code></pre>"},{"location":"reference/agents/textual/","title":"TextualAgent","text":""},{"location":"reference/agents/textual/#textualagent","title":"TextualAgent","text":"<p>TextualAgent class</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>\"\"\"\nExtension of the `default.py` agent that uses Textual for an interactive TUI.\nFor a simpler version of an interactive UI that does not require threading and more, see `interactive.py`.\n\"\"\"\n\nimport logging\nimport os\nimport re\nimport threading\nimport time\nimport traceback\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Literal\n\nfrom rich.spinner import Spinner\nfrom rich.text import Text\nfrom textual.app import App, ComposeResult, SystemCommand\nfrom textual.binding import Binding\nfrom textual.containers import Container, Vertical, VerticalScroll\nfrom textual.css.query import NoMatches\nfrom textual.events import Key\nfrom textual.screen import Screen\nfrom textual.widgets import Footer, Header, Input, Static, TextArea\n\nfrom minisweagent.agents.default import AgentConfig, DefaultAgent, NonTerminatingException, Submitted\n\n\n@dataclass\nclass TextualAgentConfig(AgentConfig):\n    mode: Literal[\"confirm\", \"yolo\"] = \"confirm\"\n    \"\"\"Mode for action execution: 'confirm' requires user confirmation, 'yolo' executes immediately.\"\"\"\n    whitelist_actions: list[str] = field(default_factory=list)\n    \"\"\"Never confirm actions that match these regular expressions.\"\"\"\n    confirm_exit: bool = True\n    \"\"\"If the agent wants to finish, do we ask for confirmation from user?\"\"\"\n\n\nclass _TextualAgent(DefaultAgent):\n    def __init__(self, app: \"TextualAgent\", *args, **kwargs):\n        \"\"\"Connects the DefaultAgent to the TextualApp.\"\"\"\n        self.app = app\n        super().__init__(*args, config_class=TextualAgentConfig, **kwargs)\n        self._current_action_from_human = False\n\n    def add_message(self, role: str, content: str, **kwargs):\n        super().add_message(role, content, **kwargs)\n        if self.app.agent_state != \"UNINITIALIZED\":\n            self.app.call_from_thread(self.app.on_message_added)\n\n    def query(self) -&gt; dict:\n        if self.config.mode == \"human\":\n            human_input = self.app.input_container.request_input(\"Enter your command:\")\n            self._current_action_from_human = True\n            msg = {\"content\": f\"\\n```bash\\n{human_input}\\n```\"}\n            self.add_message(\"assistant\", msg[\"content\"])\n            return msg\n        self._current_action_from_human = False\n        return super().query()\n\n    def run(self, task: str, **kwargs) -&gt; tuple[str, str]:\n        try:\n            exit_status, result = super().run(task, **kwargs)\n        except Exception as e:\n            result = str(e)\n            self.app.call_from_thread(self.app.action_quit)\n            print(traceback.format_exc())\n            return \"ERROR\", result\n        else:\n            self.app.call_from_thread(self.app.on_agent_finished, exit_status, result)\n        self.app.call_from_thread(self.app.action_quit)\n        return exit_status, result\n\n    def execute_action(self, action: dict) -&gt; dict:\n        if self.config.mode == \"human\" and not self._current_action_from_human:  # threading, grrrrr\n            raise NonTerminatingException(\"Command not executed because user switched to manual mode.\")\n        if (\n            self.config.mode == \"confirm\"\n            and action[\"action\"].strip()\n            and not any(re.match(r, action[\"action\"]) for r in self.config.whitelist_actions)\n        ):\n            result = self.app.input_container.request_input(\"Press ENTER to confirm or provide rejection reason\")\n            if result:  # Non-empty string means rejection\n                raise NonTerminatingException(f\"Command not executed: {result}\")\n        return super().execute_action(action)\n\n    def has_finished(self, output: dict[str, str]):\n        try:\n            return super().has_finished(output)\n        except Submitted as e:\n            if self.config.confirm_exit:\n                if new_task := self.app.input_container.request_input(\n                    \"[bold green]Agent wants to finish.[/bold green] \"\n                    \"[green]Type a comment to give it a new task or press enter to quit.\\n\"\n                ).strip():\n                    raise NonTerminatingException(f\"The user added a new task: {new_task}\")\n            raise e\n\n\nclass AddLogEmitCallback(logging.Handler):\n    def __init__(self, callback):\n        \"\"\"Custom log handler that forwards messages via callback.\"\"\"\n        super().__init__()\n        self.callback = callback\n\n    def emit(self, record: logging.LogRecord):\n        self.callback(record)  # type: ignore[attr-defined]\n\n\ndef _messages_to_steps(messages: list[dict]) -&gt; list[list[dict]]:\n    \"\"\"Group messages into \"pages\" as shown by the UI.\"\"\"\n    steps = []\n    current_step = []\n    for message in messages:\n        current_step.append(message)\n        if message[\"role\"] == \"user\":\n            steps.append(current_step)\n            current_step = []\n    if current_step:\n        steps.append(current_step)\n    return steps\n\n\nclass SmartInputContainer(Container):\n    def __init__(self, app: \"TextualAgent\"):\n        \"\"\"Smart input container supporting single-line and multi-line input modes.\"\"\"\n        super().__init__(classes=\"smart-input-container\")\n        self._app = app\n        self._multiline_mode = False\n        self.can_focus = True\n        self.display = False\n\n        self.pending_prompt: str | None = None\n        self._input_event = threading.Event()\n        self._input_result: str | None = None\n\n        self._header_display = Static(id=\"input-header-display\", classes=\"message-header input-request-header\")\n        self._hint_text = Static(classes=\"hint-text\")\n        self._single_input = Input(placeholder=\"Type your input...\")\n        self._multi_input = TextArea(show_line_numbers=False, classes=\"multi-input\")\n        self._input_elements_container = Vertical(\n            self._header_display,\n            self._hint_text,\n            self._single_input,\n            self._multi_input,\n            classes=\"message-container\",\n        )\n\n    def compose(self) -&gt; ComposeResult:\n        yield self._input_elements_container\n\n    def on_mount(self) -&gt; None:\n        \"\"\"Initialize the widget state.\"\"\"\n        self._multi_input.display = False\n        self._update_mode_display()\n\n    def on_focus(self) -&gt; None:\n        \"\"\"Called when the container gains focus.\"\"\"\n        if self._multiline_mode:\n            self._multi_input.focus()\n        else:\n            self._single_input.focus()\n\n    def request_input(self, prompt: str) -&gt; str:\n        \"\"\"Request input from user. Returns input text (empty string if confirmed without reason).\"\"\"\n        self._input_event.clear()\n        self._input_result = None\n        self.pending_prompt = prompt\n        self._header_display.update(prompt)\n        self._update_mode_display()\n        self._app.call_from_thread(self._app.update_content)\n        self._input_event.wait()\n        return self._input_result or \"\"\n\n    def _complete_input(self, input_text: str):\n        \"\"\"Internal method to complete the input process.\"\"\"\n        self._input_result = input_text\n        self.pending_prompt = None\n        self.display = False\n        self._single_input.value = \"\"\n        self._multi_input.text = \"\"\n        self._multiline_mode = False\n        self._update_mode_display()\n        self._app.agent_state = \"RUNNING\"\n        self._app.update_content()\n        # Reset scroll position to bottom since input container disappearing changes layout\n        # somehow scroll_to doesn't work.\n        self._app._vscroll.scroll_y = 0\n        self._input_event.set()\n\n    def action_toggle_mode(self) -&gt; None:\n        \"\"\"Switch from single-line to multi-line mode (one-way only).\"\"\"\n        if self.pending_prompt is None or self._multiline_mode:\n            return\n\n        self._multiline_mode = True\n        self._update_mode_display()\n        self.on_focus()\n\n    def _update_mode_display(self) -&gt; None:\n        \"\"\"Update the display based on current mode.\"\"\"\n        if self._multiline_mode:\n            self._multi_input.text = self._single_input.value\n            self._single_input.display = False\n            self._multi_input.display = True\n            self._hint_text.update(\n                \"[reverse][bold][$accent] Ctrl+D [/][/][/] to submit, [reverse][bold][$accent] Tab [/][/][/] to switch focus with other controls\"\n            )\n        else:\n            self._hint_text.update(\n                \"[reverse][bold][$accent] Enter [/][/][/] to submit, [reverse][bold][$accent] Ctrl+T [/][/][/] to switch to multi-line input, [reverse][bold][$accent] Tab [/][/][/] to switch focus with other controls\",\n            )\n            self._multi_input.display = False\n            self._single_input.display = True\n\n    def on_input_submitted(self, event: Input.Submitted) -&gt; None:\n        \"\"\"Handle single-line input submission.\"\"\"\n        if not self._multiline_mode:\n            text = event.input.value.strip()\n            self._complete_input(text)\n\n    def on_key(self, event: Key) -&gt; None:\n        \"\"\"Handle key events.\"\"\"\n        if event.key == \"ctrl+t\" and not self._multiline_mode:\n            event.prevent_default()\n            self.action_toggle_mode()\n            return\n\n        if self._multiline_mode and event.key == \"ctrl+d\":\n            event.prevent_default()\n            self._complete_input(self._multi_input.text.strip())\n            return\n\n        if event.key == \"escape\":\n            event.prevent_default()\n            self.can_focus = False\n            self._app.set_focus(None)\n            return\n\n\nclass TextualAgent(App):\n    BINDINGS = [\n        Binding(\"right,l\", \"next_step\", \"Step++\", tooltip=\"Show next step of the agent\"),\n        Binding(\"left,h\", \"previous_step\", \"Step--\", tooltip=\"Show previous step of the agent\"),\n        Binding(\"0\", \"first_step\", \"Step=0\", tooltip=\"Show first step of the agent\", show=False),\n        Binding(\"$\", \"last_step\", \"Step=-1\", tooltip=\"Show last step of the agent\", show=False),\n        Binding(\"j,down\", \"scroll_down\", \"Scroll down\", show=False),\n        Binding(\"k,up\", \"scroll_up\", \"Scroll up\", show=False),\n        Binding(\"q,ctrl+q\", \"quit\", \"Quit\", tooltip=\"Quit the agent\"),\n        Binding(\"y,ctrl+y\", \"yolo\", \"YOLO mode\", tooltip=\"Switch to YOLO Mode (LM actions will execute immediately)\"),\n        Binding(\n            \"c\",\n            \"confirm\",\n            \"CONFIRM mode\",\n            tooltip=\"Switch to Confirm Mode (LM proposes commands and you confirm/reject them)\",\n        ),\n        Binding(\"u,ctrl+u\", \"human\", \"HUMAN mode\", tooltip=\"Switch to Human Mode (you can now type commands directly)\"),\n        Binding(\"f1,question_mark\", \"toggle_help_panel\", \"Help\", tooltip=\"Show help\"),\n    ]\n\n    def __init__(self, model, env, **kwargs):\n        css_path = os.environ.get(\"MSWEA_MINI_STYLE_PATH\", str(Path(__file__).parent.parent / \"config\" / \"mini.tcss\"))\n        self.__class__.CSS = Path(css_path).read_text()\n        super().__init__()\n        self.agent_state = \"UNINITIALIZED\"\n        self.agent = _TextualAgent(self, model=model, env=env, **kwargs)\n        self._i_step = 0\n        self.n_steps = 1\n        self.input_container = SmartInputContainer(self)\n        self.log_handler = AddLogEmitCallback(lambda record: self.call_from_thread(self.on_log_message_emitted, record))\n        logging.getLogger().addHandler(self.log_handler)\n        self._spinner = Spinner(\"dots\")\n        self.exit_status: str = \"ExitStatusUnset\"\n        self.result: str = \"\"\n\n        self._vscroll = VerticalScroll()\n\n    def run(self, task: str, **kwargs) -&gt; tuple[str, str]:\n        threading.Thread(target=lambda: self.agent.run(task, **kwargs), daemon=True).start()\n        super().run()\n        return self.exit_status, self.result\n\n    # --- Basics ---\n\n    @property\n    def config(self):\n        return self.agent.config\n\n    @property\n    def i_step(self) -&gt; int:\n        \"\"\"Current step index.\"\"\"\n        return self._i_step\n\n    @i_step.setter\n    def i_step(self, value: int) -&gt; None:\n        \"\"\"Set current step index, automatically clamping to valid bounds.\"\"\"\n        if value != self._i_step:\n            self._i_step = max(0, min(value, self.n_steps - 1))\n            self._vscroll.scroll_to(y=0, animate=False)\n            self.update_content()\n\n    def compose(self) -&gt; ComposeResult:\n        yield Header()\n        with Container(id=\"main\"):\n            with self._vscroll:\n                with Vertical(id=\"content\"):\n                    pass\n                yield self.input_container\n        yield Footer()\n\n    def on_mount(self) -&gt; None:\n        self.agent_state = \"RUNNING\"\n        self.update_content()\n        self.set_interval(1 / 8, self._update_headers)\n\n    @property\n    def messages(self) -&gt; list[dict]:\n        return self.agent.messages\n\n    @property\n    def model(self):\n        return self.agent.model\n\n    @property\n    def env(self):\n        return self.agent.env\n\n    # --- Reacting to events ---\n\n    def on_message_added(self) -&gt; None:\n        auto_follow = self.i_step == self.n_steps - 1 and self._vscroll.scroll_y &lt;= 1\n        self.n_steps = len(_messages_to_steps(self.agent.messages))\n        self.update_content()\n        if auto_follow:\n            self.action_last_step()\n\n    def on_log_message_emitted(self, record: logging.LogRecord) -&gt; None:\n        \"\"\"Handle log messages of warning level or higher by showing them as notifications.\"\"\"\n        if record.levelno &gt;= logging.WARNING:\n            self.notify(f\"[{record.levelname}] {record.getMessage()}\", severity=\"warning\")\n\n    def on_unmount(self) -&gt; None:\n        \"\"\"Clean up the log handler when the app shuts down.\"\"\"\n        if hasattr(self, \"log_handler\"):\n            logging.getLogger().removeHandler(self.log_handler)\n\n    def on_agent_finished(self, exit_status: str, result: str):\n        self.agent_state = \"STOPPED\"\n        self.notify(f\"Agent finished with status: {exit_status}\")\n        self.exit_status = exit_status\n        self.result = result\n        self.update_content()\n\n    # --- UI update logic ---\n\n    def update_content(self) -&gt; None:\n        container = self.query_one(\"#content\", Vertical)\n        container.remove_children()\n        items = _messages_to_steps(self.agent.messages)\n\n        if not items:\n            container.mount(Static(\"Waiting for agent to start...\"))\n            return\n\n        for message in items[self.i_step]:\n            if isinstance(message[\"content\"], list):\n                content_str = \"\\n\".join([item[\"text\"] for item in message[\"content\"]])\n            else:\n                content_str = str(message[\"content\"])\n            message_container = Vertical(classes=\"message-container\")\n            container.mount(message_container)\n            role = message[\"role\"].replace(\"assistant\", \"mini-swe-agent\")\n            message_container.mount(Static(role.upper(), classes=\"message-header\"))\n            message_container.mount(Static(Text(content_str, no_wrap=False), classes=\"message-content\"))\n\n        if self.input_container.pending_prompt is not None:\n            self.agent_state = \"AWAITING_INPUT\"\n        self.input_container.display = self.input_container.pending_prompt is not None and self.i_step == len(items) - 1\n        if self.input_container.display:\n            self.input_container.on_focus()\n\n        self._update_headers()\n        self.refresh()\n\n    def _update_headers(self) -&gt; None:\n        \"\"\"Update just the title with current state and spinner if needed.\"\"\"\n        status_text = self.agent_state\n        if self.agent_state == \"RUNNING\":\n            spinner_frame = str(self._spinner.render(time.time())).strip()\n            status_text = f\"{self.agent_state} {spinner_frame}\"\n        self.title = f\"Step {self.i_step + 1}/{self.n_steps} - {status_text} - Cost: ${self.agent.model.cost:.2f}\"\n        try:\n            self.query_one(\"Header\").set_class(self.agent_state == \"RUNNING\", \"running\")\n        except NoMatches:  # might be called when shutting down\n            pass\n\n    # --- Other textual overrides ---\n\n    def get_system_commands(self, screen: Screen) -&gt; Iterable[SystemCommand]:\n        # Add to palette\n        yield from super().get_system_commands(screen)\n        for binding in self.BINDINGS:\n            description = f\"{binding.description} (shortcut {' OR '.join(binding.key.split(','))})\"  # type: ignore[attr-defined]\n            action_method = getattr(self, f\"action_{binding.action}\")  # type: ignore[attr-defined]\n            yield SystemCommand(description, binding.tooltip, action_method)  # type: ignore[attr-defined]\n\n    # --- Textual bindings ---\n\n    def action_yolo(self):\n        self.agent.config.mode = \"yolo\"\n        if self.input_container.pending_prompt is not None:\n            self.input_container._complete_input(\"\")  # accept\n        self.notify(\"YOLO mode enabled - LM actions will execute immediately\")\n\n    def action_human(self):\n        if self.agent.config.mode == \"confirm\" and self.input_container.pending_prompt is not None:\n            self.input_container._complete_input(\"User switched to manual mode, this command will be ignored\")\n        self.agent.config.mode = \"human\"\n        self.notify(\"Human mode enabled - you can now type commands directly\")\n\n    def action_confirm(self):\n        if self.agent.config.mode == \"human\" and self.input_container.pending_prompt is not None:\n            self.input_container._complete_input(\"\")  # just submit blank action\n        self.agent.config.mode = \"confirm\"\n        self.notify(\"Confirm mode enabled - LM proposes commands and you confirm/reject them\")\n\n    def action_next_step(self) -&gt; None:\n        self.i_step += 1\n\n    def action_previous_step(self) -&gt; None:\n        self.i_step -= 1\n\n    def action_first_step(self) -&gt; None:\n        self.i_step = 0\n\n    def action_last_step(self) -&gt; None:\n        self.i_step = self.n_steps - 1\n\n    def action_scroll_down(self) -&gt; None:\n        self._vscroll.scroll_to(y=self._vscroll.scroll_target_y + 15)\n\n    def action_scroll_up(self) -&gt; None:\n        self._vscroll.scroll_to(y=self._vscroll.scroll_target_y - 15)\n\n    def action_toggle_help_panel(self) -&gt; None:\n        if self.query(\"HelpPanel\"):\n            self.action_hide_help_panel()\n        else:\n            self.action_show_help_panel()\n</code></pre> <p>See also</p> <ul> <li>This agent subclass builds on top of the default agent, make sure to read that first.</li> <li>This class powers the <code>mini</code> command line tool, see usage for more details.</li> </ul> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent","title":"minisweagent.agents.interactive_textual.TextualAgent","text":"<pre><code>TextualAgent(model, env, **kwargs)\n</code></pre> <p>               Bases: <code>App</code></p> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def __init__(self, model, env, **kwargs):\n    css_path = os.environ.get(\"MSWEA_MINI_STYLE_PATH\", str(Path(__file__).parent.parent / \"config\" / \"mini.tcss\"))\n    self.__class__.CSS = Path(css_path).read_text()\n    super().__init__()\n    self.agent_state = \"UNINITIALIZED\"\n    self.agent = _TextualAgent(self, model=model, env=env, **kwargs)\n    self._i_step = 0\n    self.n_steps = 1\n    self.input_container = SmartInputContainer(self)\n    self.log_handler = AddLogEmitCallback(lambda record: self.call_from_thread(self.on_log_message_emitted, record))\n    logging.getLogger().addHandler(self.log_handler)\n    self._spinner = Spinner(\"dots\")\n    self.exit_status: str = \"ExitStatusUnset\"\n    self.result: str = \"\"\n\n    self._vscroll = VerticalScroll()\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.BINDINGS","title":"BINDINGS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BINDINGS = [\n    Binding(\n        \"right,l\",\n        \"next_step\",\n        \"Step++\",\n        tooltip=\"Show next step of the agent\",\n    ),\n    Binding(\n        \"left,h\",\n        \"previous_step\",\n        \"Step--\",\n        tooltip=\"Show previous step of the agent\",\n    ),\n    Binding(\n        \"0\",\n        \"first_step\",\n        \"Step=0\",\n        tooltip=\"Show first step of the agent\",\n        show=False,\n    ),\n    Binding(\n        \"$\",\n        \"last_step\",\n        \"Step=-1\",\n        tooltip=\"Show last step of the agent\",\n        show=False,\n    ),\n    Binding(\n        \"j,down\", \"scroll_down\", \"Scroll down\", show=False\n    ),\n    Binding(\"k,up\", \"scroll_up\", \"Scroll up\", show=False),\n    Binding(\n        \"q,ctrl+q\", \"quit\", \"Quit\", tooltip=\"Quit the agent\"\n    ),\n    Binding(\n        \"y,ctrl+y\",\n        \"yolo\",\n        \"YOLO mode\",\n        tooltip=\"Switch to YOLO Mode (LM actions will execute immediately)\",\n    ),\n    Binding(\n        \"c\",\n        \"confirm\",\n        \"CONFIRM mode\",\n        tooltip=\"Switch to Confirm Mode (LM proposes commands and you confirm/reject them)\",\n    ),\n    Binding(\n        \"u,ctrl+u\",\n        \"human\",\n        \"HUMAN mode\",\n        tooltip=\"Switch to Human Mode (you can now type commands directly)\",\n    ),\n    Binding(\n        \"f1,question_mark\",\n        \"toggle_help_panel\",\n        \"Help\",\n        tooltip=\"Show help\",\n    ),\n]\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.agent_state","title":"agent_state  <code>instance-attribute</code>","text":"<pre><code>agent_state = 'UNINITIALIZED'\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent = _TextualAgent(self, model=model, env=env, **kwargs)\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.n_steps","title":"n_steps  <code>instance-attribute</code>","text":"<pre><code>n_steps = 1\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.input_container","title":"input_container  <code>instance-attribute</code>","text":"<pre><code>input_container = SmartInputContainer(self)\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.log_handler","title":"log_handler  <code>instance-attribute</code>","text":"<pre><code>log_handler = AddLogEmitCallback(\n    lambda record: call_from_thread(\n        on_log_message_emitted, record\n    )\n)\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.exit_status","title":"exit_status  <code>instance-attribute</code>","text":"<pre><code>exit_status: str = 'ExitStatusUnset'\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.result","title":"result  <code>instance-attribute</code>","text":"<pre><code>result: str = ''\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.config","title":"config  <code>property</code>","text":"<pre><code>config\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.i_step","title":"i_step  <code>property</code> <code>writable</code>","text":"<pre><code>i_step: int\n</code></pre> <p>Current step index.</p>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.messages","title":"messages  <code>property</code>","text":"<pre><code>messages: list[dict]\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.model","title":"model  <code>property</code>","text":"<pre><code>model\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.env","title":"env  <code>property</code>","text":"<pre><code>env\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.run","title":"run","text":"<pre><code>run(task: str, **kwargs) -&gt; tuple[str, str]\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def run(self, task: str, **kwargs) -&gt; tuple[str, str]:\n    threading.Thread(target=lambda: self.agent.run(task, **kwargs), daemon=True).start()\n    super().run()\n    return self.exit_status, self.result\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.compose","title":"compose","text":"<pre><code>compose() -&gt; ComposeResult\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def compose(self) -&gt; ComposeResult:\n    yield Header()\n    with Container(id=\"main\"):\n        with self._vscroll:\n            with Vertical(id=\"content\"):\n                pass\n            yield self.input_container\n    yield Footer()\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.on_mount","title":"on_mount","text":"<pre><code>on_mount() -&gt; None\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def on_mount(self) -&gt; None:\n    self.agent_state = \"RUNNING\"\n    self.update_content()\n    self.set_interval(1 / 8, self._update_headers)\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.on_message_added","title":"on_message_added","text":"<pre><code>on_message_added() -&gt; None\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def on_message_added(self) -&gt; None:\n    auto_follow = self.i_step == self.n_steps - 1 and self._vscroll.scroll_y &lt;= 1\n    self.n_steps = len(_messages_to_steps(self.agent.messages))\n    self.update_content()\n    if auto_follow:\n        self.action_last_step()\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.on_log_message_emitted","title":"on_log_message_emitted","text":"<pre><code>on_log_message_emitted(record: LogRecord) -&gt; None\n</code></pre> <p>Handle log messages of warning level or higher by showing them as notifications.</p> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def on_log_message_emitted(self, record: logging.LogRecord) -&gt; None:\n    \"\"\"Handle log messages of warning level or higher by showing them as notifications.\"\"\"\n    if record.levelno &gt;= logging.WARNING:\n        self.notify(f\"[{record.levelname}] {record.getMessage()}\", severity=\"warning\")\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.on_unmount","title":"on_unmount","text":"<pre><code>on_unmount() -&gt; None\n</code></pre> <p>Clean up the log handler when the app shuts down.</p> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def on_unmount(self) -&gt; None:\n    \"\"\"Clean up the log handler when the app shuts down.\"\"\"\n    if hasattr(self, \"log_handler\"):\n        logging.getLogger().removeHandler(self.log_handler)\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.on_agent_finished","title":"on_agent_finished","text":"<pre><code>on_agent_finished(exit_status: str, result: str)\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def on_agent_finished(self, exit_status: str, result: str):\n    self.agent_state = \"STOPPED\"\n    self.notify(f\"Agent finished with status: {exit_status}\")\n    self.exit_status = exit_status\n    self.result = result\n    self.update_content()\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.update_content","title":"update_content","text":"<pre><code>update_content() -&gt; None\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def update_content(self) -&gt; None:\n    container = self.query_one(\"#content\", Vertical)\n    container.remove_children()\n    items = _messages_to_steps(self.agent.messages)\n\n    if not items:\n        container.mount(Static(\"Waiting for agent to start...\"))\n        return\n\n    for message in items[self.i_step]:\n        if isinstance(message[\"content\"], list):\n            content_str = \"\\n\".join([item[\"text\"] for item in message[\"content\"]])\n        else:\n            content_str = str(message[\"content\"])\n        message_container = Vertical(classes=\"message-container\")\n        container.mount(message_container)\n        role = message[\"role\"].replace(\"assistant\", \"mini-swe-agent\")\n        message_container.mount(Static(role.upper(), classes=\"message-header\"))\n        message_container.mount(Static(Text(content_str, no_wrap=False), classes=\"message-content\"))\n\n    if self.input_container.pending_prompt is not None:\n        self.agent_state = \"AWAITING_INPUT\"\n    self.input_container.display = self.input_container.pending_prompt is not None and self.i_step == len(items) - 1\n    if self.input_container.display:\n        self.input_container.on_focus()\n\n    self._update_headers()\n    self.refresh()\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.get_system_commands","title":"get_system_commands","text":"<pre><code>get_system_commands(\n    screen: Screen,\n) -&gt; Iterable[SystemCommand]\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def get_system_commands(self, screen: Screen) -&gt; Iterable[SystemCommand]:\n    # Add to palette\n    yield from super().get_system_commands(screen)\n    for binding in self.BINDINGS:\n        description = f\"{binding.description} (shortcut {' OR '.join(binding.key.split(','))})\"  # type: ignore[attr-defined]\n        action_method = getattr(self, f\"action_{binding.action}\")  # type: ignore[attr-defined]\n        yield SystemCommand(description, binding.tooltip, action_method)  # type: ignore[attr-defined]\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.action_yolo","title":"action_yolo","text":"<pre><code>action_yolo()\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def action_yolo(self):\n    self.agent.config.mode = \"yolo\"\n    if self.input_container.pending_prompt is not None:\n        self.input_container._complete_input(\"\")  # accept\n    self.notify(\"YOLO mode enabled - LM actions will execute immediately\")\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.action_human","title":"action_human","text":"<pre><code>action_human()\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def action_human(self):\n    if self.agent.config.mode == \"confirm\" and self.input_container.pending_prompt is not None:\n        self.input_container._complete_input(\"User switched to manual mode, this command will be ignored\")\n    self.agent.config.mode = \"human\"\n    self.notify(\"Human mode enabled - you can now type commands directly\")\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.action_confirm","title":"action_confirm","text":"<pre><code>action_confirm()\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def action_confirm(self):\n    if self.agent.config.mode == \"human\" and self.input_container.pending_prompt is not None:\n        self.input_container._complete_input(\"\")  # just submit blank action\n    self.agent.config.mode = \"confirm\"\n    self.notify(\"Confirm mode enabled - LM proposes commands and you confirm/reject them\")\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.action_next_step","title":"action_next_step","text":"<pre><code>action_next_step() -&gt; None\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def action_next_step(self) -&gt; None:\n    self.i_step += 1\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.action_previous_step","title":"action_previous_step","text":"<pre><code>action_previous_step() -&gt; None\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def action_previous_step(self) -&gt; None:\n    self.i_step -= 1\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.action_first_step","title":"action_first_step","text":"<pre><code>action_first_step() -&gt; None\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def action_first_step(self) -&gt; None:\n    self.i_step = 0\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.action_last_step","title":"action_last_step","text":"<pre><code>action_last_step() -&gt; None\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def action_last_step(self) -&gt; None:\n    self.i_step = self.n_steps - 1\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.action_scroll_down","title":"action_scroll_down","text":"<pre><code>action_scroll_down() -&gt; None\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def action_scroll_down(self) -&gt; None:\n    self._vscroll.scroll_to(y=self._vscroll.scroll_target_y + 15)\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.action_scroll_up","title":"action_scroll_up","text":"<pre><code>action_scroll_up() -&gt; None\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def action_scroll_up(self) -&gt; None:\n    self._vscroll.scroll_to(y=self._vscroll.scroll_target_y - 15)\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgent.action_toggle_help_panel","title":"action_toggle_help_panel","text":"<pre><code>action_toggle_help_panel() -&gt; None\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def action_toggle_help_panel(self) -&gt; None:\n    if self.query(\"HelpPanel\"):\n        self.action_hide_help_panel()\n    else:\n        self.action_show_help_panel()\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgentConfig","title":"minisweagent.agents.interactive_textual.TextualAgentConfig  <code>dataclass</code>","text":"<pre><code>TextualAgentConfig(\n    system_template: str = \"You are a helpful assistant that can do anything.\",\n    instance_template: str = \"Your task: {{task}}. Please reply with a single shell command in triple backticks. To finish, the first line of the output of the shell command must be 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'.\",\n    timeout_template: str = \"The last command &lt;command&gt;{{action['action']}}&lt;/command&gt; timed out and has been killed.\\nThe output of the command was:\\n &lt;output&gt;\\n{{output}}\\n&lt;/output&gt;\\nPlease try another command and make sure to avoid those requiring interactive input.\",\n    format_error_template: str = \"Please always provide EXACTLY ONE action in triple backticks.\",\n    action_observation_template: str = \"Observation: {{output}}\",\n    step_limit: int = 0,\n    cost_limit: float = 3.0,\n    mode: Literal[\"confirm\", \"yolo\"] = \"confirm\",\n    whitelist_actions: list[str] = list(),\n    confirm_exit: bool = True,\n)\n</code></pre> <p>               Bases: <code>AgentConfig</code></p>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgentConfig.mode","title":"mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mode: Literal['confirm', 'yolo'] = 'confirm'\n</code></pre> <p>Mode for action execution: 'confirm' requires user confirmation, 'yolo' executes immediately.</p>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgentConfig.whitelist_actions","title":"whitelist_actions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>whitelist_actions: list[str] = field(default_factory=list)\n</code></pre> <p>Never confirm actions that match these regular expressions.</p>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.TextualAgentConfig.confirm_exit","title":"confirm_exit  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>confirm_exit: bool = True\n</code></pre> <p>If the agent wants to finish, do we ask for confirmation from user?</p>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.SmartInputContainer","title":"minisweagent.agents.interactive_textual.SmartInputContainer","text":"<pre><code>SmartInputContainer(app: TextualAgent)\n</code></pre> <p>               Bases: <code>Container</code></p> <p>Smart input container supporting single-line and multi-line input modes.</p> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def __init__(self, app: \"TextualAgent\"):\n    \"\"\"Smart input container supporting single-line and multi-line input modes.\"\"\"\n    super().__init__(classes=\"smart-input-container\")\n    self._app = app\n    self._multiline_mode = False\n    self.can_focus = True\n    self.display = False\n\n    self.pending_prompt: str | None = None\n    self._input_event = threading.Event()\n    self._input_result: str | None = None\n\n    self._header_display = Static(id=\"input-header-display\", classes=\"message-header input-request-header\")\n    self._hint_text = Static(classes=\"hint-text\")\n    self._single_input = Input(placeholder=\"Type your input...\")\n    self._multi_input = TextArea(show_line_numbers=False, classes=\"multi-input\")\n    self._input_elements_container = Vertical(\n        self._header_display,\n        self._hint_text,\n        self._single_input,\n        self._multi_input,\n        classes=\"message-container\",\n    )\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.SmartInputContainer.can_focus","title":"can_focus  <code>instance-attribute</code>","text":"<pre><code>can_focus = True\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.SmartInputContainer.display","title":"display  <code>instance-attribute</code>","text":"<pre><code>display = False\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.SmartInputContainer.pending_prompt","title":"pending_prompt  <code>instance-attribute</code>","text":"<pre><code>pending_prompt: str | None = None\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.SmartInputContainer.compose","title":"compose","text":"<pre><code>compose() -&gt; ComposeResult\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def compose(self) -&gt; ComposeResult:\n    yield self._input_elements_container\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.SmartInputContainer.on_mount","title":"on_mount","text":"<pre><code>on_mount() -&gt; None\n</code></pre> <p>Initialize the widget state.</p> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def on_mount(self) -&gt; None:\n    \"\"\"Initialize the widget state.\"\"\"\n    self._multi_input.display = False\n    self._update_mode_display()\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.SmartInputContainer.on_focus","title":"on_focus","text":"<pre><code>on_focus() -&gt; None\n</code></pre> <p>Called when the container gains focus.</p> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def on_focus(self) -&gt; None:\n    \"\"\"Called when the container gains focus.\"\"\"\n    if self._multiline_mode:\n        self._multi_input.focus()\n    else:\n        self._single_input.focus()\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.SmartInputContainer.request_input","title":"request_input","text":"<pre><code>request_input(prompt: str) -&gt; str\n</code></pre> <p>Request input from user. Returns input text (empty string if confirmed without reason).</p> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def request_input(self, prompt: str) -&gt; str:\n    \"\"\"Request input from user. Returns input text (empty string if confirmed without reason).\"\"\"\n    self._input_event.clear()\n    self._input_result = None\n    self.pending_prompt = prompt\n    self._header_display.update(prompt)\n    self._update_mode_display()\n    self._app.call_from_thread(self._app.update_content)\n    self._input_event.wait()\n    return self._input_result or \"\"\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.SmartInputContainer.action_toggle_mode","title":"action_toggle_mode","text":"<pre><code>action_toggle_mode() -&gt; None\n</code></pre> <p>Switch from single-line to multi-line mode (one-way only).</p> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def action_toggle_mode(self) -&gt; None:\n    \"\"\"Switch from single-line to multi-line mode (one-way only).\"\"\"\n    if self.pending_prompt is None or self._multiline_mode:\n        return\n\n    self._multiline_mode = True\n    self._update_mode_display()\n    self.on_focus()\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.SmartInputContainer.on_input_submitted","title":"on_input_submitted","text":"<pre><code>on_input_submitted(event: Submitted) -&gt; None\n</code></pre> <p>Handle single-line input submission.</p> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def on_input_submitted(self, event: Input.Submitted) -&gt; None:\n    \"\"\"Handle single-line input submission.\"\"\"\n    if not self._multiline_mode:\n        text = event.input.value.strip()\n        self._complete_input(text)\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.SmartInputContainer.on_key","title":"on_key","text":"<pre><code>on_key(event: Key) -&gt; None\n</code></pre> <p>Handle key events.</p> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def on_key(self, event: Key) -&gt; None:\n    \"\"\"Handle key events.\"\"\"\n    if event.key == \"ctrl+t\" and not self._multiline_mode:\n        event.prevent_default()\n        self.action_toggle_mode()\n        return\n\n    if self._multiline_mode and event.key == \"ctrl+d\":\n        event.prevent_default()\n        self._complete_input(self._multi_input.text.strip())\n        return\n\n    if event.key == \"escape\":\n        event.prevent_default()\n        self.can_focus = False\n        self._app.set_focus(None)\n        return\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.AddLogEmitCallback","title":"minisweagent.agents.interactive_textual.AddLogEmitCallback","text":"<pre><code>AddLogEmitCallback(callback)\n</code></pre> <p>               Bases: <code>Handler</code></p> <p>Custom log handler that forwards messages via callback.</p> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def __init__(self, callback):\n    \"\"\"Custom log handler that forwards messages via callback.\"\"\"\n    super().__init__()\n    self.callback = callback\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.AddLogEmitCallback.callback","title":"callback  <code>instance-attribute</code>","text":"<pre><code>callback = callback\n</code></pre>"},{"location":"reference/agents/textual/#minisweagent.agents.interactive_textual.AddLogEmitCallback.emit","title":"emit","text":"<pre><code>emit(record: LogRecord)\n</code></pre> Source code in <code>src/minisweagent/agents/interactive_textual.py</code> <pre><code>def emit(self, record: logging.LogRecord):\n    self.callback(record)  # type: ignore[attr-defined]\n</code></pre>"},{"location":"reference/environments/bubblewrap/","title":"BubblewrapEnvironment","text":""},{"location":"reference/environments/bubblewrap/#bubblewrap","title":"Bubblewrap","text":"bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap","title":"minisweagent.environments.extra.bubblewrap","text":"<p>Bubblewrap is a low-level, unprivileged sandboxing tool for Linux that enables running applications in isolated environments with restricted access to the operating system and user data. This environment uses bubblewrap to execute commands in a sandboxed environment.</p> <p>Warning</p> <p>This environment is experimental.</p> <p>Warning</p> <p>This environment is not supported on Windows.</p>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironmentConfig","title":"BubblewrapEnvironmentConfig  <code>dataclass</code>","text":"<pre><code>BubblewrapEnvironmentConfig(\n    cwd: str = \"\",\n    env: dict[str, str] = dict(),\n    timeout: int = 30,\n    executable: str = getenv(\n        \"MSWEA_BUBBLEWRAP_EXECUTABLE\", \"bwrap\"\n    ),\n    wrapper_args: list[str] = (\n        lambda: [\n            \"--unshare-user-try\",\n            \"--ro-bind\",\n            \"/usr\",\n            \"/usr\",\n            \"--ro-bind\",\n            \"/bin\",\n            \"/bin\",\n            \"--ro-bind\",\n            \"/lib\",\n            \"/lib\",\n            \"--ro-bind\",\n            \"/lib64\",\n            \"/lib64\",\n            \"--ro-bind\",\n            \"/etc\",\n            \"/etc\",\n            \"--tmpfs\",\n            \"/tmp\",\n            \"--proc\",\n            \"/proc\",\n            \"--dev\",\n            \"/dev\",\n            \"--new-session\",\n            \"--setenv\",\n            \"PATH\",\n            \"/usr/local/bin:/usr/sbin:/usr/bin:/bin\",\n        ]\n    )(),\n)\n</code></pre>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironmentConfig.cwd","title":"cwd  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cwd: str = ''\n</code></pre> <p>Working directory for the sandbox.</p>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironmentConfig.env","title":"env  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env: dict[str, str] = field(default_factory=dict)\n</code></pre> <p>Dictionary of environment variables to set in the sandbox.</p>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironmentConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: int = 30\n</code></pre> <p>Timeout for the command in seconds.</p>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironmentConfig.executable","title":"executable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>executable: str = getenv(\n    \"MSWEA_BUBBLEWRAP_EXECUTABLE\", \"bwrap\"\n)\n</code></pre> <p>Path to the bubblewrap executable.</p>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironmentConfig.wrapper_args","title":"wrapper_args  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wrapper_args: list[str] = field(\n    default_factory=lambda: [\n        \"--unshare-user-try\",\n        \"--ro-bind\",\n        \"/usr\",\n        \"/usr\",\n        \"--ro-bind\",\n        \"/bin\",\n        \"/bin\",\n        \"--ro-bind\",\n        \"/lib\",\n        \"/lib\",\n        \"--ro-bind\",\n        \"/lib64\",\n        \"/lib64\",\n        \"--ro-bind\",\n        \"/etc\",\n        \"/etc\",\n        \"--tmpfs\",\n        \"/tmp\",\n        \"--proc\",\n        \"/proc\",\n        \"--dev\",\n        \"/dev\",\n        \"--new-session\",\n        \"--setenv\",\n        \"PATH\",\n        \"/usr/local/bin:/usr/sbin:/usr/bin:/bin\",\n    ]\n)\n</code></pre> <p>Arguments to pass to the bubblewrap executable.</p>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironment","title":"BubblewrapEnvironment","text":"<pre><code>BubblewrapEnvironment(\n    *,\n    config_class: type = BubblewrapEnvironmentConfig,\n    logger: Logger | None = None,\n    **kwargs,\n)\n</code></pre> <p>This class executes bash commands in a bubblewrap environment and a separate working directory for each environment. See <code>BubblewrapEnvironmentConfig</code> for kwargs.</p> Source code in <code>src/minisweagent/environments/extra/bubblewrap.py</code> <pre><code>def __init__(\n    self, *, config_class: type = BubblewrapEnvironmentConfig, logger: logging.Logger | None = None, **kwargs\n):\n    \"\"\"This class executes bash commands in a bubblewrap environment and a separate working\n    directory for each environment. See `BubblewrapEnvironmentConfig` for kwargs.\n    \"\"\"\n    self.logger = logger or logging.getLogger(\"minisweagent.environment\")\n    self.config = config_class(**kwargs)\n    self.working_dir = Path(tempfile.gettempdir()) / f\"minisweagent-{uuid.uuid4().hex[:8]}\"\n    self.working_dir.mkdir(parents=True)\n</code></pre>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironment.logger","title":"logger  <code>instance-attribute</code>","text":"<pre><code>logger = logger or getLogger('minisweagent.environment')\n</code></pre>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironment.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config_class(**kwargs)\n</code></pre>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironment.working_dir","title":"working_dir  <code>instance-attribute</code>","text":"<pre><code>working_dir = Path(gettempdir()) / f\"minisweagent-{hex[:8]}\"\n</code></pre>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironment.execute","title":"execute","text":"<pre><code>execute(\n    command: str,\n    cwd: str = \"\",\n    *,\n    timeout: int | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Execute a command in the bubblewrap environment and return the result as a dict.</p> Source code in <code>src/minisweagent/environments/extra/bubblewrap.py</code> <pre><code>def execute(self, command: str, cwd: str = \"\", *, timeout: int | None = None) -&gt; dict[str, Any]:\n    \"\"\"Execute a command in the bubblewrap environment and return the result as a dict.\"\"\"\n    cwd = cwd or self.config.cwd or str(self.working_dir)\n\n    cmd = [self.config.executable] + self.config.wrapper_args + [\"--bind\", cwd, cwd, \"--chdir\", cwd]\n\n    # Add environment variables\n    for key, value in self.config.env.items():\n        cmd.extend([\"--setenv\", key, value])\n\n    cmd.extend([\"bash\", \"-c\", command])\n\n    result = subprocess.run(\n        cmd,\n        text=True,\n        timeout=timeout or self.config.timeout,\n        encoding=\"utf-8\",\n        errors=\"replace\",\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n    )\n    return {\"output\": result.stdout, \"returncode\": result.returncode}\n</code></pre>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironment.cleanup","title":"cleanup","text":"<pre><code>cleanup()\n</code></pre> Source code in <code>src/minisweagent/environments/extra/bubblewrap.py</code> <pre><code>def cleanup(self):\n    if self.working_dir.exists():\n        shutil.rmtree(self.working_dir)\n</code></pre>"},{"location":"reference/environments/bubblewrap/#minisweagent.environments.extra.bubblewrap.BubblewrapEnvironment.get_template_vars","title":"get_template_vars","text":"<pre><code>get_template_vars() -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/minisweagent/environments/extra/bubblewrap.py</code> <pre><code>def get_template_vars(self) -&gt; dict[str, Any]:\n    return asdict(self.config) | platform.uname()._asdict()\n</code></pre>"},{"location":"reference/environments/docker/","title":"DockerEnvironment","text":""},{"location":"reference/environments/docker/#docker","title":"Docker","text":"<p>Docker Environment class</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>import logging\nimport os\nimport shlex\nimport subprocess\nimport uuid\nfrom dataclasses import asdict, dataclass, field\nfrom typing import Any\n\n\n@dataclass\nclass DockerEnvironmentConfig:\n    image: str\n    cwd: str = \"/\"\n    \"\"\"Working directory in which to execute commands.\"\"\"\n    env: dict[str, str] = field(default_factory=dict)\n    \"\"\"Environment variables to set in the container.\"\"\"\n    forward_env: list[str] = field(default_factory=list)\n    \"\"\"Environment variables to forward to the container.\n    Variables are only forwarded if they are set in the host environment.\n    In case of conflict with `env`, the `env` variables take precedence.\n    \"\"\"\n    timeout: int = 30\n    \"\"\"Timeout for executing commands in the container.\"\"\"\n    executable: str = os.getenv(\"MSWEA_DOCKER_EXECUTABLE\", \"docker\")\n    \"\"\"Path to the docker/container executable.\"\"\"\n    run_args: list[str] = field(default_factory=lambda: [\"--rm\"])\n    \"\"\"Additional arguments to pass to the docker/container executable.\n    Default is [\"--rm\"], which removes the container after it exits.\n    \"\"\"\n    container_timeout: str = \"2h\"\n    \"\"\"Max duration to keep container running. Uses the same format as the sleep command.\"\"\"\n    pull_timeout: int = 120\n    \"\"\"Timeout in seconds for pulling images.\"\"\"\n\n\nclass DockerEnvironment:\n    def __init__(self, *, config_class: type = DockerEnvironmentConfig, logger: logging.Logger | None = None, **kwargs):\n        \"\"\"This class executes bash commands in a Docker container using direct docker commands.\n        See `DockerEnvironmentConfig` for keyword arguments.\n        \"\"\"\n        self.logger = logger or logging.getLogger(\"minisweagent.environment\")\n        self.container_id: str | None = None\n        self.config = config_class(**kwargs)\n        self._start_container()\n\n    def get_template_vars(self) -&gt; dict[str, Any]:\n        return asdict(self.config)\n\n    def _start_container(self):\n        \"\"\"Start the Docker container and return the container ID.\"\"\"\n        container_name = f\"minisweagent-{uuid.uuid4().hex[:8]}\"\n        cmd = [\n            self.config.executable,\n            \"run\",\n            \"-d\",\n            \"--name\",\n            container_name,\n            \"-w\",\n            self.config.cwd,\n            *self.config.run_args,\n            self.config.image,\n            \"sleep\",\n            self.config.container_timeout,\n        ]\n        self.logger.debug(f\"Starting container with command: {shlex.join(cmd)}\")\n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            timeout=self.config.pull_timeout,  # docker pull might take a while\n            check=True,\n        )\n        self.logger.info(f\"Started container {container_name} with ID {result.stdout.strip()}\")\n        self.container_id = result.stdout.strip()\n\n    def execute(self, command: str, cwd: str = \"\", *, timeout: int | None = None) -&gt; dict[str, Any]:\n        \"\"\"Execute a command in the Docker container and return the result as a dict.\"\"\"\n        cwd = cwd or self.config.cwd\n        assert self.container_id, \"Container not started\"\n\n        cmd = [self.config.executable, \"exec\", \"-w\", cwd]\n        for key in self.config.forward_env:\n            if (value := os.getenv(key)) is not None:\n                cmd.extend([\"-e\", f\"{key}={value}\"])\n        for key, value in self.config.env.items():\n            cmd.extend([\"-e\", f\"{key}={value}\"])\n        cmd.extend([self.container_id, \"bash\", \"-lc\", command])\n\n        result = subprocess.run(\n            cmd,\n            text=True,\n            timeout=timeout or self.config.timeout,\n            encoding=\"utf-8\",\n            errors=\"replace\",\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n        )\n        return {\"output\": result.stdout, \"returncode\": result.returncode}\n\n    def cleanup(self):\n        \"\"\"Stop and remove the Docker container.\"\"\"\n        if getattr(self, \"container_id\", None) is not None:  # if init fails early, container_id might not be set\n            cmd = f\"(timeout 60 {self.config.executable} stop {self.container_id} || {self.config.executable} rm -f {self.container_id}) &gt;/dev/null 2&gt;&amp;1 &amp;\"\n            subprocess.Popen(cmd, shell=True)\n\n    def __del__(self):\n        \"\"\"Cleanup container when object is destroyed.\"\"\"\n        self.cleanup()\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/environments/docker/#minisweagent.environments.docker","title":"minisweagent.environments.docker","text":""},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironmentConfig","title":"DockerEnvironmentConfig  <code>dataclass</code>","text":"<pre><code>DockerEnvironmentConfig(\n    image: str,\n    cwd: str = \"/\",\n    env: dict[str, str] = dict(),\n    forward_env: list[str] = list(),\n    timeout: int = 30,\n    executable: str = getenv(\n        \"MSWEA_DOCKER_EXECUTABLE\", \"docker\"\n    ),\n    run_args: list[str] = (lambda: [\"--rm\"])(),\n    container_timeout: str = \"2h\",\n    pull_timeout: int = 120,\n)\n</code></pre>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironmentConfig.image","title":"image  <code>instance-attribute</code>","text":"<pre><code>image: str\n</code></pre>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironmentConfig.cwd","title":"cwd  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cwd: str = '/'\n</code></pre> <p>Working directory in which to execute commands.</p>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironmentConfig.env","title":"env  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env: dict[str, str] = field(default_factory=dict)\n</code></pre> <p>Environment variables to set in the container.</p>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironmentConfig.forward_env","title":"forward_env  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>forward_env: list[str] = field(default_factory=list)\n</code></pre> <p>Environment variables to forward to the container. Variables are only forwarded if they are set in the host environment. In case of conflict with <code>env</code>, the <code>env</code> variables take precedence.</p>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironmentConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: int = 30\n</code></pre> <p>Timeout for executing commands in the container.</p>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironmentConfig.executable","title":"executable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>executable: str = getenv(\n    \"MSWEA_DOCKER_EXECUTABLE\", \"docker\"\n)\n</code></pre> <p>Path to the docker/container executable.</p>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironmentConfig.run_args","title":"run_args  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>run_args: list[str] = field(\n    default_factory=lambda: [\"--rm\"]\n)\n</code></pre> <p>Additional arguments to pass to the docker/container executable. Default is [\"--rm\"], which removes the container after it exits.</p>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironmentConfig.container_timeout","title":"container_timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>container_timeout: str = '2h'\n</code></pre> <p>Max duration to keep container running. Uses the same format as the sleep command.</p>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironmentConfig.pull_timeout","title":"pull_timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pull_timeout: int = 120\n</code></pre> <p>Timeout in seconds for pulling images.</p>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironment","title":"DockerEnvironment","text":"<pre><code>DockerEnvironment(\n    *,\n    config_class: type = DockerEnvironmentConfig,\n    logger: Logger | None = None,\n    **kwargs,\n)\n</code></pre> <p>This class executes bash commands in a Docker container using direct docker commands. See <code>DockerEnvironmentConfig</code> for keyword arguments.</p> Source code in <code>src/minisweagent/environments/docker.py</code> <pre><code>def __init__(self, *, config_class: type = DockerEnvironmentConfig, logger: logging.Logger | None = None, **kwargs):\n    \"\"\"This class executes bash commands in a Docker container using direct docker commands.\n    See `DockerEnvironmentConfig` for keyword arguments.\n    \"\"\"\n    self.logger = logger or logging.getLogger(\"minisweagent.environment\")\n    self.container_id: str | None = None\n    self.config = config_class(**kwargs)\n    self._start_container()\n</code></pre>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironment.logger","title":"logger  <code>instance-attribute</code>","text":"<pre><code>logger = logger or getLogger('minisweagent.environment')\n</code></pre>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironment.container_id","title":"container_id  <code>instance-attribute</code>","text":"<pre><code>container_id: str | None = None\n</code></pre>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironment.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config_class(**kwargs)\n</code></pre>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironment.get_template_vars","title":"get_template_vars","text":"<pre><code>get_template_vars() -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/minisweagent/environments/docker.py</code> <pre><code>def get_template_vars(self) -&gt; dict[str, Any]:\n    return asdict(self.config)\n</code></pre>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironment.execute","title":"execute","text":"<pre><code>execute(\n    command: str,\n    cwd: str = \"\",\n    *,\n    timeout: int | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Execute a command in the Docker container and return the result as a dict.</p> Source code in <code>src/minisweagent/environments/docker.py</code> <pre><code>def execute(self, command: str, cwd: str = \"\", *, timeout: int | None = None) -&gt; dict[str, Any]:\n    \"\"\"Execute a command in the Docker container and return the result as a dict.\"\"\"\n    cwd = cwd or self.config.cwd\n    assert self.container_id, \"Container not started\"\n\n    cmd = [self.config.executable, \"exec\", \"-w\", cwd]\n    for key in self.config.forward_env:\n        if (value := os.getenv(key)) is not None:\n            cmd.extend([\"-e\", f\"{key}={value}\"])\n    for key, value in self.config.env.items():\n        cmd.extend([\"-e\", f\"{key}={value}\"])\n    cmd.extend([self.container_id, \"bash\", \"-lc\", command])\n\n    result = subprocess.run(\n        cmd,\n        text=True,\n        timeout=timeout or self.config.timeout,\n        encoding=\"utf-8\",\n        errors=\"replace\",\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n    )\n    return {\"output\": result.stdout, \"returncode\": result.returncode}\n</code></pre>"},{"location":"reference/environments/docker/#minisweagent.environments.docker.DockerEnvironment.cleanup","title":"cleanup","text":"<pre><code>cleanup()\n</code></pre> <p>Stop and remove the Docker container.</p> Source code in <code>src/minisweagent/environments/docker.py</code> <pre><code>def cleanup(self):\n    \"\"\"Stop and remove the Docker container.\"\"\"\n    if getattr(self, \"container_id\", None) is not None:  # if init fails early, container_id might not be set\n        cmd = f\"(timeout 60 {self.config.executable} stop {self.container_id} || {self.config.executable} rm -f {self.container_id}) &gt;/dev/null 2&gt;&amp;1 &amp;\"\n        subprocess.Popen(cmd, shell=True)\n</code></pre>"},{"location":"reference/environments/local/","title":"LocalEnvironment","text":""},{"location":"reference/environments/local/#local","title":"Local","text":"<p>Local Environment class</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>import os\nimport platform\nimport subprocess\nfrom dataclasses import asdict, dataclass, field\nfrom typing import Any\n\n\n@dataclass\nclass LocalEnvironmentConfig:\n    cwd: str = \"\"\n    env: dict[str, str] = field(default_factory=dict)\n    timeout: int = 30\n\n\nclass LocalEnvironment:\n    def __init__(self, *, config_class: type = LocalEnvironmentConfig, **kwargs):\n        \"\"\"This class executes bash commands directly on the local machine.\"\"\"\n        self.config = config_class(**kwargs)\n\n    def execute(self, command: str, cwd: str = \"\", *, timeout: int | None = None):\n        \"\"\"Execute a command in the local environment and return the result as a dict.\"\"\"\n        cwd = cwd or self.config.cwd or os.getcwd()\n        result = subprocess.run(\n            command,\n            shell=True,\n            text=True,\n            cwd=cwd,\n            env=os.environ | self.config.env,\n            timeout=timeout or self.config.timeout,\n            encoding=\"utf-8\",\n            errors=\"replace\",\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n        )\n        return {\"output\": result.stdout, \"returncode\": result.returncode}\n\n    def get_template_vars(self) -&gt; dict[str, Any]:\n        return asdict(self.config) | platform.uname()._asdict() | os.environ\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/environments/local/#minisweagent.environments.local","title":"minisweagent.environments.local","text":""},{"location":"reference/environments/local/#minisweagent.environments.local.LocalEnvironmentConfig","title":"LocalEnvironmentConfig  <code>dataclass</code>","text":"<pre><code>LocalEnvironmentConfig(\n    cwd: str = \"\",\n    env: dict[str, str] = dict(),\n    timeout: int = 30,\n)\n</code></pre>"},{"location":"reference/environments/local/#minisweagent.environments.local.LocalEnvironmentConfig.cwd","title":"cwd  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cwd: str = ''\n</code></pre>"},{"location":"reference/environments/local/#minisweagent.environments.local.LocalEnvironmentConfig.env","title":"env  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env: dict[str, str] = field(default_factory=dict)\n</code></pre>"},{"location":"reference/environments/local/#minisweagent.environments.local.LocalEnvironmentConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: int = 30\n</code></pre>"},{"location":"reference/environments/local/#minisweagent.environments.local.LocalEnvironment","title":"LocalEnvironment","text":"<pre><code>LocalEnvironment(\n    *, config_class: type = LocalEnvironmentConfig, **kwargs\n)\n</code></pre> <p>This class executes bash commands directly on the local machine.</p> Source code in <code>src/minisweagent/environments/local.py</code> <pre><code>def __init__(self, *, config_class: type = LocalEnvironmentConfig, **kwargs):\n    \"\"\"This class executes bash commands directly on the local machine.\"\"\"\n    self.config = config_class(**kwargs)\n</code></pre>"},{"location":"reference/environments/local/#minisweagent.environments.local.LocalEnvironment.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config_class(**kwargs)\n</code></pre>"},{"location":"reference/environments/local/#minisweagent.environments.local.LocalEnvironment.execute","title":"execute","text":"<pre><code>execute(\n    command: str,\n    cwd: str = \"\",\n    *,\n    timeout: int | None = None,\n)\n</code></pre> <p>Execute a command in the local environment and return the result as a dict.</p> Source code in <code>src/minisweagent/environments/local.py</code> <pre><code>def execute(self, command: str, cwd: str = \"\", *, timeout: int | None = None):\n    \"\"\"Execute a command in the local environment and return the result as a dict.\"\"\"\n    cwd = cwd or self.config.cwd or os.getcwd()\n    result = subprocess.run(\n        command,\n        shell=True,\n        text=True,\n        cwd=cwd,\n        env=os.environ | self.config.env,\n        timeout=timeout or self.config.timeout,\n        encoding=\"utf-8\",\n        errors=\"replace\",\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n    )\n    return {\"output\": result.stdout, \"returncode\": result.returncode}\n</code></pre>"},{"location":"reference/environments/local/#minisweagent.environments.local.LocalEnvironment.get_template_vars","title":"get_template_vars","text":"<pre><code>get_template_vars() -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/minisweagent/environments/local.py</code> <pre><code>def get_template_vars(self) -&gt; dict[str, Any]:\n    return asdict(self.config) | platform.uname()._asdict() | os.environ\n</code></pre>"},{"location":"reference/environments/singularity/","title":"SingularityEnvironment","text":""},{"location":"reference/environments/singularity/#singularity","title":"Singularity","text":"<p>Singularity Environment class</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>#!/usr/bin/env python3\n\nimport logging\nimport os\nimport shutil\nimport subprocess\nimport tempfile\nimport uuid\nfrom dataclasses import asdict, dataclass, field\nfrom pathlib import Path\nfrom typing import Any\n\n\n@dataclass\nclass SingularityEnvironmentConfig:\n    image: str\n    cwd: str = \"/\"\n    env: dict[str, str] = field(default_factory=dict)\n    \"\"\"Environment variables to set in the container.\"\"\"\n    forward_env: list[str] = field(default_factory=list)\n    \"\"\"Environment variables to forward to the container.\"\"\"\n    timeout: int = 30\n    \"\"\"Timeout for executing commands in the container.\"\"\"\n    executable: str = os.getenv(\"MSWEA_SINGULARITY_EXECUTABLE\", \"singularity\")\n    \"\"\"Path to the singularity executable.\"\"\"\n    sandbox_build_retries: int = 3\n    \"\"\"Number of retries for building the sandbox if an error occurs.\"\"\"\n\n\nclass SingularityEnvironment:\n    def __init__(\n        self, *, config_class: type = SingularityEnvironmentConfig, logger: logging.Logger | None = None, **kwargs\n    ):\n        \"\"\"Singularity environment. See `SingularityEnvironmentConfig` for kwargs.\"\"\"\n        self.logger = logger or logging.getLogger(\"minisweagent.environment\")\n        self.config = config_class(**kwargs)\n        self.sandbox_dir = self._build_sandbox()\n\n    def _build_sandbox(self) -&gt; Path:\n        # Building the sandbox can fail (very rarely), so we retry it\n        max_retries = self.config.sandbox_build_retries\n        for attempt in range(max_retries):\n            sandbox_dir = Path(tempfile.gettempdir()) / f\"minisweagent-{uuid.uuid4().hex[:8]}\"\n            try:\n                subprocess.run(\n                    [self.config.executable, \"build\", \"--sandbox\", sandbox_dir, self.config.image],\n                    check=True,\n                    capture_output=True,\n                )\n                break\n            except subprocess.CalledProcessError as e:\n                shutil.rmtree(sandbox_dir, ignore_errors=True)\n                self.logger.error(\n                    f\"Error building image {self.config.image}, stdout: {e.stdout}, stderr: {e.stderr} (attempt {attempt + 1}/{max_retries})\"\n                )\n                if attempt == max_retries - 1:\n                    raise\n        return sandbox_dir\n\n    def get_template_vars(self) -&gt; dict[str, Any]:\n        return asdict(self.config)\n\n    def execute(self, command: str, cwd: str = \"\", *, timeout: int | None = None) -&gt; dict[str, Any]:\n        \"\"\"Execute a command in a Singularity container and return the result as a dict.\"\"\"\n        cmd = [self.config.executable, \"exec\"]\n\n        # Do not inherit directories and env vars from host\n        cmd.extend([\"--contain\", \"--cleanenv\"])\n\n        work_dir = cwd or self.config.cwd\n        if work_dir and work_dir != \"/\":\n            cmd.extend([\"--pwd\", work_dir])\n\n        for key in self.config.forward_env:\n            if (value := os.getenv(key)) is not None:\n                cmd.extend([\"--env\", f\"{key}={value}\"])\n        for key, value in self.config.env.items():\n            cmd.extend([\"--env\", f\"{key}={value}\"])\n\n        cmd.extend([\"--writable\", str(self.sandbox_dir), \"bash\", \"-c\", command])\n        result = subprocess.run(\n            cmd,\n            text=True,\n            timeout=timeout or self.config.timeout,\n            encoding=\"utf-8\",\n            errors=\"replace\",\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n        )\n        return {\"output\": result.stdout, \"returncode\": result.returncode}\n\n    def cleanup(self):\n        shutil.rmtree(self.sandbox_dir, ignore_errors=True)\n\n    def __del__(self):\n        \"\"\"Cleanup sandbox when object is destroyed.\"\"\"\n        self.cleanup()\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity","title":"minisweagent.environments.singularity","text":""},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironmentConfig","title":"SingularityEnvironmentConfig  <code>dataclass</code>","text":"<pre><code>SingularityEnvironmentConfig(\n    image: str,\n    cwd: str = \"/\",\n    env: dict[str, str] = dict(),\n    forward_env: list[str] = list(),\n    timeout: int = 30,\n    executable: str = getenv(\n        \"MSWEA_SINGULARITY_EXECUTABLE\", \"singularity\"\n    ),\n    sandbox_build_retries: int = 3,\n)\n</code></pre>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironmentConfig.image","title":"image  <code>instance-attribute</code>","text":"<pre><code>image: str\n</code></pre>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironmentConfig.cwd","title":"cwd  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cwd: str = '/'\n</code></pre>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironmentConfig.env","title":"env  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env: dict[str, str] = field(default_factory=dict)\n</code></pre> <p>Environment variables to set in the container.</p>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironmentConfig.forward_env","title":"forward_env  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>forward_env: list[str] = field(default_factory=list)\n</code></pre> <p>Environment variables to forward to the container.</p>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironmentConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: int = 30\n</code></pre> <p>Timeout for executing commands in the container.</p>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironmentConfig.executable","title":"executable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>executable: str = getenv(\n    \"MSWEA_SINGULARITY_EXECUTABLE\", \"singularity\"\n)\n</code></pre> <p>Path to the singularity executable.</p>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironmentConfig.sandbox_build_retries","title":"sandbox_build_retries  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sandbox_build_retries: int = 3\n</code></pre> <p>Number of retries for building the sandbox if an error occurs.</p>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironment","title":"SingularityEnvironment","text":"<pre><code>SingularityEnvironment(\n    *,\n    config_class: type = SingularityEnvironmentConfig,\n    logger: Logger | None = None,\n    **kwargs,\n)\n</code></pre> <p>Singularity environment. See <code>SingularityEnvironmentConfig</code> for kwargs.</p> Source code in <code>src/minisweagent/environments/singularity.py</code> <pre><code>def __init__(\n    self, *, config_class: type = SingularityEnvironmentConfig, logger: logging.Logger | None = None, **kwargs\n):\n    \"\"\"Singularity environment. See `SingularityEnvironmentConfig` for kwargs.\"\"\"\n    self.logger = logger or logging.getLogger(\"minisweagent.environment\")\n    self.config = config_class(**kwargs)\n    self.sandbox_dir = self._build_sandbox()\n</code></pre>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironment.logger","title":"logger  <code>instance-attribute</code>","text":"<pre><code>logger = logger or getLogger('minisweagent.environment')\n</code></pre>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironment.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config_class(**kwargs)\n</code></pre>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironment.sandbox_dir","title":"sandbox_dir  <code>instance-attribute</code>","text":"<pre><code>sandbox_dir = _build_sandbox()\n</code></pre>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironment.get_template_vars","title":"get_template_vars","text":"<pre><code>get_template_vars() -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/minisweagent/environments/singularity.py</code> <pre><code>def get_template_vars(self) -&gt; dict[str, Any]:\n    return asdict(self.config)\n</code></pre>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironment.execute","title":"execute","text":"<pre><code>execute(\n    command: str,\n    cwd: str = \"\",\n    *,\n    timeout: int | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Execute a command in a Singularity container and return the result as a dict.</p> Source code in <code>src/minisweagent/environments/singularity.py</code> <pre><code>def execute(self, command: str, cwd: str = \"\", *, timeout: int | None = None) -&gt; dict[str, Any]:\n    \"\"\"Execute a command in a Singularity container and return the result as a dict.\"\"\"\n    cmd = [self.config.executable, \"exec\"]\n\n    # Do not inherit directories and env vars from host\n    cmd.extend([\"--contain\", \"--cleanenv\"])\n\n    work_dir = cwd or self.config.cwd\n    if work_dir and work_dir != \"/\":\n        cmd.extend([\"--pwd\", work_dir])\n\n    for key in self.config.forward_env:\n        if (value := os.getenv(key)) is not None:\n            cmd.extend([\"--env\", f\"{key}={value}\"])\n    for key, value in self.config.env.items():\n        cmd.extend([\"--env\", f\"{key}={value}\"])\n\n    cmd.extend([\"--writable\", str(self.sandbox_dir), \"bash\", \"-c\", command])\n    result = subprocess.run(\n        cmd,\n        text=True,\n        timeout=timeout or self.config.timeout,\n        encoding=\"utf-8\",\n        errors=\"replace\",\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n    )\n    return {\"output\": result.stdout, \"returncode\": result.returncode}\n</code></pre>"},{"location":"reference/environments/singularity/#minisweagent.environments.singularity.SingularityEnvironment.cleanup","title":"cleanup","text":"<pre><code>cleanup()\n</code></pre> Source code in <code>src/minisweagent/environments/singularity.py</code> <pre><code>def cleanup(self):\n    shutil.rmtree(self.sandbox_dir, ignore_errors=True)\n</code></pre>"},{"location":"reference/environments/swerex_docker/","title":"SwerexDockerEnvironment","text":""},{"location":"reference/environments/swerex_docker/#swe-rex-docker","title":"SWE-rex Docker","text":"<p>SWE-rex Docker Environment class</p> <ul> <li>Read on GitHub</li> </ul> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/environments/swerex_docker/#minisweagent.environments.extra.swerex_docker","title":"minisweagent.environments.extra.swerex_docker","text":""},{"location":"reference/environments/swerex_docker/#minisweagent.environments.extra.swerex_docker.SwerexDockerEnvironmentConfig","title":"SwerexDockerEnvironmentConfig  <code>dataclass</code>","text":"<pre><code>SwerexDockerEnvironmentConfig(\n    image: str,\n    cwd: str = \"/\",\n    timeout: int = 30,\n    deployment_extra_kwargs: dict[str, Any] = dict(),\n)\n</code></pre>"},{"location":"reference/environments/swerex_docker/#minisweagent.environments.extra.swerex_docker.SwerexDockerEnvironmentConfig.image","title":"image  <code>instance-attribute</code>","text":"<pre><code>image: str\n</code></pre>"},{"location":"reference/environments/swerex_docker/#minisweagent.environments.extra.swerex_docker.SwerexDockerEnvironmentConfig.cwd","title":"cwd  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cwd: str = '/'\n</code></pre> <p>Working directory in which to execute commands.</p>"},{"location":"reference/environments/swerex_docker/#minisweagent.environments.extra.swerex_docker.SwerexDockerEnvironmentConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: int = 30\n</code></pre> <p>Timeout for executing commands in the container.</p>"},{"location":"reference/environments/swerex_docker/#minisweagent.environments.extra.swerex_docker.SwerexDockerEnvironmentConfig.deployment_extra_kwargs","title":"deployment_extra_kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>deployment_extra_kwargs: dict[str, Any] = field(\n    default_factory=dict\n)\n</code></pre> <p>Extra kwargs to pass to DockerDeployment.</p>"},{"location":"reference/environments/swerex_docker/#minisweagent.environments.extra.swerex_docker.SwerexDockerEnvironment","title":"SwerexDockerEnvironment","text":"<pre><code>SwerexDockerEnvironment(**kwargs)\n</code></pre> <p>This class executes bash commands in a Docker container using SWE-ReX for sandboxing.</p> Source code in <code>src/minisweagent/environments/extra/swerex_docker.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"This class executes bash commands in a Docker container using SWE-ReX for sandboxing.\"\"\"\n    self.config = SwerexDockerEnvironmentConfig(**kwargs)\n    self.deployment = DockerDeployment(image=self.config.image, **self.config.deployment_extra_kwargs)\n    asyncio.run(self.deployment.start())\n</code></pre>"},{"location":"reference/environments/swerex_docker/#minisweagent.environments.extra.swerex_docker.SwerexDockerEnvironment.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = SwerexDockerEnvironmentConfig(**kwargs)\n</code></pre>"},{"location":"reference/environments/swerex_docker/#minisweagent.environments.extra.swerex_docker.SwerexDockerEnvironment.deployment","title":"deployment  <code>instance-attribute</code>","text":"<pre><code>deployment = DockerDeployment(\n    image=image, **(deployment_extra_kwargs)\n)\n</code></pre>"},{"location":"reference/environments/swerex_docker/#minisweagent.environments.extra.swerex_docker.SwerexDockerEnvironment.execute","title":"execute","text":"<pre><code>execute(\n    command: str,\n    cwd: str = \"\",\n    *,\n    timeout: int | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Execute a command in the environment and return the raw output.</p> Source code in <code>src/minisweagent/environments/extra/swerex_docker.py</code> <pre><code>def execute(self, command: str, cwd: str = \"\", *, timeout: int | None = None) -&gt; dict[str, Any]:\n    \"\"\"Execute a command in the environment and return the raw output.\"\"\"\n    output = asyncio.run(\n        self.deployment.runtime.execute(\n            RexCommand(\n                command=command,\n                shell=True,\n                check=False,\n                cwd=cwd or self.config.cwd,\n                timeout=timeout or self.config.timeout,\n                merge_output_streams=True,\n            )\n        )\n    )\n    return {\n        \"output\": output.stdout,\n        \"returncode\": output.exit_code,\n    }\n</code></pre>"},{"location":"reference/environments/swerex_docker/#minisweagent.environments.extra.swerex_docker.SwerexDockerEnvironment.get_template_vars","title":"get_template_vars","text":"<pre><code>get_template_vars() -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/minisweagent/environments/extra/swerex_docker.py</code> <pre><code>def get_template_vars(self) -&gt; dict[str, Any]:\n    return asdict(self.config)\n</code></pre>"},{"location":"reference/models/anthropic/","title":"AnthropicModel","text":""},{"location":"reference/models/anthropic/#anthropic","title":"Anthropic","text":"<p>Anthropic Model class</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>import os\nimport warnings\nfrom typing import Literal\n\nfrom minisweagent.models.litellm_model import LitellmModel, LitellmModelConfig\nfrom minisweagent.models.utils.cache_control import set_cache_control\nfrom minisweagent.models.utils.key_per_thread import get_key_per_thread\n\n\nclass AnthropicModelConfig(LitellmModelConfig):\n    set_cache_control: Literal[\"default_end\"] | None = \"default_end\"\n    \"\"\"Set explicit cache control markers, for example for Anthropic models\"\"\"\n\n\nclass AnthropicModel(LitellmModel):\n    \"\"\"This class is now only a thin wrapper around the LitellmModel class.\n    It is largely kept for backwards compatibility.\n    It will not be selected by `get_model` and `get_model_class` unless explicitly specified.\n    \"\"\"\n\n    def __init__(self, *, config_class: type = AnthropicModelConfig, **kwargs):\n        super().__init__(config_class=config_class, **kwargs)\n\n    def query(self, messages: list[dict], **kwargs) -&gt; dict:\n        api_key = None\n        # Legacy only\n        if rotating_keys := os.getenv(\"ANTHROPIC_API_KEYS\"):\n            warnings.warn(\n                \"ANTHROPIC_API_KEYS is deprecated and will be removed in the future. \"\n                \"Simply use the ANTHROPIC_API_KEY environment variable instead. \"\n                \"Key rotation is no longer required.\"\n            )\n            api_key = get_key_per_thread(rotating_keys.split(\"::\"))\n        messages = set_cache_control(messages, mode=\"default_end\")\n        return super().query(messages, api_key=api_key, **kwargs)\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/models/anthropic/#minisweagent.models.anthropic","title":"minisweagent.models.anthropic","text":""},{"location":"reference/models/anthropic/#minisweagent.models.anthropic.AnthropicModelConfig","title":"AnthropicModelConfig  <code>dataclass</code>","text":"<pre><code>AnthropicModelConfig(\n    model_name: str,\n    model_kwargs: dict[str, Any] = dict(),\n    litellm_model_registry: Path | str | None = getenv(\n        \"LITELLM_MODEL_REGISTRY_PATH\"\n    ),\n    set_cache_control: Literal[\"default_end\"] | None = None,\n)\n</code></pre> <p>               Bases: <code>LitellmModelConfig</code></p>"},{"location":"reference/models/anthropic/#minisweagent.models.anthropic.AnthropicModelConfig.set_cache_control","title":"set_cache_control  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>set_cache_control: Literal[\"default_end\"] | None = (\n    \"default_end\"\n)\n</code></pre> <p>Set explicit cache control markers, for example for Anthropic models</p>"},{"location":"reference/models/anthropic/#minisweagent.models.anthropic.AnthropicModel","title":"AnthropicModel","text":"<pre><code>AnthropicModel(\n    *, config_class: type = AnthropicModelConfig, **kwargs\n)\n</code></pre> <p>               Bases: <code>LitellmModel</code></p> <p>This class is now only a thin wrapper around the LitellmModel class. It is largely kept for backwards compatibility. It will not be selected by <code>get_model</code> and <code>get_model_class</code> unless explicitly specified.</p> Source code in <code>src/minisweagent/models/anthropic.py</code> <pre><code>def __init__(self, *, config_class: type = AnthropicModelConfig, **kwargs):\n    super().__init__(config_class=config_class, **kwargs)\n</code></pre>"},{"location":"reference/models/anthropic/#minisweagent.models.anthropic.AnthropicModel.query","title":"query","text":"<pre><code>query(messages: list[dict], **kwargs) -&gt; dict\n</code></pre> Source code in <code>src/minisweagent/models/anthropic.py</code> <pre><code>def query(self, messages: list[dict], **kwargs) -&gt; dict:\n    api_key = None\n    # Legacy only\n    if rotating_keys := os.getenv(\"ANTHROPIC_API_KEYS\"):\n        warnings.warn(\n            \"ANTHROPIC_API_KEYS is deprecated and will be removed in the future. \"\n            \"Simply use the ANTHROPIC_API_KEY environment variable instead. \"\n            \"Key rotation is no longer required.\"\n        )\n        api_key = get_key_per_thread(rotating_keys.split(\"::\"))\n    messages = set_cache_control(messages, mode=\"default_end\")\n    return super().query(messages, api_key=api_key, **kwargs)\n</code></pre>"},{"location":"reference/models/extra/","title":"Extra Models","text":""},{"location":"reference/models/extra/#extra-models","title":"Extra Models","text":"<p>Extra Models</p> <ul> <li>Read roulette.py on GitHub</li> </ul> <p>These are advanced \"meta-models\" that combine or modify the behavior of other models.</p> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette","title":"minisweagent.models.extra.roulette","text":""},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.RouletteModelConfig","title":"RouletteModelConfig  <code>dataclass</code>","text":"<pre><code>RouletteModelConfig(\n    model_kwargs: list[dict], model_name: str = \"roulette\"\n)\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.RouletteModelConfig.model_kwargs","title":"model_kwargs  <code>instance-attribute</code>","text":"<pre><code>model_kwargs: list[dict]\n</code></pre> <p>The models to choose from</p>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.RouletteModelConfig.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str = 'roulette'\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.RouletteModel","title":"RouletteModel","text":"<pre><code>RouletteModel(\n    *,\n    config_class: Callable = RouletteModelConfig,\n    **kwargs,\n)\n</code></pre> <p>This \"meta\"-model randomly selects one of the models at every call</p> Source code in <code>src/minisweagent/models/extra/roulette.py</code> <pre><code>def __init__(self, *, config_class: Callable = RouletteModelConfig, **kwargs):\n    \"\"\"This \"meta\"-model randomly selects one of the models at every call\"\"\"\n    self.config = config_class(**kwargs)\n    self.models = [get_model(config=config) for config in self.config.model_kwargs]\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.RouletteModel.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config_class(**kwargs)\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.RouletteModel.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models = [\n    (get_model(config=config)) for config in (model_kwargs)\n]\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.RouletteModel.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.RouletteModel.n_calls","title":"n_calls  <code>property</code>","text":"<pre><code>n_calls: int\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.RouletteModel.get_template_vars","title":"get_template_vars","text":"<pre><code>get_template_vars() -&gt; dict\n</code></pre> Source code in <code>src/minisweagent/models/extra/roulette.py</code> <pre><code>def get_template_vars(self) -&gt; dict:\n    return asdict(self.config) | {\"n_model_calls\": self.n_calls, \"model_cost\": self.cost}\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.RouletteModel.select_model","title":"select_model","text":"<pre><code>select_model() -&gt; Model\n</code></pre> Source code in <code>src/minisweagent/models/extra/roulette.py</code> <pre><code>def select_model(self) -&gt; Model:\n    return random.choice(self.models)\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.RouletteModel.query","title":"query","text":"<pre><code>query(*args, **kwargs) -&gt; dict\n</code></pre> Source code in <code>src/minisweagent/models/extra/roulette.py</code> <pre><code>def query(self, *args, **kwargs) -&gt; dict:\n    model = self.select_model()\n    response = model.query(*args, **kwargs)\n    response[\"model_name\"] = model.config.model_name\n    return response\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.InterleavingModelConfig","title":"InterleavingModelConfig  <code>dataclass</code>","text":"<pre><code>InterleavingModelConfig(\n    model_kwargs: list[dict],\n    sequence: list[int] | None = None,\n    model_name: str = \"interleaving\",\n)\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.InterleavingModelConfig.model_kwargs","title":"model_kwargs  <code>instance-attribute</code>","text":"<pre><code>model_kwargs: list[dict]\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.InterleavingModelConfig.sequence","title":"sequence  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sequence: list[int] | None = None\n</code></pre> <p>If set to 0, 0, 1, we will return the first model 2 times, then the second model 1 time, then the first model again, etc.</p>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.InterleavingModelConfig.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str = 'interleaving'\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.InterleavingModel","title":"InterleavingModel","text":"<pre><code>InterleavingModel(\n    *,\n    config_class: Callable = InterleavingModelConfig,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>RouletteModel</code></p> <p>This \"meta\"-model alternates between the models in the sequence for every call</p> Source code in <code>src/minisweagent/models/extra/roulette.py</code> <pre><code>def __init__(self, *, config_class: Callable = InterleavingModelConfig, **kwargs):\n    \"\"\"This \"meta\"-model alternates between the models in the sequence for every call\"\"\"\n    super().__init__(config_class=config_class, **kwargs)\n</code></pre>"},{"location":"reference/models/extra/#minisweagent.models.extra.roulette.InterleavingModel.select_model","title":"select_model","text":"<pre><code>select_model() -&gt; Model\n</code></pre> Source code in <code>src/minisweagent/models/extra/roulette.py</code> <pre><code>def select_model(self) -&gt; Model:\n    if self.config.sequence is None:\n        i_model = self.n_calls % len(self.models)\n    else:\n        i_model = self.config.sequence[self.n_calls % len(self.config.sequence)]\n    return self.models[i_model]\n</code></pre>"},{"location":"reference/models/litellm/","title":"LitellmModel","text":""},{"location":"reference/models/litellm/#litellm-model","title":"Litellm Model","text":"<p>LiteLLM Model class</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>import json\nimport logging\nimport os\nfrom dataclasses import asdict, dataclass, field\nfrom pathlib import Path\nfrom typing import Any, Literal\n\nimport litellm\nfrom tenacity import (\n    before_sleep_log,\n    retry,\n    retry_if_not_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom minisweagent.models import GLOBAL_MODEL_STATS\nfrom minisweagent.models.utils.cache_control import set_cache_control\n\nlogger = logging.getLogger(\"litellm_model\")\n\n\n@dataclass\nclass LitellmModelConfig:\n    model_name: str\n    model_kwargs: dict[str, Any] = field(default_factory=dict)\n    litellm_model_registry: Path | str | None = os.getenv(\"LITELLM_MODEL_REGISTRY_PATH\")\n    set_cache_control: Literal[\"default_end\"] | None = None\n    \"\"\"Set explicit cache control markers, for example for Anthropic models\"\"\"\n\n\nclass LitellmModel:\n    def __init__(self, *, config_class: type = LitellmModelConfig, **kwargs):\n        self.config = config_class(**kwargs)\n        self.cost = 0.0\n        self.n_calls = 0\n        if self.config.litellm_model_registry and Path(self.config.litellm_model_registry).is_file():\n            litellm.utils.register_model(json.loads(Path(self.config.litellm_model_registry).read_text()))\n\n    @retry(\n        stop=stop_after_attempt(10),\n        wait=wait_exponential(multiplier=1, min=4, max=60),\n        before_sleep=before_sleep_log(logger, logging.WARNING),\n        retry=retry_if_not_exception_type(\n            (\n                litellm.exceptions.UnsupportedParamsError,\n                litellm.exceptions.NotFoundError,\n                litellm.exceptions.PermissionDeniedError,\n                litellm.exceptions.ContextWindowExceededError,\n                litellm.exceptions.APIError,\n                litellm.exceptions.AuthenticationError,\n                KeyboardInterrupt,\n            )\n        ),\n    )\n    def _query(self, messages: list[dict[str, str]], **kwargs):\n        try:\n            return litellm.completion(\n                model=self.config.model_name, messages=messages, **(self.config.model_kwargs | kwargs)\n            )\n        except litellm.exceptions.AuthenticationError as e:\n            e.message += \" You can permanently set your API key with `mini-extra config set KEY VALUE`.\"\n            raise e\n\n    def query(self, messages: list[dict[str, str]], **kwargs) -&gt; dict:\n        if self.config.set_cache_control:\n            messages = set_cache_control(messages, mode=self.config.set_cache_control)\n        response = self._query(messages, **kwargs)\n        try:\n            cost = litellm.cost_calculator.completion_cost(response)\n        except Exception as e:\n            logger.critical(\n                f\"Error calculating cost for model {self.config.model_name}: {e}. \"\n                \"Please check the 'Updating the model registry' section in the documentation at \"\n                \"https://klieret.short.gy/litellm-model-registry Still stuck? Please open a github issue for help!\"\n            )\n            raise\n        self.n_calls += 1\n        assert cost &gt;= 0.0, f\"Cost is negative: {cost}\"\n        self.cost += cost\n        GLOBAL_MODEL_STATS.add(cost)\n        return {\n            \"content\": response.choices[0].message.content or \"\",  # type: ignore\n            \"extra\": {\n                \"response\": response.model_dump(),\n            },\n        }\n\n    def get_template_vars(self) -&gt; dict[str, Any]:\n        return asdict(self.config) | {\"n_model_calls\": self.n_calls, \"model_cost\": self.cost}\n</code></pre> <p>Guides</p> <ul> <li>Setting up most models is covered in the quickstart guide.</li> <li>If you want to use local models, please check this guide.</li> </ul> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/models/litellm/#minisweagent.models.litellm_model","title":"minisweagent.models.litellm_model","text":""},{"location":"reference/models/litellm/#minisweagent.models.litellm_model.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger('litellm_model')\n</code></pre>"},{"location":"reference/models/litellm/#minisweagent.models.litellm_model.LitellmModelConfig","title":"LitellmModelConfig  <code>dataclass</code>","text":"<pre><code>LitellmModelConfig(\n    model_name: str,\n    model_kwargs: dict[str, Any] = dict(),\n    litellm_model_registry: Path | str | None = getenv(\n        \"LITELLM_MODEL_REGISTRY_PATH\"\n    ),\n    set_cache_control: Literal[\"default_end\"] | None = None,\n)\n</code></pre>"},{"location":"reference/models/litellm/#minisweagent.models.litellm_model.LitellmModelConfig.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name: str\n</code></pre>"},{"location":"reference/models/litellm/#minisweagent.models.litellm_model.LitellmModelConfig.model_kwargs","title":"model_kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_kwargs: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"reference/models/litellm/#minisweagent.models.litellm_model.LitellmModelConfig.litellm_model_registry","title":"litellm_model_registry  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>litellm_model_registry: Path | str | None = getenv(\n    \"LITELLM_MODEL_REGISTRY_PATH\"\n)\n</code></pre>"},{"location":"reference/models/litellm/#minisweagent.models.litellm_model.LitellmModelConfig.set_cache_control","title":"set_cache_control  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>set_cache_control: Literal['default_end'] | None = None\n</code></pre> <p>Set explicit cache control markers, for example for Anthropic models</p>"},{"location":"reference/models/litellm/#minisweagent.models.litellm_model.LitellmModel","title":"LitellmModel","text":"<pre><code>LitellmModel(\n    *, config_class: type = LitellmModelConfig, **kwargs\n)\n</code></pre> Source code in <code>src/minisweagent/models/litellm_model.py</code> <pre><code>def __init__(self, *, config_class: type = LitellmModelConfig, **kwargs):\n    self.config = config_class(**kwargs)\n    self.cost = 0.0\n    self.n_calls = 0\n    if self.config.litellm_model_registry and Path(self.config.litellm_model_registry).is_file():\n        litellm.utils.register_model(json.loads(Path(self.config.litellm_model_registry).read_text()))\n</code></pre>"},{"location":"reference/models/litellm/#minisweagent.models.litellm_model.LitellmModel.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config_class(**kwargs)\n</code></pre>"},{"location":"reference/models/litellm/#minisweagent.models.litellm_model.LitellmModel.cost","title":"cost  <code>instance-attribute</code>","text":"<pre><code>cost = 0.0\n</code></pre>"},{"location":"reference/models/litellm/#minisweagent.models.litellm_model.LitellmModel.n_calls","title":"n_calls  <code>instance-attribute</code>","text":"<pre><code>n_calls = 0\n</code></pre>"},{"location":"reference/models/litellm/#minisweagent.models.litellm_model.LitellmModel.query","title":"query","text":"<pre><code>query(messages: list[dict[str, str]], **kwargs) -&gt; dict\n</code></pre> Source code in <code>src/minisweagent/models/litellm_model.py</code> <pre><code>def query(self, messages: list[dict[str, str]], **kwargs) -&gt; dict:\n    if self.config.set_cache_control:\n        messages = set_cache_control(messages, mode=self.config.set_cache_control)\n    response = self._query(messages, **kwargs)\n    try:\n        cost = litellm.cost_calculator.completion_cost(response)\n    except Exception as e:\n        logger.critical(\n            f\"Error calculating cost for model {self.config.model_name}: {e}. \"\n            \"Please check the 'Updating the model registry' section in the documentation at \"\n            \"https://klieret.short.gy/litellm-model-registry Still stuck? Please open a github issue for help!\"\n        )\n        raise\n    self.n_calls += 1\n    assert cost &gt;= 0.0, f\"Cost is negative: {cost}\"\n    self.cost += cost\n    GLOBAL_MODEL_STATS.add(cost)\n    return {\n        \"content\": response.choices[0].message.content or \"\",  # type: ignore\n        \"extra\": {\n            \"response\": response.model_dump(),\n        },\n    }\n</code></pre>"},{"location":"reference/models/litellm/#minisweagent.models.litellm_model.LitellmModel.get_template_vars","title":"get_template_vars","text":"<pre><code>get_template_vars() -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/minisweagent/models/litellm_model.py</code> <pre><code>def get_template_vars(self) -&gt; dict[str, Any]:\n    return asdict(self.config) | {\"n_model_calls\": self.n_calls, \"model_cost\": self.cost}\n</code></pre>"},{"location":"reference/models/openrouter/","title":"OpenRouterModel","text":""},{"location":"reference/models/openrouter/#openrouter-model","title":"OpenRouter Model","text":"<p>OpenRouter Model class</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>import json\nimport logging\nimport os\nfrom dataclasses import asdict, dataclass, field\nfrom typing import Any, Literal\n\nimport requests\nfrom tenacity import (\n    before_sleep_log,\n    retry,\n    retry_if_not_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom minisweagent.models import GLOBAL_MODEL_STATS\nfrom minisweagent.models.utils.cache_control import set_cache_control\n\nlogger = logging.getLogger(\"openrouter_model\")\n\n\n@dataclass\nclass OpenRouterModelConfig:\n    model_name: str\n    model_kwargs: dict[str, Any] = field(default_factory=dict)\n    set_cache_control: Literal[\"default_end\"] | None = None\n    \"\"\"Set explicit cache control markers, for example for Anthropic models\"\"\"\n\n\nclass OpenRouterAPIError(Exception):\n    \"\"\"Custom exception for OpenRouter API errors.\"\"\"\n\n    pass\n\n\nclass OpenRouterAuthenticationError(Exception):\n    \"\"\"Custom exception for OpenRouter authentication errors.\"\"\"\n\n    pass\n\n\nclass OpenRouterRateLimitError(Exception):\n    \"\"\"Custom exception for OpenRouter rate limit errors.\"\"\"\n\n    pass\n\n\nclass OpenRouterModel:\n    def __init__(self, **kwargs):\n        self.config = OpenRouterModelConfig(**kwargs)\n        self.cost = 0.0\n        self.n_calls = 0\n        self._api_url = \"https://openrouter.ai/api/v1/chat/completions\"\n        self._api_key = os.getenv(\"OPENROUTER_API_KEY\", \"\")\n\n    @retry(\n        stop=stop_after_attempt(10),\n        wait=wait_exponential(multiplier=1, min=4, max=60),\n        before_sleep=before_sleep_log(logger, logging.WARNING),\n        retry=retry_if_not_exception_type(\n            (\n                OpenRouterAuthenticationError,\n                KeyboardInterrupt,\n            )\n        ),\n    )\n    def _query(self, messages: list[dict[str, str]], **kwargs):\n        headers = {\n            \"Authorization\": f\"Bearer {self._api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n        payload = {\n            \"model\": self.config.model_name,\n            \"messages\": messages,\n            \"usage\": {\"include\": True},\n            **(self.config.model_kwargs | kwargs),\n        }\n\n        try:\n            response = requests.post(self._api_url, headers=headers, data=json.dumps(payload), timeout=60)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.HTTPError as e:\n            if response.status_code == 401:\n                error_msg = \"Authentication failed. You can permanently set your API key with `mini-extra config set OPENROUTER_API_KEY YOUR_KEY`.\"\n                raise OpenRouterAuthenticationError(error_msg) from e\n            elif response.status_code == 429:\n                raise OpenRouterRateLimitError(\"Rate limit exceeded\") from e\n            else:\n                raise OpenRouterAPIError(f\"HTTP {response.status_code}: {response.text}\") from e\n        except requests.exceptions.RequestException as e:\n            raise OpenRouterAPIError(f\"Request failed: {e}\") from e\n\n    def query(self, messages: list[dict[str, str]], **kwargs) -&gt; dict:\n        if self.config.set_cache_control:\n            messages = set_cache_control(messages, mode=self.config.set_cache_control)\n        response = self._query(messages, **kwargs)\n\n        # Extract cost from usage information\n        usage = response.get(\"usage\", {})\n        cost = usage.get(\"cost\", 0.0)\n        assert cost &gt;= 0.0, f\"Cost is negative: {cost}\"\n\n        # If total_cost is not available, raise an error\n        if cost == 0.0:\n            raise OpenRouterAPIError(\n                f\"No cost information available from OpenRouter API for model {self.config.model_name}. \"\n                \"Cost tracking is required but not provided by the API response.\"\n            )\n\n        self.n_calls += 1\n        self.cost += cost\n        GLOBAL_MODEL_STATS.add(cost)\n\n        return {\n            \"content\": response[\"choices\"][0][\"message\"][\"content\"] or \"\",\n            \"extra\": {\n                \"response\": response,  # already is json\n            },\n        }\n\n    def get_template_vars(self) -&gt; dict[str, Any]:\n        return asdict(self.config) | {\"n_model_calls\": self.n_calls, \"model_cost\": self.cost}\n</code></pre> <p>Guide</p> <p>Setting up OpenRouter models is covered in the quickstart guide.</p> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model","title":"minisweagent.models.openrouter_model","text":""},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger('openrouter_model')\n</code></pre>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterModelConfig","title":"OpenRouterModelConfig  <code>dataclass</code>","text":"<pre><code>OpenRouterModelConfig(\n    model_name: str,\n    model_kwargs: dict[str, Any] = dict(),\n    set_cache_control: Literal[\"default_end\"] | None = None,\n)\n</code></pre>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterModelConfig.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name: str\n</code></pre>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterModelConfig.model_kwargs","title":"model_kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_kwargs: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterModelConfig.set_cache_control","title":"set_cache_control  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>set_cache_control: Literal['default_end'] | None = None\n</code></pre> <p>Set explicit cache control markers, for example for Anthropic models</p>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterAPIError","title":"OpenRouterAPIError","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for OpenRouter API errors.</p>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterAuthenticationError","title":"OpenRouterAuthenticationError","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for OpenRouter authentication errors.</p>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterRateLimitError","title":"OpenRouterRateLimitError","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for OpenRouter rate limit errors.</p>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterModel","title":"OpenRouterModel","text":"<pre><code>OpenRouterModel(**kwargs)\n</code></pre> Source code in <code>src/minisweagent/models/openrouter_model.py</code> <pre><code>def __init__(self, **kwargs):\n    self.config = OpenRouterModelConfig(**kwargs)\n    self.cost = 0.0\n    self.n_calls = 0\n    self._api_url = \"https://openrouter.ai/api/v1/chat/completions\"\n    self._api_key = os.getenv(\"OPENROUTER_API_KEY\", \"\")\n</code></pre>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterModel.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = OpenRouterModelConfig(**kwargs)\n</code></pre>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterModel.cost","title":"cost  <code>instance-attribute</code>","text":"<pre><code>cost = 0.0\n</code></pre>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterModel.n_calls","title":"n_calls  <code>instance-attribute</code>","text":"<pre><code>n_calls = 0\n</code></pre>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterModel.query","title":"query","text":"<pre><code>query(messages: list[dict[str, str]], **kwargs) -&gt; dict\n</code></pre> Source code in <code>src/minisweagent/models/openrouter_model.py</code> <pre><code>def query(self, messages: list[dict[str, str]], **kwargs) -&gt; dict:\n    if self.config.set_cache_control:\n        messages = set_cache_control(messages, mode=self.config.set_cache_control)\n    response = self._query(messages, **kwargs)\n\n    # Extract cost from usage information\n    usage = response.get(\"usage\", {})\n    cost = usage.get(\"cost\", 0.0)\n    assert cost &gt;= 0.0, f\"Cost is negative: {cost}\"\n\n    # If total_cost is not available, raise an error\n    if cost == 0.0:\n        raise OpenRouterAPIError(\n            f\"No cost information available from OpenRouter API for model {self.config.model_name}. \"\n            \"Cost tracking is required but not provided by the API response.\"\n        )\n\n    self.n_calls += 1\n    self.cost += cost\n    GLOBAL_MODEL_STATS.add(cost)\n\n    return {\n        \"content\": response[\"choices\"][0][\"message\"][\"content\"] or \"\",\n        \"extra\": {\n            \"response\": response,  # already is json\n        },\n    }\n</code></pre>"},{"location":"reference/models/openrouter/#minisweagent.models.openrouter_model.OpenRouterModel.get_template_vars","title":"get_template_vars","text":"<pre><code>get_template_vars() -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/minisweagent/models/openrouter_model.py</code> <pre><code>def get_template_vars(self) -&gt; dict[str, Any]:\n    return asdict(self.config) | {\"n_model_calls\": self.n_calls, \"model_cost\": self.cost}\n</code></pre>"},{"location":"reference/models/portkey/","title":"Portkey Model","text":""},{"location":"reference/models/portkey/#portkey-model","title":"Portkey Model","text":"<p>Portkey Model class</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>import json\nimport logging\nimport os\nfrom dataclasses import asdict, dataclass, field\nfrom pathlib import Path\nfrom typing import Any, Literal\n\nimport litellm\nfrom tenacity import (\n    before_sleep_log,\n    retry,\n    retry_if_not_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom minisweagent.models import GLOBAL_MODEL_STATS\nfrom minisweagent.models.utils.cache_control import set_cache_control\n\nlogger = logging.getLogger(\"portkey_model\")\n\ntry:\n    from portkey_ai import Portkey\nexcept ImportError:\n    Portkey = None\n\n\n@dataclass\nclass PortkeyModelConfig:\n    model_name: str\n    model_kwargs: dict[str, Any] = field(default_factory=dict)\n    litellm_model_registry: Path | str | None = os.getenv(\"LITELLM_MODEL_REGISTRY_PATH\")\n    \"\"\"We currently use litellm to calculate costs. Here you can register additional models to litellm's model registry.\n    Note that this might change if we get better support for Portkey and change how we calculate costs.\n    \"\"\"\n    litellm_model_name_override: str = \"\"\n    \"\"\"We currently use litellm to calculate costs. Here you can override the model name to use for litellm in case it\n    doesn't match the Portkey model name.\n    Note that this might change if we get better support for Portkey and change how we calculate costs.\n    \"\"\"\n    set_cache_control: Literal[\"default_end\"] | None = None\n    \"\"\"Set explicit cache control markers, for example for Anthropic models\"\"\"\n\n\nclass PortkeyModel:\n    def __init__(self, **kwargs):\n        if Portkey is None:\n            raise ImportError(\n                \"The portkey-ai package is required to use PortkeyModel. Please install it with: pip install portkey-ai\"\n            )\n        self.config = PortkeyModelConfig(**kwargs)\n        self.cost = 0.0\n        self.n_calls = 0\n        if self.config.litellm_model_registry and Path(self.config.litellm_model_registry).is_file():\n            litellm.utils.register_model(json.loads(Path(self.config.litellm_model_registry).read_text()))\n\n        # Get API key from environment or raise error\n        self._api_key = os.getenv(\"PORTKEY_API_KEY\")\n        if not self._api_key:\n            raise ValueError(\n                \"Portkey API key is required. Set it via the \"\n                \"PORTKEY_API_KEY environment variable. You can permanently set it with \"\n                \"`mini-extra config set PORTKEY_API_KEY YOUR_KEY`.\"\n            )\n\n        # Get virtual key from environment\n        virtual_key = os.getenv(\"PORTKEY_VIRTUAL_KEY\")\n\n        # Initialize Portkey client\n        client_kwargs = {\"api_key\": self._api_key}\n        if virtual_key:\n            client_kwargs[\"virtual_key\"] = virtual_key\n\n        self.client = Portkey(**client_kwargs)\n\n    @retry(\n        stop=stop_after_attempt(10),\n        wait=wait_exponential(multiplier=1, min=4, max=60),\n        before_sleep=before_sleep_log(logger, logging.WARNING),\n        retry=retry_if_not_exception_type((KeyboardInterrupt, TypeError, ValueError)),\n    )\n    def _query(self, messages: list[dict[str, str]], **kwargs):\n        # return self.client.with_options(metadata={\"request_id\": request_id}).chat.completions.create(\n        return self.client.chat.completions.create(\n            model=self.config.model_name,\n            messages=messages,\n            **(self.config.model_kwargs | kwargs),\n        )\n\n    def query(self, messages: list[dict[str, str]], **kwargs) -&gt; dict:\n        if self.config.set_cache_control:\n            messages = set_cache_control(messages, mode=self.config.set_cache_control)\n        response = self._query(messages, **kwargs)\n        response_for_cost_calc = response.model_copy()\n        if self.config.litellm_model_name_override:\n            if response_for_cost_calc.model:\n                response_for_cost_calc.model = self.config.litellm_model_name_override\n        prompt_tokens = response_for_cost_calc.usage.prompt_tokens\n        total_tokens = response_for_cost_calc.usage.total_tokens\n        completion_tokens = response_for_cost_calc.usage.completion_tokens\n        if total_tokens - prompt_tokens - completion_tokens != 0:\n            # This is most likely related to how portkey treats cached tokens: It doesn't count them towards the prompt tokens (?)\n            logger.warning(\n                f\"WARNING: Total tokens - prompt tokens - completion tokens != 0: {response_for_cost_calc.model_dump()}.\"\n                \" This is probably a portkey bug or incompatibility with litellm cost tracking. \"\n                \"Setting prompt tokens based on total tokens and completion tokens. You might want to double check your costs.\"\n            )\n            response_for_cost_calc.usage.prompt_tokens = total_tokens - completion_tokens\n        try:\n            cost = litellm.cost_calculator.completion_cost(\n                response_for_cost_calc, model=self.config.litellm_model_name_override or None\n            )\n        except Exception as e:\n            logger.critical(\n                f\"Error calculating cost for model {self.config.model_name} based on {response_for_cost_calc.model_dump()}: {e}. \"\n                \"Please check the 'Updating the model registry' section in the documentation at \"\n                \"https://klieret.short.gy/litellm-model-registry Still stuck? Please open a github issue for help!\"\n            )\n            raise\n        assert cost &gt;= 0.0, f\"Cost is negative: {cost}\"\n\n        self.n_calls += 1\n        self.cost += cost\n        GLOBAL_MODEL_STATS.add(cost)\n\n        return {\n            \"content\": response.choices[0].message.content or \"\",\n            \"extra\": {\n                \"response\": response.model_dump(),\n                \"cost\": cost,\n            },\n        }\n\n    def get_template_vars(self) -&gt; dict[str, Any]:\n        return asdict(self.config) | {\"n_model_calls\": self.n_calls, \"model_cost\": self.cost}\n</code></pre> <p>Guide</p> <p>Setting up Portkey models is covered in the quickstart guide.</p> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model","title":"minisweagent.models.portkey_model","text":""},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger('portkey_model')\n</code></pre>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModelConfig","title":"PortkeyModelConfig  <code>dataclass</code>","text":"<pre><code>PortkeyModelConfig(\n    model_name: str,\n    model_kwargs: dict[str, Any] = dict(),\n    litellm_model_registry: Path | str | None = getenv(\n        \"LITELLM_MODEL_REGISTRY_PATH\"\n    ),\n    litellm_model_name_override: str = \"\",\n    set_cache_control: Literal[\"default_end\"] | None = None,\n)\n</code></pre>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModelConfig.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name: str\n</code></pre>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModelConfig.model_kwargs","title":"model_kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_kwargs: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModelConfig.litellm_model_registry","title":"litellm_model_registry  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>litellm_model_registry: Path | str | None = getenv(\n    \"LITELLM_MODEL_REGISTRY_PATH\"\n)\n</code></pre> <p>We currently use litellm to calculate costs. Here you can register additional models to litellm's model registry. Note that this might change if we get better support for Portkey and change how we calculate costs.</p>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModelConfig.litellm_model_name_override","title":"litellm_model_name_override  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>litellm_model_name_override: str = ''\n</code></pre> <p>We currently use litellm to calculate costs. Here you can override the model name to use for litellm in case it doesn't match the Portkey model name. Note that this might change if we get better support for Portkey and change how we calculate costs.</p>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModelConfig.set_cache_control","title":"set_cache_control  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>set_cache_control: Literal['default_end'] | None = None\n</code></pre> <p>Set explicit cache control markers, for example for Anthropic models</p>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModel","title":"PortkeyModel","text":"<pre><code>PortkeyModel(**kwargs)\n</code></pre> Source code in <code>src/minisweagent/models/portkey_model.py</code> <pre><code>def __init__(self, **kwargs):\n    if Portkey is None:\n        raise ImportError(\n            \"The portkey-ai package is required to use PortkeyModel. Please install it with: pip install portkey-ai\"\n        )\n    self.config = PortkeyModelConfig(**kwargs)\n    self.cost = 0.0\n    self.n_calls = 0\n    if self.config.litellm_model_registry and Path(self.config.litellm_model_registry).is_file():\n        litellm.utils.register_model(json.loads(Path(self.config.litellm_model_registry).read_text()))\n\n    # Get API key from environment or raise error\n    self._api_key = os.getenv(\"PORTKEY_API_KEY\")\n    if not self._api_key:\n        raise ValueError(\n            \"Portkey API key is required. Set it via the \"\n            \"PORTKEY_API_KEY environment variable. You can permanently set it with \"\n            \"`mini-extra config set PORTKEY_API_KEY YOUR_KEY`.\"\n        )\n\n    # Get virtual key from environment\n    virtual_key = os.getenv(\"PORTKEY_VIRTUAL_KEY\")\n\n    # Initialize Portkey client\n    client_kwargs = {\"api_key\": self._api_key}\n    if virtual_key:\n        client_kwargs[\"virtual_key\"] = virtual_key\n\n    self.client = Portkey(**client_kwargs)\n</code></pre>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModel.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = PortkeyModelConfig(**kwargs)\n</code></pre>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModel.cost","title":"cost  <code>instance-attribute</code>","text":"<pre><code>cost = 0.0\n</code></pre>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModel.n_calls","title":"n_calls  <code>instance-attribute</code>","text":"<pre><code>n_calls = 0\n</code></pre>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModel.client","title":"client  <code>instance-attribute</code>","text":"<pre><code>client = Portkey(**client_kwargs)\n</code></pre>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModel.query","title":"query","text":"<pre><code>query(messages: list[dict[str, str]], **kwargs) -&gt; dict\n</code></pre> Source code in <code>src/minisweagent/models/portkey_model.py</code> <pre><code>def query(self, messages: list[dict[str, str]], **kwargs) -&gt; dict:\n    if self.config.set_cache_control:\n        messages = set_cache_control(messages, mode=self.config.set_cache_control)\n    response = self._query(messages, **kwargs)\n    response_for_cost_calc = response.model_copy()\n    if self.config.litellm_model_name_override:\n        if response_for_cost_calc.model:\n            response_for_cost_calc.model = self.config.litellm_model_name_override\n    prompt_tokens = response_for_cost_calc.usage.prompt_tokens\n    total_tokens = response_for_cost_calc.usage.total_tokens\n    completion_tokens = response_for_cost_calc.usage.completion_tokens\n    if total_tokens - prompt_tokens - completion_tokens != 0:\n        # This is most likely related to how portkey treats cached tokens: It doesn't count them towards the prompt tokens (?)\n        logger.warning(\n            f\"WARNING: Total tokens - prompt tokens - completion tokens != 0: {response_for_cost_calc.model_dump()}.\"\n            \" This is probably a portkey bug or incompatibility with litellm cost tracking. \"\n            \"Setting prompt tokens based on total tokens and completion tokens. You might want to double check your costs.\"\n        )\n        response_for_cost_calc.usage.prompt_tokens = total_tokens - completion_tokens\n    try:\n        cost = litellm.cost_calculator.completion_cost(\n            response_for_cost_calc, model=self.config.litellm_model_name_override or None\n        )\n    except Exception as e:\n        logger.critical(\n            f\"Error calculating cost for model {self.config.model_name} based on {response_for_cost_calc.model_dump()}: {e}. \"\n            \"Please check the 'Updating the model registry' section in the documentation at \"\n            \"https://klieret.short.gy/litellm-model-registry Still stuck? Please open a github issue for help!\"\n        )\n        raise\n    assert cost &gt;= 0.0, f\"Cost is negative: {cost}\"\n\n    self.n_calls += 1\n    self.cost += cost\n    GLOBAL_MODEL_STATS.add(cost)\n\n    return {\n        \"content\": response.choices[0].message.content or \"\",\n        \"extra\": {\n            \"response\": response.model_dump(),\n            \"cost\": cost,\n        },\n    }\n</code></pre>"},{"location":"reference/models/portkey/#minisweagent.models.portkey_model.PortkeyModel.get_template_vars","title":"get_template_vars","text":"<pre><code>get_template_vars() -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/minisweagent/models/portkey_model.py</code> <pre><code>def get_template_vars(self) -&gt; dict[str, Any]:\n    return asdict(self.config) | {\"n_model_calls\": self.n_calls, \"model_cost\": self.cost}\n</code></pre>"},{"location":"reference/models/test_models/","title":"DeterministicModel","text":""},{"location":"reference/models/test_models/#test-models","title":"Test Models","text":"<p>Test Models class</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>import logging\nimport time\nfrom dataclasses import asdict, dataclass\nfrom typing import Any\n\nfrom minisweagent.models import GLOBAL_MODEL_STATS\n\n\n@dataclass\nclass DeterministicModelConfig:\n    outputs: list[str]\n    model_name: str = \"deterministic\"\n    cost_per_call: float = 1.0\n\n\nclass DeterministicModel:\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize with a list of outputs to return in sequence.\n        \"\"\"\n        self.config = DeterministicModelConfig(**kwargs)\n        self.current_index = -1\n        self.cost = 0.0\n        self.n_calls = 0\n\n    def query(self, messages: list[dict[str, str]], **kwargs) -&gt; dict:\n        self.current_index += 1\n        output = self.config.outputs[self.current_index]\n        if \"/sleep\" in output:\n            print(\"SLEEPING\")\n            time.sleep(float(output.split(\"/sleep\")[1]))\n            return self.query(messages, **kwargs)\n        if \"/warning\" in output:\n            logging.warning(output.split(\"/warning\")[1])\n            return self.query(messages, **kwargs)\n        self.n_calls += 1\n        self.cost += self.config.cost_per_call\n        GLOBAL_MODEL_STATS.add(self.config.cost_per_call)\n        return {\"content\": output}\n\n    def get_template_vars(self) -&gt; dict[str, Any]:\n        return asdict(self.config) | {\"n_model_calls\": self.n_calls, \"model_cost\": self.cost}\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/models/test_models/#minisweagent.models.test_models","title":"minisweagent.models.test_models","text":""},{"location":"reference/models/test_models/#minisweagent.models.test_models.DeterministicModelConfig","title":"DeterministicModelConfig  <code>dataclass</code>","text":"<pre><code>DeterministicModelConfig(\n    outputs: list[str],\n    model_name: str = \"deterministic\",\n    cost_per_call: float = 1.0,\n)\n</code></pre>"},{"location":"reference/models/test_models/#minisweagent.models.test_models.DeterministicModelConfig.outputs","title":"outputs  <code>instance-attribute</code>","text":"<pre><code>outputs: list[str]\n</code></pre>"},{"location":"reference/models/test_models/#minisweagent.models.test_models.DeterministicModelConfig.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str = 'deterministic'\n</code></pre>"},{"location":"reference/models/test_models/#minisweagent.models.test_models.DeterministicModelConfig.cost_per_call","title":"cost_per_call  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cost_per_call: float = 1.0\n</code></pre>"},{"location":"reference/models/test_models/#minisweagent.models.test_models.DeterministicModel","title":"DeterministicModel","text":"<pre><code>DeterministicModel(**kwargs)\n</code></pre> <p>Initialize with a list of outputs to return in sequence.</p> Source code in <code>src/minisweagent/models/test_models.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize with a list of outputs to return in sequence.\n    \"\"\"\n    self.config = DeterministicModelConfig(**kwargs)\n    self.current_index = -1\n    self.cost = 0.0\n    self.n_calls = 0\n</code></pre>"},{"location":"reference/models/test_models/#minisweagent.models.test_models.DeterministicModel.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = DeterministicModelConfig(**kwargs)\n</code></pre>"},{"location":"reference/models/test_models/#minisweagent.models.test_models.DeterministicModel.current_index","title":"current_index  <code>instance-attribute</code>","text":"<pre><code>current_index = -1\n</code></pre>"},{"location":"reference/models/test_models/#minisweagent.models.test_models.DeterministicModel.cost","title":"cost  <code>instance-attribute</code>","text":"<pre><code>cost = 0.0\n</code></pre>"},{"location":"reference/models/test_models/#minisweagent.models.test_models.DeterministicModel.n_calls","title":"n_calls  <code>instance-attribute</code>","text":"<pre><code>n_calls = 0\n</code></pre>"},{"location":"reference/models/test_models/#minisweagent.models.test_models.DeterministicModel.query","title":"query","text":"<pre><code>query(messages: list[dict[str, str]], **kwargs) -&gt; dict\n</code></pre> Source code in <code>src/minisweagent/models/test_models.py</code> <pre><code>def query(self, messages: list[dict[str, str]], **kwargs) -&gt; dict:\n    self.current_index += 1\n    output = self.config.outputs[self.current_index]\n    if \"/sleep\" in output:\n        print(\"SLEEPING\")\n        time.sleep(float(output.split(\"/sleep\")[1]))\n        return self.query(messages, **kwargs)\n    if \"/warning\" in output:\n        logging.warning(output.split(\"/warning\")[1])\n        return self.query(messages, **kwargs)\n    self.n_calls += 1\n    self.cost += self.config.cost_per_call\n    GLOBAL_MODEL_STATS.add(self.config.cost_per_call)\n    return {\"content\": output}\n</code></pre>"},{"location":"reference/models/test_models/#minisweagent.models.test_models.DeterministicModel.get_template_vars","title":"get_template_vars","text":"<pre><code>get_template_vars() -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/minisweagent/models/test_models.py</code> <pre><code>def get_template_vars(self) -&gt; dict[str, Any]:\n    return asdict(self.config) | {\"n_model_calls\": self.n_calls, \"model_cost\": self.cost}\n</code></pre>"},{"location":"reference/models/utils/","title":"Model Utilities","text":""},{"location":"reference/models/utils/#model-utilities","title":"Model Utilities","text":"<p>Model Utilities</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>\"\"\"This file provides convenience functions for selecting models.\nYou can ignore this file completely if you explicitly set your model in your run script.\n\"\"\"\n\nimport copy\nimport importlib\nimport os\nimport threading\n\nfrom minisweagent import Model\n\n\nclass GlobalModelStats:\n    \"\"\"Global model statistics tracker with optional limits.\"\"\"\n\n    def __init__(self):\n        self._cost = 0.0\n        self._n_calls = 0\n        self._lock = threading.Lock()\n        self.cost_limit = float(os.getenv(\"MSWEA_GLOBAL_COST_LIMIT\", \"0\"))\n        self.call_limit = int(os.getenv(\"MSWEA_GLOBAL_CALL_LIMIT\", \"0\"))\n        if (self.cost_limit &gt; 0 or self.call_limit &gt; 0) and not os.getenv(\"MSWEA_SILENT_STARTUP\"):\n            print(\n                f\"Global cost/call limit: ${self.cost_limit:.4f} / {self.call_limit}\")\n\n    def add(self, cost: float) -&gt; None:\n        \"\"\"Add a model call with its cost, checking limits.\"\"\"\n        with self._lock:\n            self._cost += cost\n            self._n_calls += 1\n        if 0 &lt; self.cost_limit &lt; self._cost or 0 &lt; self.call_limit &lt; self._n_calls + 1:\n            raise RuntimeError(\n                f\"Global cost/call limit exceeded: ${self._cost:.4f} / {self._n_calls + 1}\")\n\n    @property\n    def cost(self) -&gt; float:\n        return self._cost\n\n    @property\n    def n_calls(self) -&gt; int:\n        return self._n_calls\n\n\nGLOBAL_MODEL_STATS = GlobalModelStats()\n\n\ndef get_model(input_model_name: str | None = None, config: dict | None = None) -&gt; Model:\n    \"\"\"Get an initialized model object from any kind of user input or settings.\"\"\"\n    resolved_model_name = get_model_name(input_model_name, config)\n    if config is None:\n        config = {}\n    config = copy.deepcopy(config)\n    config[\"model_name\"] = resolved_model_name\n\n    model_class = get_model_class(\n        resolved_model_name, config.pop(\"model_class\", \"\"))\n\n    if (from_env := os.getenv(\"MSWEA_MODEL_API_KEY\")) and not str(type(model_class)).endswith(\"DeterministicModel\"):\n        config.setdefault(\"model_kwargs\", {})[\"api_key\"] = from_env\n\n    if (\n        any(s in resolved_model_name.lower()\n            for s in [\"anthropic\", \"sonnet\", \"opus\", \"claude\"])\n        and \"set_cache_control\" not in config\n    ):\n        # Select cache control for Anthropic models by default\n        config[\"set_cache_control\"] = \"default_end\"\n\n    return model_class(**config)\n\n\ndef get_model_name(input_model_name: str | None = None, config: dict | None = None) -&gt; str:\n    \"\"\"Get a model name from any kind of user input or settings.\"\"\"\n    if config is None:\n        config = {}\n    if input_model_name:\n        return input_model_name\n    if from_config := config.get(\"model_name\"):\n        return from_config\n    if from_env := os.getenv(\"MSWEA_MODEL_NAME\"):\n        return from_env\n    raise ValueError(\n        \"No default model set. Please run `mini-extra config setup` to set one.\")\n\n\n_MODEL_CLASS_MAPPING = {\n    \"anthropic\": \"minisweagent.models.anthropic.AnthropicModel\",\n    \"litellm\": \"minisweagent.models.litellm_model.LitellmModel\",\n    \"openrouter\": \"minisweagent.models.openrouter_model.OpenRouterModel\",\n    \"portkey\": \"minisweagent.models.portkey_model.PortkeyModel\",\n    \"deterministic\": \"minisweagent.models.test_models.DeterministicModel\",\n}\n\n\ndef get_model_class(model_name: str, model_class: str = \"\") -&gt; type:\n    \"\"\"Select the best model class.\n\n    If a model_class is provided (as shortcut name, or as full import path,\n    e.g., \"anthropic\" or \"minisweagent.models.anthropic.AnthropicModel\"),\n    it takes precedence over the `model_name`.\n    Otherwise, the model_name is used to select the best model class.\n    \"\"\"\n    if model_class:\n        full_path = _MODEL_CLASS_MAPPING.get(model_class, model_class)\n        try:\n            module_name, class_name = full_path.rsplit(\".\", 1)\n            module = importlib.import_module(module_name)\n            return getattr(module, class_name)\n        except (ValueError, ImportError, AttributeError):\n            msg = f\"Unknown model class: {model_class} (resolved to {full_path}, available: {\n                _MODEL_CLASS_MAPPING})\"\n            raise ValueError(msg)\n\n    # Default to LitellmModel\n    from minisweagent.models.litellm_model import LitellmModel\n    if model_name.startswith(\"tensorzero\"):\n        from minisweagent.models.tensorzero import TensorZeroModel\n        return TensorZeroModel\n    return LitellmModel\n</code></pre> <p>Convenience functions for selecting and configuring models.</p> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/models/utils/#minisweagent.models.get_model","title":"minisweagent.models.get_model","text":"<pre><code>get_model(\n    input_model_name: str | None = None,\n    config: dict | None = None,\n) -&gt; Model\n</code></pre> <p>Get an initialized model object from any kind of user input or settings.</p> Source code in <code>src/minisweagent/models/__init__.py</code> <pre><code>def get_model(input_model_name: str | None = None, config: dict | None = None) -&gt; Model:\n    \"\"\"Get an initialized model object from any kind of user input or settings.\"\"\"\n    resolved_model_name = get_model_name(input_model_name, config)\n    if config is None:\n        config = {}\n    config = copy.deepcopy(config)\n    config[\"model_name\"] = resolved_model_name\n\n    model_class = get_model_class(\n        resolved_model_name, config.pop(\"model_class\", \"\"))\n\n    if (from_env := os.getenv(\"MSWEA_MODEL_API_KEY\")) and not str(type(model_class)).endswith(\"DeterministicModel\"):\n        config.setdefault(\"model_kwargs\", {})[\"api_key\"] = from_env\n\n    if (\n        any(s in resolved_model_name.lower()\n            for s in [\"anthropic\", \"sonnet\", \"opus\", \"claude\"])\n        and \"set_cache_control\" not in config\n    ):\n        # Select cache control for Anthropic models by default\n        config[\"set_cache_control\"] = \"default_end\"\n\n    return model_class(**config)\n</code></pre>"},{"location":"reference/models/utils/#minisweagent.models.get_model_name","title":"minisweagent.models.get_model_name","text":"<pre><code>get_model_name(\n    input_model_name: str | None = None,\n    config: dict | None = None,\n) -&gt; str\n</code></pre> <p>Get a model name from any kind of user input or settings.</p> Source code in <code>src/minisweagent/models/__init__.py</code> <pre><code>def get_model_name(input_model_name: str | None = None, config: dict | None = None) -&gt; str:\n    \"\"\"Get a model name from any kind of user input or settings.\"\"\"\n    if config is None:\n        config = {}\n    if input_model_name:\n        return input_model_name\n    if from_config := config.get(\"model_name\"):\n        return from_config\n    if from_env := os.getenv(\"MSWEA_MODEL_NAME\"):\n        return from_env\n    raise ValueError(\n        \"No default model set. Please run `mini-extra config setup` to set one.\")\n</code></pre>"},{"location":"reference/models/utils/#minisweagent.models.get_model_class","title":"minisweagent.models.get_model_class","text":"<pre><code>get_model_class(\n    model_name: str, model_class: str = \"\"\n) -&gt; type\n</code></pre> <p>Select the best model class.</p> <p>If a model_class is provided (as shortcut name, or as full import path, e.g., \"anthropic\" or \"minisweagent.models.anthropic.AnthropicModel\"), it takes precedence over the <code>model_name</code>. Otherwise, the model_name is used to select the best model class.</p> Source code in <code>src/minisweagent/models/__init__.py</code> <pre><code>def get_model_class(model_name: str, model_class: str = \"\") -&gt; type:\n    \"\"\"Select the best model class.\n\n    If a model_class is provided (as shortcut name, or as full import path,\n    e.g., \"anthropic\" or \"minisweagent.models.anthropic.AnthropicModel\"),\n    it takes precedence over the `model_name`.\n    Otherwise, the model_name is used to select the best model class.\n    \"\"\"\n    if model_class:\n        full_path = _MODEL_CLASS_MAPPING.get(model_class, model_class)\n        try:\n            module_name, class_name = full_path.rsplit(\".\", 1)\n            module = importlib.import_module(module_name)\n            return getattr(module, class_name)\n        except (ValueError, ImportError, AttributeError):\n            msg = f\"Unknown model class: {model_class} (resolved to {full_path}, available: {\n                _MODEL_CLASS_MAPPING})\"\n            raise ValueError(msg)\n\n    # Default to LitellmModel\n    from minisweagent.models.litellm_model import LitellmModel\n    if model_name.startswith(\"tensorzero\"):\n        from minisweagent.models.tensorzero import TensorZeroModel\n        return TensorZeroModel\n    return LitellmModel\n</code></pre>"},{"location":"reference/run/github_issue/","title":"GitHub Issue","text":""},{"location":"reference/run/github_issue/#github-issue","title":"GitHub Issue","text":"<p>GitHub Issue run script</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>#!/usr/bin/env python3\nimport os\nfrom pathlib import Path\n\nimport requests\nimport typer\nimport yaml\nfrom rich.console import Console\n\nfrom minisweagent.agents.interactive import InteractiveAgent\nfrom minisweagent.config import builtin_config_dir, get_config_path\nfrom minisweagent.environments.docker import DockerEnvironment\nfrom minisweagent.models import get_model\nfrom minisweagent.run.extra.config import configure_if_first_time\nfrom minisweagent.run.utils.save import save_traj\n\nDEFAULT_CONFIG = Path(os.getenv(\"MSWEA_GITHUB_CONFIG_PATH\", builtin_config_dir / \"github_issue.yaml\"))\nconsole = Console(highlight=False)\napp = typer.Typer(rich_markup_mode=\"rich\", add_completion=False)\n\n\ndef fetch_github_issue(issue_url: str) -&gt; str:\n    \"\"\"Fetch GitHub issue text from the URL.\"\"\"\n    # Convert GitHub issue URL to API URL\n    api_url = issue_url.replace(\"github.com\", \"api.github.com/repos\").replace(\"/issues/\", \"/issues/\")\n\n    headers = {}\n    if github_token := os.getenv(\"GITHUB_TOKEN\"):\n        headers[\"Authorization\"] = f\"token {github_token}\"\n\n    response = requests.get(api_url, headers=headers)\n    issue_data = response.json()\n\n    title = issue_data[\"title\"]\n    body = issue_data[\"body\"] or \"\"\n\n    return f\"GitHub Issue: {title}\\n\\n{body}\"\n\n\n# fmt: off\n@app.command()\ndef main(\n    issue_url: str = typer.Option(prompt=\"Enter GitHub issue URL\", help=\"GitHub issue URL\"),\n    config: Path = typer.Option(DEFAULT_CONFIG, \"-c\", \"--config\", help=\"Path to config file\"),\n    model: str | None = typer.Option(None, \"-m\", \"--model\", help=\"Model to use\"),\n    model_class: str | None = typer.Option(None, \"--model-class\", help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\", rich_help_panel=\"Advanced\"),\n    yolo: bool = typer.Option(False, \"-y\", \"--yolo\", help=\"Run without confirmation\"),\n) -&gt; InteractiveAgent:\n    # fmt: on\n    \"\"\"Run mini-SWE-agent on a GitHub issue\"\"\"\n    configure_if_first_time()\n\n    config_path = get_config_path(config)\n    console.print(f\"Loading agent config from [bold green]'{config_path}'[/bold green]\")\n    _config = yaml.safe_load(config_path.read_text())\n    _agent_config = _config.setdefault(\"agent\", {})\n    if yolo:\n        _agent_config[\"mode\"] = \"yolo\"\n    if model_class is not None:\n        _config.setdefault(\"model\", {})[\"model_class\"] = model_class\n\n    task = fetch_github_issue(issue_url)\n\n    agent = InteractiveAgent(\n        get_model(model, _config.get(\"model\", {})),\n        DockerEnvironment(**_config.get(\"environment\", {})),\n        **_agent_config,\n    )\n\n    repo_url = issue_url.split(\"/issues/\")[0]\n    if github_token := os.getenv(\"GITHUB_TOKEN\"):\n        repo_url = repo_url.replace(\"https://github.com/\", f\"https://{github_token}@github.com/\") + \".git\"\n\n    agent.env.execute(f\"git clone {repo_url} /testbed\", cwd=\"/\")\n\n    exit_status, result = None, None\n    try:\n        exit_status, result = agent.run(task)\n    except KeyboardInterrupt:\n        console.print(\"\\n[bold red]KeyboardInterrupt -- goodbye[/bold red]\")\n    finally:\n        save_traj(agent, Path(\"traj.json\"), exit_status=exit_status, result=result)\n    return agent\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/run/github_issue/#minisweagent.run.github_issue","title":"minisweagent.run.github_issue","text":""},{"location":"reference/run/github_issue/#minisweagent.run.github_issue.DEFAULT_CONFIG","title":"DEFAULT_CONFIG  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CONFIG = Path(\n    getenv(\n        \"MSWEA_GITHUB_CONFIG_PATH\",\n        builtin_config_dir / \"github_issue.yaml\",\n    )\n)\n</code></pre>"},{"location":"reference/run/github_issue/#minisweagent.run.github_issue.console","title":"console  <code>module-attribute</code>","text":"<pre><code>console = Console(highlight=False)\n</code></pre>"},{"location":"reference/run/github_issue/#minisweagent.run.github_issue.app","title":"app  <code>module-attribute</code>","text":"<pre><code>app = Typer(rich_markup_mode='rich', add_completion=False)\n</code></pre>"},{"location":"reference/run/github_issue/#minisweagent.run.github_issue.fetch_github_issue","title":"fetch_github_issue","text":"<pre><code>fetch_github_issue(issue_url: str) -&gt; str\n</code></pre> <p>Fetch GitHub issue text from the URL.</p> Source code in <code>src/minisweagent/run/github_issue.py</code> <pre><code>def fetch_github_issue(issue_url: str) -&gt; str:\n    \"\"\"Fetch GitHub issue text from the URL.\"\"\"\n    # Convert GitHub issue URL to API URL\n    api_url = issue_url.replace(\"github.com\", \"api.github.com/repos\").replace(\"/issues/\", \"/issues/\")\n\n    headers = {}\n    if github_token := os.getenv(\"GITHUB_TOKEN\"):\n        headers[\"Authorization\"] = f\"token {github_token}\"\n\n    response = requests.get(api_url, headers=headers)\n    issue_data = response.json()\n\n    title = issue_data[\"title\"]\n    body = issue_data[\"body\"] or \"\"\n\n    return f\"GitHub Issue: {title}\\n\\n{body}\"\n</code></pre>"},{"location":"reference/run/github_issue/#minisweagent.run.github_issue.main","title":"main","text":"<pre><code>main(\n    issue_url: str = Option(\n        prompt=\"Enter GitHub issue URL\",\n        help=\"GitHub issue URL\",\n    ),\n    config: Path = Option(\n        DEFAULT_CONFIG,\n        \"-c\",\n        \"--config\",\n        help=\"Path to config file\",\n    ),\n    model: str | None = Option(\n        None, \"-m\", \"--model\", help=\"Model to use\"\n    ),\n    model_class: str | None = Option(\n        None,\n        \"--model-class\",\n        help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\",\n        rich_help_panel=\"Advanced\",\n    ),\n    yolo: bool = Option(\n        False,\n        \"-y\",\n        \"--yolo\",\n        help=\"Run without confirmation\",\n    ),\n) -&gt; InteractiveAgent\n</code></pre> <p>Run mini-SWE-agent on a GitHub issue</p> Source code in <code>src/minisweagent/run/github_issue.py</code> <pre><code>@app.command()\ndef main(\n    issue_url: str = typer.Option(prompt=\"Enter GitHub issue URL\", help=\"GitHub issue URL\"),\n    config: Path = typer.Option(DEFAULT_CONFIG, \"-c\", \"--config\", help=\"Path to config file\"),\n    model: str | None = typer.Option(None, \"-m\", \"--model\", help=\"Model to use\"),\n    model_class: str | None = typer.Option(None, \"--model-class\", help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\", rich_help_panel=\"Advanced\"),\n    yolo: bool = typer.Option(False, \"-y\", \"--yolo\", help=\"Run without confirmation\"),\n) -&gt; InteractiveAgent:\n    # fmt: on\n    \"\"\"Run mini-SWE-agent on a GitHub issue\"\"\"\n    configure_if_first_time()\n\n    config_path = get_config_path(config)\n    console.print(f\"Loading agent config from [bold green]'{config_path}'[/bold green]\")\n    _config = yaml.safe_load(config_path.read_text())\n    _agent_config = _config.setdefault(\"agent\", {})\n    if yolo:\n        _agent_config[\"mode\"] = \"yolo\"\n    if model_class is not None:\n        _config.setdefault(\"model\", {})[\"model_class\"] = model_class\n\n    task = fetch_github_issue(issue_url)\n\n    agent = InteractiveAgent(\n        get_model(model, _config.get(\"model\", {})),\n        DockerEnvironment(**_config.get(\"environment\", {})),\n        **_agent_config,\n    )\n\n    repo_url = issue_url.split(\"/issues/\")[0]\n    if github_token := os.getenv(\"GITHUB_TOKEN\"):\n        repo_url = repo_url.replace(\"https://github.com/\", f\"https://{github_token}@github.com/\") + \".git\"\n\n    agent.env.execute(f\"git clone {repo_url} /testbed\", cwd=\"/\")\n\n    exit_status, result = None, None\n    try:\n        exit_status, result = agent.run(task)\n    except KeyboardInterrupt:\n        console.print(\"\\n[bold red]KeyboardInterrupt -- goodbye[/bold red]\")\n    finally:\n        save_traj(agent, Path(\"traj.json\"), exit_status=exit_status, result=result)\n    return agent\n</code></pre>"},{"location":"reference/run/hello_world/","title":"Hello World","text":""},{"location":"reference/run/hello_world/#hello-world","title":"Hello World","text":"<p>Hello World run script</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>import os\nfrom pathlib import Path\n\nfrom minisweagent.models.tensorzero import TensorZeroModel\nimport typer\nimport yaml\n\nfrom minisweagent import package_dir\nfrom minisweagent.agents.default import DefaultAgent\nfrom minisweagent.environments.local import LocalEnvironment\nfrom minisweagent.models.litellm_model import LitellmModel\n\napp = typer.Typer()\n\n\n@app.command()\ndef main(\n    task: str = typer.Option(..., \"-t\", \"--task\", help=\"Task/problem statement\", show_default=False, prompt=True),\n    model_name: str = typer.Option(\n        os.getenv(\"MSWEA_MODEL_NAME\"),\n        \"-m\",\n        \"--model\",\n        help=\"Model name (defaults to MSWEA_MODEL_NAME env var)\",\n        prompt=\"What model do you want to use?\",\n    ),\n) -&gt; DefaultAgent:\n    agent = DefaultAgent(\n        TensorZeroModel(config_file=\"/Users/viraj/tensorzero/mini-swe-agent/src/minisweagent/models/tensorzero/tensorzero.toml\"),\n        LocalEnvironment(),\n        **yaml.safe_load(Path(package_dir / \"config\" / \"default.yaml\").read_text())[\"agent\"],\n    )\n    agent.run(task)\n    return agent\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/run/hello_world/#minisweagent.run.hello_world","title":"minisweagent.run.hello_world","text":""},{"location":"reference/run/hello_world/#minisweagent.run.hello_world.app","title":"app  <code>module-attribute</code>","text":"<pre><code>app = Typer()\n</code></pre>"},{"location":"reference/run/hello_world/#minisweagent.run.hello_world.main","title":"main","text":"<pre><code>main(\n    task: str = Option(\n        ...,\n        \"-t\",\n        \"--task\",\n        help=\"Task/problem statement\",\n        show_default=False,\n        prompt=True,\n    ),\n    model_name: str = Option(\n        getenv(\"MSWEA_MODEL_NAME\"),\n        \"-m\",\n        \"--model\",\n        help=\"Model name (defaults to MSWEA_MODEL_NAME env var)\",\n        prompt=\"What model do you want to use?\",\n    ),\n) -&gt; DefaultAgent\n</code></pre> Source code in <code>src/minisweagent/run/hello_world.py</code> <pre><code>@app.command()\ndef main(\n    task: str = typer.Option(..., \"-t\", \"--task\", help=\"Task/problem statement\", show_default=False, prompt=True),\n    model_name: str = typer.Option(\n        os.getenv(\"MSWEA_MODEL_NAME\"),\n        \"-m\",\n        \"--model\",\n        help=\"Model name (defaults to MSWEA_MODEL_NAME env var)\",\n        prompt=\"What model do you want to use?\",\n    ),\n) -&gt; DefaultAgent:\n    agent = DefaultAgent(\n        TensorZeroModel(config_file=\"/Users/viraj/tensorzero/mini-swe-agent/src/minisweagent/models/tensorzero/tensorzero.toml\"),\n        LocalEnvironment(),\n        **yaml.safe_load(Path(package_dir / \"config\" / \"default.yaml\").read_text())[\"agent\"],\n    )\n    agent.run(task)\n    return agent\n</code></pre>"},{"location":"reference/run/mini/","title":"mini","text":""},{"location":"reference/run/mini/#local","title":"Local","text":"<p>Mini run script</p> <ul> <li>Read on GitHub</li> </ul> Full source code <pre><code>#!/usr/bin/env python3\n\n\"\"\"Run mini-SWE-agent in your local environment. This is the default executable `mini`.\"\"\"\n# Read this first: https://mini-swe-agent.com/latest/usage/mini/  (usage)\n\nimport os\nimport traceback\nfrom pathlib import Path\nfrom typing import Any\n\nimport typer\nimport yaml\nfrom prompt_toolkit.formatted_text import HTML\nfrom prompt_toolkit.history import FileHistory\nfrom prompt_toolkit.shortcuts import PromptSession\nfrom rich.console import Console\n\nfrom minisweagent import global_config_dir\nfrom minisweagent.agents.interactive import InteractiveAgent\nfrom minisweagent.agents.interactive_textual import TextualAgent\nfrom minisweagent.config import builtin_config_dir, get_config_path\nfrom minisweagent.environments.local import LocalEnvironment\nfrom minisweagent.models import get_model\nfrom minisweagent.run.extra.config import configure_if_first_time\nfrom minisweagent.run.utils.save import save_traj\nfrom minisweagent.utils.log import logger\n\nDEFAULT_CONFIG = Path(os.getenv(\"MSWEA_MINI_CONFIG_PATH\", builtin_config_dir / \"mini.yaml\"))\nDEFAULT_OUTPUT = global_config_dir / \"last_mini_run.traj.json\"\nconsole = Console(highlight=False)\napp = typer.Typer(rich_markup_mode=\"rich\")\nprompt_session = PromptSession(history=FileHistory(global_config_dir / \"mini_task_history.txt\"))\n_HELP_TEXT = \"\"\"Run mini-SWE-agent in your local environment.\n\n[not dim]\nThere are two different user interfaces:\n\n[bold green]mini[/bold green] Simple REPL-style interface\n[bold green]mini -v[/bold green] Pager-style interface (Textual)\n\nMore information about the usage: [bold green]https://mini-swe-agent.com/latest/usage/mini/[/bold green]\n[/not dim]\n\"\"\"\n\n\n# fmt: off\n@app.command(help=_HELP_TEXT)\ndef main(\n    visual: bool = typer.Option(False, \"-v\", \"--visual\", help=\"Toggle (pager-style) UI (Textual) depending on the MSWEA_VISUAL_MODE_DEFAULT environment setting\",),\n    model_name: str | None = typer.Option( None, \"-m\", \"--model\", help=\"Model to use\",),\n    model_class: str | None = typer.Option(None, \"--model-class\", help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\", rich_help_panel=\"Advanced\"),\n    task: str | None = typer.Option(None, \"-t\", \"--task\", help=\"Task/problem statement\", show_default=False),\n    yolo: bool = typer.Option(False, \"-y\", \"--yolo\", help=\"Run without confirmation\"),\n    cost_limit: float | None = typer.Option(None, \"-l\", \"--cost-limit\", help=\"Cost limit. Set to 0 to disable.\"),\n    config_spec: Path = typer.Option(DEFAULT_CONFIG, \"-c\", \"--config\", help=\"Path to config file\"),\n    output: Path | None = typer.Option(DEFAULT_OUTPUT, \"-o\", \"--output\", help=\"Output trajectory file\"),\n    exit_immediately: bool = typer.Option( False, \"--exit-immediately\", help=\"Exit immediately when the agent wants to finish instead of prompting.\", rich_help_panel=\"Advanced\"),\n) -&gt; Any:\n    # fmt: on\n    configure_if_first_time()\n    config_path = get_config_path(config_spec)\n    console.print(f\"Loading agent config from [bold green]'{config_path}'[/bold green]\")\n    config = yaml.safe_load(config_path.read_text())\n\n    if not task:\n        console.print(\"[bold yellow]What do you want to do?\")\n        task = prompt_session.prompt(\n            \"\",\n            multiline=True,\n            bottom_toolbar=HTML(\n                \"Submit task: &lt;b fg='yellow' bg='black'&gt;Esc+Enter&lt;/b&gt; | \"\n                \"Navigate history: &lt;b fg='yellow' bg='black'&gt;Arrow Up/Down&lt;/b&gt; | \"\n                \"Search history: &lt;b fg='yellow' bg='black'&gt;Ctrl+R&lt;/b&gt;\"\n            ),\n        )\n        console.print(\"[bold green]Got that, thanks![/bold green]\")\n\n    if yolo:\n        config.setdefault(\"agent\", {})[\"mode\"] = \"yolo\"\n    if cost_limit:\n        config.setdefault(\"agent\", {})[\"cost_limit\"] = cost_limit\n    if exit_immediately:\n        config.setdefault(\"agent\", {})[\"confirm_exit\"] = False\n    if model_class is not None:\n        config.setdefault(\"model\", {})[\"model_class\"] = model_class\n    model = get_model(model_name, config.get(\"model\", {}))\n    env = LocalEnvironment(**config.get(\"env\", {}))\n\n    # Both visual flag and the MSWEA_VISUAL_MODE_DEFAULT flip the mode, so it's essentially a XOR\n    agent_class = InteractiveAgent\n    if visual == (os.getenv(\"MSWEA_VISUAL_MODE_DEFAULT\", \"false\") == \"false\"):\n        agent_class = TextualAgent\n\n    agent = agent_class(model, env, **config.get(\"agent\", {}))\n    exit_status, result, extra_info = None, None, None\n    try:\n        exit_status, result = agent.run(task)  # type: ignore[arg-type]\n    except Exception as e:\n        logger.error(f\"Error running agent: {e}\", exc_info=True)\n        exit_status, result = type(e).__name__, str(e)\n        extra_info = {\"traceback\": traceback.format_exc()}\n    finally:\n        if output:\n            save_traj(agent, output, exit_status=exit_status, result=result, extra_info=extra_info)  # type: ignore[arg-type]\n    return agent\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/run/mini/#minisweagent.run.mini","title":"minisweagent.run.mini","text":"<p>Run mini-SWE-agent in your local environment. This is the default executable <code>mini</code>.</p>"},{"location":"reference/run/mini/#minisweagent.run.mini.DEFAULT_CONFIG","title":"DEFAULT_CONFIG  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CONFIG = Path(\n    getenv(\n        \"MSWEA_MINI_CONFIG_PATH\",\n        builtin_config_dir / \"mini.yaml\",\n    )\n)\n</code></pre>"},{"location":"reference/run/mini/#minisweagent.run.mini.DEFAULT_OUTPUT","title":"DEFAULT_OUTPUT  <code>module-attribute</code>","text":"<pre><code>DEFAULT_OUTPUT = (\n    global_config_dir / \"last_mini_run.traj.json\"\n)\n</code></pre>"},{"location":"reference/run/mini/#minisweagent.run.mini.console","title":"console  <code>module-attribute</code>","text":"<pre><code>console = Console(highlight=False)\n</code></pre>"},{"location":"reference/run/mini/#minisweagent.run.mini.app","title":"app  <code>module-attribute</code>","text":"<pre><code>app = Typer(rich_markup_mode='rich')\n</code></pre>"},{"location":"reference/run/mini/#minisweagent.run.mini.prompt_session","title":"prompt_session  <code>module-attribute</code>","text":"<pre><code>prompt_session = PromptSession(\n    history=FileHistory(\n        global_config_dir / \"mini_task_history.txt\"\n    )\n)\n</code></pre>"},{"location":"reference/run/mini/#minisweagent.run.mini.main","title":"main","text":"<pre><code>main(\n    visual: bool = Option(\n        False,\n        \"-v\",\n        \"--visual\",\n        help=\"Toggle (pager-style) UI (Textual) depending on the MSWEA_VISUAL_MODE_DEFAULT environment setting\",\n    ),\n    model_name: str | None = Option(\n        None, \"-m\", \"--model\", help=\"Model to use\"\n    ),\n    model_class: str | None = Option(\n        None,\n        \"--model-class\",\n        help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\",\n        rich_help_panel=\"Advanced\",\n    ),\n    task: str | None = Option(\n        None,\n        \"-t\",\n        \"--task\",\n        help=\"Task/problem statement\",\n        show_default=False,\n    ),\n    yolo: bool = Option(\n        False,\n        \"-y\",\n        \"--yolo\",\n        help=\"Run without confirmation\",\n    ),\n    cost_limit: float | None = Option(\n        None,\n        \"-l\",\n        \"--cost-limit\",\n        help=\"Cost limit. Set to 0 to disable.\",\n    ),\n    config_spec: Path = Option(\n        DEFAULT_CONFIG,\n        \"-c\",\n        \"--config\",\n        help=\"Path to config file\",\n    ),\n    output: Path | None = Option(\n        DEFAULT_OUTPUT,\n        \"-o\",\n        \"--output\",\n        help=\"Output trajectory file\",\n    ),\n    exit_immediately: bool = Option(\n        False,\n        \"--exit-immediately\",\n        help=\"Exit immediately when the agent wants to finish instead of prompting.\",\n        rich_help_panel=\"Advanced\",\n    ),\n) -&gt; Any\n</code></pre> Source code in <code>src/minisweagent/run/mini.py</code> <pre><code>@app.command(help=_HELP_TEXT)\ndef main(\n    visual: bool = typer.Option(False, \"-v\", \"--visual\", help=\"Toggle (pager-style) UI (Textual) depending on the MSWEA_VISUAL_MODE_DEFAULT environment setting\",),\n    model_name: str | None = typer.Option( None, \"-m\", \"--model\", help=\"Model to use\",),\n    model_class: str | None = typer.Option(None, \"--model-class\", help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\", rich_help_panel=\"Advanced\"),\n    task: str | None = typer.Option(None, \"-t\", \"--task\", help=\"Task/problem statement\", show_default=False),\n    yolo: bool = typer.Option(False, \"-y\", \"--yolo\", help=\"Run without confirmation\"),\n    cost_limit: float | None = typer.Option(None, \"-l\", \"--cost-limit\", help=\"Cost limit. Set to 0 to disable.\"),\n    config_spec: Path = typer.Option(DEFAULT_CONFIG, \"-c\", \"--config\", help=\"Path to config file\"),\n    output: Path | None = typer.Option(DEFAULT_OUTPUT, \"-o\", \"--output\", help=\"Output trajectory file\"),\n    exit_immediately: bool = typer.Option( False, \"--exit-immediately\", help=\"Exit immediately when the agent wants to finish instead of prompting.\", rich_help_panel=\"Advanced\"),\n) -&gt; Any:\n    # fmt: on\n    configure_if_first_time()\n    config_path = get_config_path(config_spec)\n    console.print(f\"Loading agent config from [bold green]'{config_path}'[/bold green]\")\n    config = yaml.safe_load(config_path.read_text())\n\n    if not task:\n        console.print(\"[bold yellow]What do you want to do?\")\n        task = prompt_session.prompt(\n            \"\",\n            multiline=True,\n            bottom_toolbar=HTML(\n                \"Submit task: &lt;b fg='yellow' bg='black'&gt;Esc+Enter&lt;/b&gt; | \"\n                \"Navigate history: &lt;b fg='yellow' bg='black'&gt;Arrow Up/Down&lt;/b&gt; | \"\n                \"Search history: &lt;b fg='yellow' bg='black'&gt;Ctrl+R&lt;/b&gt;\"\n            ),\n        )\n        console.print(\"[bold green]Got that, thanks![/bold green]\")\n\n    if yolo:\n        config.setdefault(\"agent\", {})[\"mode\"] = \"yolo\"\n    if cost_limit:\n        config.setdefault(\"agent\", {})[\"cost_limit\"] = cost_limit\n    if exit_immediately:\n        config.setdefault(\"agent\", {})[\"confirm_exit\"] = False\n    if model_class is not None:\n        config.setdefault(\"model\", {})[\"model_class\"] = model_class\n    model = get_model(model_name, config.get(\"model\", {}))\n    env = LocalEnvironment(**config.get(\"env\", {}))\n\n    # Both visual flag and the MSWEA_VISUAL_MODE_DEFAULT flip the mode, so it's essentially a XOR\n    agent_class = InteractiveAgent\n    if visual == (os.getenv(\"MSWEA_VISUAL_MODE_DEFAULT\", \"false\") == \"false\"):\n        agent_class = TextualAgent\n\n    agent = agent_class(model, env, **config.get(\"agent\", {}))\n    exit_status, result, extra_info = None, None, None\n    try:\n        exit_status, result = agent.run(task)  # type: ignore[arg-type]\n    except Exception as e:\n        logger.error(f\"Error running agent: {e}\", exc_info=True)\n        exit_status, result = type(e).__name__, str(e)\n        extra_info = {\"traceback\": traceback.format_exc()}\n    finally:\n        if output:\n            save_traj(agent, output, exit_status=exit_status, result=result, extra_info=extra_info)  # type: ignore[arg-type]\n    return agent\n</code></pre>"},{"location":"reference/run/swebench/","title":"SWE-bench (batch)","text":""},{"location":"reference/run/swebench/#swe-bench","title":"SWE-bench","text":"<p>SWE-bench run script</p> <ul> <li>Read on GitHub</li> </ul> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench","title":"minisweagent.run.extra.swebench","text":"<p>Run mini-SWE-agent on SWE-bench instances in batch mode.</p>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.app","title":"app  <code>module-attribute</code>","text":"<pre><code>app = Typer(rich_markup_mode='rich', add_completion=False)\n</code></pre>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.DATASET_MAPPING","title":"DATASET_MAPPING  <code>module-attribute</code>","text":"<pre><code>DATASET_MAPPING = {\n    \"full\": \"princeton-nlp/SWE-Bench\",\n    \"verified\": \"princeton-nlp/SWE-Bench_Verified\",\n    \"lite\": \"princeton-nlp/SWE-Bench_Lite\",\n    \"multimodal\": \"princeton-nlp/SWE-Bench_Multimodal\",\n    \"multilingual\": \"swe-bench/SWE-Bench_Multilingual\",\n    \"smith\": \"SWE-bench/SWE-smith\",\n    \"_test\": \"klieret/swe-bench-dummy-test-dataset\",\n}\n</code></pre>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.ProgressTrackingAgent","title":"ProgressTrackingAgent","text":"<pre><code>ProgressTrackingAgent(\n    *args,\n    progress_manager: RunBatchProgressManager,\n    instance_id: str = \"\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DefaultAgent</code></p> <p>Simple wrapper around DefaultAgent that provides progress updates.</p> Source code in <code>src/minisweagent/run/extra/swebench.py</code> <pre><code>def __init__(self, *args, progress_manager: RunBatchProgressManager, instance_id: str = \"\", **kwargs):\n    super().__init__(*args, **kwargs)\n    self.progress_manager: RunBatchProgressManager = progress_manager\n    self.instance_id = instance_id\n</code></pre>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.ProgressTrackingAgent.progress_manager","title":"progress_manager  <code>instance-attribute</code>","text":"<pre><code>progress_manager: RunBatchProgressManager = progress_manager\n</code></pre>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.ProgressTrackingAgent.instance_id","title":"instance_id  <code>instance-attribute</code>","text":"<pre><code>instance_id = instance_id\n</code></pre>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.ProgressTrackingAgent.step","title":"step","text":"<pre><code>step() -&gt; dict\n</code></pre> <p>Override step to provide progress updates.</p> Source code in <code>src/minisweagent/run/extra/swebench.py</code> <pre><code>def step(self) -&gt; dict:\n    \"\"\"Override step to provide progress updates.\"\"\"\n    self.progress_manager.update_instance_status(\n        self.instance_id, f\"Step {self.model.n_calls + 1:3d} (${self.model.cost:.2f})\"\n    )\n    return super().step()\n</code></pre>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.get_swebench_docker_image_name","title":"get_swebench_docker_image_name","text":"<pre><code>get_swebench_docker_image_name(instance: dict) -&gt; str\n</code></pre> <p>Get the image name for a SWEBench instance.</p> Source code in <code>src/minisweagent/run/extra/swebench.py</code> <pre><code>def get_swebench_docker_image_name(instance: dict) -&gt; str:\n    \"\"\"Get the image name for a SWEBench instance.\"\"\"\n    image_name = instance.get(\"image_name\", None)\n    if image_name is None:\n        # Docker doesn't allow double underscore, so we replace them with a magic token\n        iid = instance[\"instance_id\"]\n        id_docker_compatible = iid.replace(\"__\", \"_1776_\")\n        image_name = f\"docker.io/swebench/sweb.eval.x86_64.{id_docker_compatible}:latest\".lower()\n    return image_name\n</code></pre>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.get_sb_environment","title":"get_sb_environment","text":"<pre><code>get_sb_environment(\n    config: dict, instance: dict\n) -&gt; Environment\n</code></pre> Source code in <code>src/minisweagent/run/extra/swebench.py</code> <pre><code>def get_sb_environment(config: dict, instance: dict) -&gt; Environment:\n    env_config = config.setdefault(\"environment\", {})\n    env_config[\"environment_class\"] = env_config.get(\"environment_class\", \"docker\")\n    image_name = get_swebench_docker_image_name(instance)\n    if env_config[\"environment_class\"] == \"docker\":\n        env_config[\"image\"] = image_name\n    elif env_config[\"environment_class\"] == \"singularity\":\n        env_config[\"image\"] = \"docker://\" + image_name\n    env = get_environment(env_config)\n    if startup_command := config.get(\"run\", {}).get(\"env_startup_command\"):\n        startup_command = Template(startup_command, undefined=StrictUndefined).render(**instance)\n        out = env.execute(startup_command)\n        if out[\"returncode\"] != 0:\n            raise RuntimeError(f\"Error executing startup command: {out}\")\n    return env\n</code></pre>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.update_preds_file","title":"update_preds_file","text":"<pre><code>update_preds_file(\n    output_path: Path,\n    instance_id: str,\n    model_name: str,\n    result: str,\n)\n</code></pre> <p>Update the output JSON file with results from a single instance.</p> Source code in <code>src/minisweagent/run/extra/swebench.py</code> <pre><code>def update_preds_file(output_path: Path, instance_id: str, model_name: str, result: str):\n    \"\"\"Update the output JSON file with results from a single instance.\"\"\"\n    with _OUTPUT_FILE_LOCK:\n        output_data = {}\n        if output_path.exists():\n            output_data = json.loads(output_path.read_text())\n        output_data[instance_id] = {\n            \"model_name_or_path\": model_name,\n            \"instance_id\": instance_id,\n            \"model_patch\": result,\n        }\n        output_path.write_text(json.dumps(output_data, indent=2))\n</code></pre>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.remove_from_preds_file","title":"remove_from_preds_file","text":"<pre><code>remove_from_preds_file(output_path: Path, instance_id: str)\n</code></pre> <p>Remove an instance from the predictions file.</p> Source code in <code>src/minisweagent/run/extra/swebench.py</code> <pre><code>def remove_from_preds_file(output_path: Path, instance_id: str):\n    \"\"\"Remove an instance from the predictions file.\"\"\"\n    if not output_path.exists():\n        return\n    with _OUTPUT_FILE_LOCK:\n        output_data = json.loads(output_path.read_text())\n        if instance_id in output_data:\n            del output_data[instance_id]\n            output_path.write_text(json.dumps(output_data, indent=2))\n</code></pre>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.process_instance","title":"process_instance","text":"<pre><code>process_instance(\n    instance: dict,\n    output_dir: Path,\n    config: dict,\n    progress_manager: RunBatchProgressManager,\n) -&gt; None\n</code></pre> <p>Process a single SWEBench instance.</p> Source code in <code>src/minisweagent/run/extra/swebench.py</code> <pre><code>def process_instance(\n    instance: dict,\n    output_dir: Path,\n    config: dict,\n    progress_manager: RunBatchProgressManager,\n) -&gt; None:\n    \"\"\"Process a single SWEBench instance.\"\"\"\n    instance_id = instance[\"instance_id\"]\n    instance_dir = output_dir / instance_id\n    # avoid inconsistent state if something here fails and there's leftover previous files\n    remove_from_preds_file(output_dir / \"preds.json\", instance_id)\n    (instance_dir / f\"{instance_id}.traj.json\").unlink(missing_ok=True)\n    model = get_model(config=config.get(\"model\", {}))\n    task = instance[\"problem_statement\"]\n\n    progress_manager.on_instance_start(instance_id)\n    progress_manager.update_instance_status(instance_id, \"Pulling/starting docker\")\n\n    agent = None\n    extra_info = None\n\n    try:\n        env = get_sb_environment(config, instance)\n        agent = ProgressTrackingAgent(\n            model,\n            env,\n            progress_manager=progress_manager,\n            instance_id=instance_id,\n            **config.get(\"agent\", {}),\n        )\n        exit_status, result = agent.run(task)\n    except Exception as e:\n        logger.error(f\"Error processing instance {instance_id}: {e}\", exc_info=True)\n        exit_status, result = type(e).__name__, str(e)\n        extra_info = {\"traceback\": traceback.format_exc()}\n    finally:\n        save_traj(\n            agent,\n            instance_dir / f\"{instance_id}.traj.json\",\n            exit_status=exit_status,\n            result=result,\n            extra_info=extra_info,\n            instance_id=instance_id,\n            print_fct=logger.info,\n        )\n        update_preds_file(output_dir / \"preds.json\", instance_id, model.config.model_name, result)\n        progress_manager.on_instance_end(instance_id, exit_status)\n</code></pre>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.filter_instances","title":"filter_instances","text":"<pre><code>filter_instances(\n    instances: list[dict],\n    *,\n    filter_spec: str,\n    slice_spec: str = \"\",\n    shuffle: bool = False,\n) -&gt; list[dict]\n</code></pre> <p>Filter and slice a list of SWEBench instances.</p> Source code in <code>src/minisweagent/run/extra/swebench.py</code> <pre><code>def filter_instances(\n    instances: list[dict], *, filter_spec: str, slice_spec: str = \"\", shuffle: bool = False\n) -&gt; list[dict]:\n    \"\"\"Filter and slice a list of SWEBench instances.\"\"\"\n    if shuffle:\n        instances = sorted(instances.copy(), key=lambda x: x[\"instance_id\"])\n        random.seed(42)\n        random.shuffle(instances)\n    before_filter = len(instances)\n    instances = [instance for instance in instances if re.match(filter_spec, instance[\"instance_id\"])]\n    if (after_filter := len(instances)) != before_filter:\n        logger.info(f\"Instance filter: {before_filter} -&gt; {after_filter} instances\")\n    if slice_spec:\n        values = [int(x) if x else None for x in slice_spec.split(\":\")]\n        instances = instances[slice(*values)]\n        if (after_slice := len(instances)) != before_filter:\n            logger.info(f\"Instance slice: {before_filter} -&gt; {after_slice} instances\")\n    return instances\n</code></pre>"},{"location":"reference/run/swebench/#minisweagent.run.extra.swebench.main","title":"main","text":"<pre><code>main(\n    subset: str = Option(\n        \"lite\",\n        \"--subset\",\n        help=\"SWEBench subset to use or path to a dataset\",\n        rich_help_panel=\"Data selection\",\n    ),\n    split: str = Option(\n        \"dev\",\n        \"--split\",\n        help=\"Dataset split\",\n        rich_help_panel=\"Data selection\",\n    ),\n    slice_spec: str = Option(\n        \"\",\n        \"--slice\",\n        help=\"Slice specification (e.g., '0:5' for first 5 instances)\",\n        rich_help_panel=\"Data selection\",\n    ),\n    filter_spec: str = Option(\n        \"\",\n        \"--filter\",\n        help=\"Filter instance IDs by regex\",\n        rich_help_panel=\"Data selection\",\n    ),\n    shuffle: bool = Option(\n        False,\n        \"--shuffle\",\n        help=\"Shuffle instances\",\n        rich_help_panel=\"Data selection\",\n    ),\n    output: str = Option(\n        \"\",\n        \"-o\",\n        \"--output\",\n        help=\"Output directory\",\n        rich_help_panel=\"Basic\",\n    ),\n    workers: int = Option(\n        1,\n        \"-w\",\n        \"--workers\",\n        help=\"Number of worker threads for parallel processing\",\n        rich_help_panel=\"Basic\",\n    ),\n    model: str | None = Option(\n        None,\n        \"-m\",\n        \"--model\",\n        help=\"Model to use\",\n        rich_help_panel=\"Basic\",\n    ),\n    model_class: str | None = Option(\n        None,\n        \"-c\",\n        \"--model-class\",\n        help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\",\n        rich_help_panel=\"Advanced\",\n    ),\n    redo_existing: bool = Option(\n        False,\n        \"--redo-existing\",\n        help=\"Redo existing instances\",\n        rich_help_panel=\"Data selection\",\n    ),\n    config_spec: Path = Option(\n        builtin_config_dir / \"extra\" / \"swebench.yaml\",\n        \"-c\",\n        \"--config\",\n        help=\"Path to a config file\",\n        rich_help_panel=\"Basic\",\n    ),\n    environment_class: str | None = Option(\n        None,\n        \"--environment-class\",\n        help=\"Environment type to use. Recommended are docker or singularity\",\n        rich_help_panel=\"Advanced\",\n    ),\n) -&gt; None\n</code></pre> Source code in <code>src/minisweagent/run/extra/swebench.py</code> <pre><code>@app.command(help=_HELP_TEXT)\ndef main(\n    subset: str = typer.Option(\"lite\", \"--subset\", help=\"SWEBench subset to use or path to a dataset\", rich_help_panel=\"Data selection\"),\n    split: str = typer.Option(\"dev\", \"--split\", help=\"Dataset split\", rich_help_panel=\"Data selection\"),\n    slice_spec: str = typer.Option(\"\", \"--slice\", help=\"Slice specification (e.g., '0:5' for first 5 instances)\", rich_help_panel=\"Data selection\"),\n    filter_spec: str = typer.Option(\"\", \"--filter\", help=\"Filter instance IDs by regex\", rich_help_panel=\"Data selection\"),\n    shuffle: bool = typer.Option(False, \"--shuffle\", help=\"Shuffle instances\", rich_help_panel=\"Data selection\"),\n    output: str = typer.Option(\"\", \"-o\", \"--output\", help=\"Output directory\", rich_help_panel=\"Basic\"),\n    workers: int = typer.Option(1, \"-w\", \"--workers\", help=\"Number of worker threads for parallel processing\", rich_help_panel=\"Basic\"),\n    model: str | None = typer.Option(None, \"-m\", \"--model\", help=\"Model to use\", rich_help_panel=\"Basic\"),\n    model_class: str | None = typer.Option(None, \"-c\", \"--model-class\", help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\", rich_help_panel=\"Advanced\"),\n    redo_existing: bool = typer.Option(False, \"--redo-existing\", help=\"Redo existing instances\", rich_help_panel=\"Data selection\"),\n    config_spec: Path = typer.Option( builtin_config_dir / \"extra\" / \"swebench.yaml\", \"-c\", \"--config\", help=\"Path to a config file\", rich_help_panel=\"Basic\"),\n    environment_class: str | None = typer.Option( None, \"--environment-class\", help=\"Environment type to use. Recommended are docker or singularity\", rich_help_panel=\"Advanced\"),\n) -&gt; None:\n    # fmt: on\n    output_path = Path(output)\n    output_path.mkdir(parents=True, exist_ok=True)\n    logger.info(f\"Results will be saved to {output_path}\")\n    add_file_handler(output_path / \"minisweagent.log\")\n\n    dataset_path = DATASET_MAPPING.get(subset, subset)\n    logger.info(f\"Loading dataset {dataset_path}, split {split}...\")\n    instances = list(load_dataset(dataset_path, split=split))\n\n    instances = filter_instances(instances, filter_spec=filter_spec, slice_spec=slice_spec, shuffle=shuffle)\n    if not redo_existing and (output_path / \"preds.json\").exists():\n        existing_instances = list(json.loads((output_path / \"preds.json\").read_text()).keys())\n        logger.info(f\"Skipping {len(existing_instances)} existing instances\")\n        instances = [instance for instance in instances if instance[\"instance_id\"] not in existing_instances]\n    logger.info(f\"Running on {len(instances)} instances...\")\n\n    config_path = get_config_path(config_spec)\n    logger.info(f\"Loading agent config from '{config_path}'\")\n    config = yaml.safe_load(config_path.read_text())\n    if environment_class is not None:\n        config.setdefault(\"environment\", {})[\"environment_class\"] = environment_class\n    if model is not None:\n        config.setdefault(\"model\", {})[\"model_name\"] = model\n    if model_class is not None:\n        config.setdefault(\"model\", {})[\"model_class\"] = model_class\n\n    progress_manager = RunBatchProgressManager(len(instances), output_path / f\"exit_statuses_{time.time()}.yaml\")\n\n    def process_futures(futures: dict[concurrent.futures.Future, str]):\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                future.result()\n            except concurrent.futures.CancelledError:\n                pass\n            except Exception as e:\n                instance_id = futures[future]\n                logger.error(f\"Error in future for instance {instance_id}: {e}\", exc_info=True)\n                progress_manager.on_uncaught_exception(instance_id, e)\n\n    with Live(progress_manager.render_group, refresh_per_second=4):\n        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n            futures = {\n                executor.submit(process_instance, instance, output_path, config, progress_manager): instance[\n                    \"instance_id\"\n                ]\n                for instance in instances\n            }\n            try:\n                process_futures(futures)\n            except KeyboardInterrupt:\n                logger.info(\"Cancelling all pending jobs. Press ^C again to exit immediately.\")\n                for future in futures:\n                    if not future.running() and not future.done():\n                        future.cancel()\n                process_futures(futures)\n</code></pre>"},{"location":"reference/run/swebench_single/","title":"SWE-bench (single)","text":""},{"location":"reference/run/swebench_single/#swe-bench-single","title":"SWE-bench Single","text":"<p>SWE-bench Single run script</p> <ul> <li>Read on GitHub</li> </ul> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"reference/run/swebench_single/#minisweagent.run.extra.swebench_single","title":"minisweagent.run.extra.swebench_single","text":"<p>Run on a single SWE-Bench instance.</p>"},{"location":"reference/run/swebench_single/#minisweagent.run.extra.swebench_single.app","title":"app  <code>module-attribute</code>","text":"<pre><code>app = Typer(add_completion=False)\n</code></pre>"},{"location":"reference/run/swebench_single/#minisweagent.run.extra.swebench_single.DEFAULT_OUTPUT","title":"DEFAULT_OUTPUT  <code>module-attribute</code>","text":"<pre><code>DEFAULT_OUTPUT = (\n    global_config_dir / \"last_swebench_single_run.traj.json\"\n)\n</code></pre>"},{"location":"reference/run/swebench_single/#minisweagent.run.extra.swebench_single.main","title":"main","text":"<pre><code>main(\n    subset: str = Option(\n        \"lite\",\n        \"--subset\",\n        help=\"SWEBench subset to use or path to a dataset\",\n        rich_help_panel=\"Data selection\",\n    ),\n    split: str = Option(\n        \"dev\",\n        \"--split\",\n        help=\"Dataset split\",\n        rich_help_panel=\"Data selection\",\n    ),\n    instance_spec: str = Option(\n        0,\n        \"-i\",\n        \"--instance\",\n        help=\"SWE-Bench instance ID or index\",\n        rich_help_panel=\"Data selection\",\n    ),\n    model_name: str | None = Option(\n        None,\n        \"-m\",\n        \"--model\",\n        help=\"Model to use\",\n        rich_help_panel=\"Basic\",\n    ),\n    model_class: str | None = Option(\n        None,\n        \"-c\",\n        \"--model-class\",\n        help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\",\n        rich_help_panel=\"Advanced\",\n    ),\n    config_path: Path = Option(\n        builtin_config_dir / \"extra\" / \"swebench.yaml\",\n        \"-c\",\n        \"--config\",\n        help=\"Path to a config file\",\n        rich_help_panel=\"Basic\",\n    ),\n    environment_class: str | None = Option(\n        None,\n        \"--environment-class\",\n        rich_help_panel=\"Advanced\",\n    ),\n    exit_immediately: bool = Option(\n        False,\n        \"--exit-immediately\",\n        help=\"Exit immediately when the agent wants to finish instead of prompting.\",\n        rich_help_panel=\"Basic\",\n    ),\n    output: Path = Option(\n        DEFAULT_OUTPUT,\n        \"-o\",\n        \"--output\",\n        help=\"Output trajectory file\",\n        rich_help_panel=\"Basic\",\n    ),\n) -&gt; None\n</code></pre> <p>Run on a single SWE-Bench instance.</p> Source code in <code>src/minisweagent/run/extra/swebench_single.py</code> <pre><code>@app.command()\ndef main(\n    subset: str = typer.Option(\"lite\", \"--subset\", help=\"SWEBench subset to use or path to a dataset\", rich_help_panel=\"Data selection\"),\n    split: str = typer.Option(\"dev\", \"--split\", help=\"Dataset split\", rich_help_panel=\"Data selection\"),\n    instance_spec: str = typer.Option(0, \"-i\", \"--instance\", help=\"SWE-Bench instance ID or index\", rich_help_panel=\"Data selection\"),\n    model_name: str | None = typer.Option(None, \"-m\", \"--model\", help=\"Model to use\", rich_help_panel=\"Basic\"),\n    model_class: str | None = typer.Option(None, \"-c\", \"--model-class\", help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\", rich_help_panel=\"Advanced\"),\n    config_path: Path = typer.Option( builtin_config_dir / \"extra\" / \"swebench.yaml\", \"-c\", \"--config\", help=\"Path to a config file\", rich_help_panel=\"Basic\"),\n    environment_class: str | None = typer.Option(None, \"--environment-class\", rich_help_panel=\"Advanced\"),\n    exit_immediately: bool = typer.Option( False, \"--exit-immediately\", help=\"Exit immediately when the agent wants to finish instead of prompting.\", rich_help_panel=\"Basic\"),\n    output: Path = typer.Option(DEFAULT_OUTPUT, \"-o\", \"--output\", help=\"Output trajectory file\", rich_help_panel=\"Basic\"),\n) -&gt; None:\n    # fmt: on\n    \"\"\"Run on a single SWE-Bench instance.\"\"\"\n    dataset_path = DATASET_MAPPING.get(subset, subset)\n    logger.info(f\"Loading dataset from {dataset_path}, split {split}...\")\n    instances = {\n        inst[\"instance_id\"]: inst  # type: ignore\n        for inst in load_dataset(dataset_path, split=split)\n    }\n    if instance_spec.isnumeric():\n        instance_spec = sorted(instances.keys())[int(instance_spec)]\n    instance: dict = instances[instance_spec]  # type: ignore\n\n    config_path = get_config_path(config_path)\n    logger.info(f\"Loading agent config from '{config_path}'\")\n    config = yaml.safe_load(config_path.read_text())\n    if environment_class is not None:\n        config.setdefault(\"environment\", {})[\"environment_class\"] = environment_class\n    if model_class is not None:\n        config.setdefault(\"model\", {})[\"model_class\"] = model_class\n    if exit_immediately:\n        config.setdefault(\"agent\", {})[\"confirm_exit\"] = False\n    env = get_sb_environment(config, instance)\n    agent = InteractiveAgent(\n        get_model(model_name, config.get(\"model\", {})),\n        env,\n        **({\"mode\": \"yolo\"} | config.get(\"agent\", {})),\n    )\n\n    exit_status, result, extra_info = None, None, None\n    try:\n        exit_status, result = agent.run(instance[\"problem_statement\"])  # type: ignore[arg-type]\n    except Exception as e:\n        logger.error(f\"Error processing instance {instance_spec}: {e}\", exc_info=True)\n        exit_status, result = type(e).__name__, str(e)\n        extra_info = {\"traceback\": traceback.format_exc()}\n    finally:\n        save_traj(agent, output, exit_status=exit_status, result=result, extra_info=extra_info)  # type: ignore[arg-type]\n</code></pre>"},{"location":"usage/inspector/","title":"Inspector","text":""},{"location":"usage/inspector/#inspector-browse-agent-trajectories","title":"Inspector: Browse agent trajectories","text":"<p>Overview</p> <ul> <li>The <code>inspector</code> is a tool that allows you to browse <code>.traj.json</code> files that show the history of a mini-SWE-agent run.</li> <li>Quickly start it with <code>mini-e i</code> or <code>mini-extra inspector</code>.</li> </ul>"},{"location":"usage/inspector/#usage","title":"Usage","text":"<pre><code># Find all .traj.json files recursively from current directory\nmini-extra inspector\n# or shorter\nmini-e i\n# Open the inspector for a specific file\nmini-e i &lt;path_to_traj.json&gt;\n# Search for trajectory files in a specific directory\nmini-e i &lt;path_to_directory&gt;\n</code></pre>"},{"location":"usage/inspector/#key-bindings","title":"Key bindings","text":"<ul> <li><code>q</code>: Quit the inspector</li> <li><code>h</code>/<code>LEFT</code>: Previous step</li> <li><code>l</code>/<code>RIGHT</code>: Next step</li> <li><code>j</code>/<code>DOWN</code>: Scroll down</li> <li><code>k</code>/<code>UP</code>: Scroll up</li> <li><code>H</code>: Previous trajectory</li> <li><code>L</code>: Next trajectory</li> </ul>"},{"location":"usage/inspector/#faq","title":"FAQ","text":"<p>How can I select/copy text on the screen?</p> <p>Hold down the <code>Alt</code>/<code>Option</code> key and use the mouse to select the text.</p>"},{"location":"usage/inspector/#implementation","title":"Implementation","text":"<p>The inspector is implemented with textual.</p> Implementation <ul> <li>Read on GitHub</li> </ul> <pre><code>#!/usr/bin/env python3\n\"\"\"\nSimple trajectory inspector for browsing agent conversation trajectories.\n\n[not dim]\nMore information about the usage: [bold green]https://mini-swe-agent.com/latest/usage/inspector/[/bold green]\n[/not dim]\n\"\"\"\n\nimport json\nimport os\nfrom pathlib import Path\n\nimport typer\nfrom rich.text import Text\nfrom textual.app import App, ComposeResult\nfrom textual.binding import Binding\nfrom textual.containers import Container, Vertical, VerticalScroll\nfrom textual.widgets import Footer, Header, Static\n\nfrom minisweagent.agents.interactive_textual import _messages_to_steps\n\napp = typer.Typer(rich_markup_mode=\"rich\", add_completion=False)\n\n\nclass TrajectoryInspector(App):\n    BINDINGS = [\n        Binding(\"right,l\", \"next_step\", \"Step++\"),\n        Binding(\"left,h\", \"previous_step\", \"Step--\"),\n        Binding(\"0\", \"first_step\", \"Step=0\"),\n        Binding(\"$\", \"last_step\", \"Step=-1\"),\n        Binding(\"j,down\", \"scroll_down\", \"Scroll down\"),\n        Binding(\"k,up\", \"scroll_up\", \"Scroll up\"),\n        Binding(\"L\", \"next_trajectory\", \"Next trajectory\"),\n        Binding(\"H\", \"previous_trajectory\", \"Previous trajectory\"),\n        Binding(\"q\", \"quit\", \"Quit\"),\n    ]\n\n    def __init__(self, trajectory_files: list[Path]):\n        css_path = os.environ.get(\n            \"MSWEA_INSPECTOR_STYLE_PATH\", str(Path(__file__).parent.parent / \"config\" / \"mini.tcss\")\n        )\n        self.__class__.CSS = Path(css_path).read_text()\n\n        super().__init__()\n        self.trajectory_files = trajectory_files\n        self._i_trajectory = 0\n        self._i_step = 0\n        self.messages = []\n        self.steps = []\n\n        if trajectory_files:\n            self._load_current_trajectory()\n\n    # --- Basics ---\n\n    @property\n    def i_step(self) -&gt; int:\n        \"\"\"Current step index.\"\"\"\n        return self._i_step\n\n    @i_step.setter\n    def i_step(self, value: int) -&gt; None:\n        \"\"\"Set current step index, automatically clamping to valid bounds.\"\"\"\n        if value != self._i_step and self.n_steps &gt; 0:\n            self._i_step = max(0, min(value, self.n_steps - 1))\n            self.query_one(VerticalScroll).scroll_to(y=0, animate=False)\n            self.update_content()\n\n    @property\n    def n_steps(self) -&gt; int:\n        \"\"\"Number of steps in current trajectory.\"\"\"\n        return len(self.steps)\n\n    @property\n    def i_trajectory(self) -&gt; int:\n        \"\"\"Current trajectory index.\"\"\"\n        return self._i_trajectory\n\n    @i_trajectory.setter\n    def i_trajectory(self, value: int) -&gt; None:\n        \"\"\"Set current trajectory index, automatically clamping to valid bounds.\"\"\"\n        if value != self._i_trajectory and self.n_trajectories &gt; 0:\n            self._i_trajectory = max(0, min(value, self.n_trajectories - 1))\n            self._load_current_trajectory()\n            self.query_one(VerticalScroll).scroll_to(y=0, animate=False)\n            self.update_content()\n\n    @property\n    def n_trajectories(self) -&gt; int:\n        \"\"\"Number of trajectory files.\"\"\"\n        return len(self.trajectory_files)\n\n    def _load_current_trajectory(self) -&gt; None:\n        \"\"\"Load the currently selected trajectory file.\"\"\"\n        if not self.trajectory_files:\n            self.messages = []\n            self.steps = []\n            return\n\n        trajectory_file = self.trajectory_files[self.i_trajectory]\n        try:\n            data = json.loads(trajectory_file.read_text())\n\n            if isinstance(data, list):\n                self.messages = data\n            elif isinstance(data, dict) and \"messages\" in data:\n                self.messages = data[\"messages\"]\n            else:\n                raise ValueError(\"Unrecognized trajectory format\")\n\n            self.steps = _messages_to_steps(self.messages)\n            self._i_step = 0\n        except (json.JSONDecodeError, FileNotFoundError, ValueError) as e:\n            self.messages = []\n            self.steps = []\n            self.notify(f\"Error loading {trajectory_file.name}: {e}\", severity=\"error\")\n\n    @property\n    def current_trajectory_name(self) -&gt; str:\n        \"\"\"Get the name of the current trajectory file.\"\"\"\n        if not self.trajectory_files:\n            return \"No trajectories\"\n        return self.trajectory_files[self.i_trajectory].name\n\n    def compose(self) -&gt; ComposeResult:\n        yield Header()\n        with Container(id=\"main\"):\n            with VerticalScroll():\n                yield Vertical(id=\"content\")\n        yield Footer()\n\n    def on_mount(self) -&gt; None:\n        self.update_content()\n\n    def update_content(self) -&gt; None:\n        \"\"\"Update the displayed content.\"\"\"\n        container = self.query_one(\"#content\", Vertical)\n        container.remove_children()\n\n        if not self.steps:\n            container.mount(Static(\"No trajectory loaded or empty trajectory\"))\n            self.title = \"Trajectory Inspector - No Data\"\n            return\n\n        for message in self.steps[self.i_step]:\n            if isinstance(message[\"content\"], list):\n                content_str = \"\\n\".join([item[\"text\"] for item in message[\"content\"]])\n            else:\n                content_str = str(message[\"content\"])\n            message_container = Vertical(classes=\"message-container\")\n            container.mount(message_container)\n            role = message[\"role\"].replace(\"assistant\", \"mini-swe-agent\")\n            message_container.mount(Static(role.upper(), classes=\"message-header\"))\n            message_container.mount(Static(Text(content_str, no_wrap=False), classes=\"message-content\"))\n\n        self.title = (\n            f\"Trajectory {self.i_trajectory + 1}/{self.n_trajectories} - \"\n            f\"{self.current_trajectory_name} - \"\n            f\"Step {self.i_step + 1}/{self.n_steps}\"\n        )\n\n    # --- Navigation actions ---\n\n    def action_next_step(self) -&gt; None:\n        self.i_step += 1\n\n    def action_previous_step(self) -&gt; None:\n        self.i_step -= 1\n\n    def action_first_step(self) -&gt; None:\n        self.i_step = 0\n\n    def action_last_step(self) -&gt; None:\n        self.i_step = self.n_steps - 1\n\n    def action_next_trajectory(self) -&gt; None:\n        self.i_trajectory += 1\n\n    def action_previous_trajectory(self) -&gt; None:\n        self.i_trajectory -= 1\n\n    def action_scroll_down(self) -&gt; None:\n        vs = self.query_one(VerticalScroll)\n        vs.scroll_to(y=vs.scroll_target_y + 15)\n\n    def action_scroll_up(self) -&gt; None:\n        vs = self.query_one(VerticalScroll)\n        vs.scroll_to(y=vs.scroll_target_y - 15)\n\n\n@app.command(help=__doc__)\ndef main(\n    path: str = typer.Argument(\".\", help=\"Directory to search for trajectory files or specific trajectory file\"),\n) -&gt; None:\n    path_obj = Path(path)\n\n    if path_obj.is_file():\n        trajectory_files = [path_obj]\n    elif path_obj.is_dir():\n        trajectory_files = sorted(path_obj.rglob(\"*.traj.json\"))\n        if not trajectory_files:\n            raise typer.BadParameter(f\"No trajectory files found in '{path}'\")\n    else:\n        raise typer.BadParameter(f\"Error: Path '{path}' does not exist\")\n\n    inspector = TrajectoryInspector(trajectory_files)\n    inspector.run()\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"usage/mini/","title":"<code>mini</code>","text":""},{"location":"usage/mini/#mini","title":"<code>mini</code>","text":"<p>Overview</p> <ul> <li><code>mini</code> is a REPL-style interactive command line interface for using mini-SWE-agent in the local requirement (as opposed for workflows that require sandboxing or large scale batch processing).</li> <li>Compared to <code>mini -v</code>, <code>mini</code> is more lightweight and does not require threading.</li> </ul> <p>Feedback wanted!</p> <p>Give feedback on the <code>mini</code> and <code>mini -v</code> interfaces at this github issue or in our Slack channel.</p>"},{"location":"usage/mini/#command-line-options","title":"Command line options","text":"<p>Useful switches:</p> <ul> <li><code>-h</code>/<code>--help</code>: Show help</li> <li><code>-t</code>/<code>--task</code>: Specify a task to run (else you will be prompted)</li> <li><code>-c</code>/<code>--config</code>: Specify a config file to use, else we will use <code>mini.yaml</code> or the config <code>MSWEA_MINI_CONFIG_PATH</code> environment variable (see global configuration).   It's enough to specify the name of the config file, e.g., <code>-c mini.yaml</code> (see global configuration for how it is resolved).</li> <li><code>-m</code>/<code>--model</code>: Specify a model to use, else we will use the model <code>MSWEA_MODEL_NAME</code> environment variable (see global configuration)</li> <li><code>-y</code>/<code>--yolo</code>: Start in <code>yolo</code> mode (see below)</li> </ul>"},{"location":"usage/mini/#modes-of-operation","title":"Modes of operation","text":"<p><code>mini</code> provides three different modes of operation</p> <ul> <li><code>confirm</code> (<code>/c</code>): The LM proposes an action and the user is prompted to confirm (press Enter) or reject (enter a rejection message)</li> <li><code>yolo</code> (<code>/y</code>): The action from the LM is executed immediately without confirmation</li> <li><code>human</code> (<code>/u</code>): The user takes over to type and execute commands</li> </ul> <p>You can switch between the modes with the <code>/c</code>, <code>/y</code>, and <code>/u</code> commands that you can enter any time the agent is waiting for input. You can also press <code>Ctrl+C</code> to interrupt the agent at any time, allowing you to switch between modes.</p> <p><code>mini</code> starts in <code>confirm</code> mode by default. To start in <code>yolo</code> mode, you can add <code>-y</code>/<code>--yolo</code> to the command line.</p>"},{"location":"usage/mini/#miscellaneous-tips","title":"Miscellaneous tips","text":"<ul> <li><code>mini</code> saves the full history of your last run to your global config directory.   The path to the directory is printed when you start <code>mini</code>.</li> </ul>"},{"location":"usage/mini/#implementation","title":"Implementation","text":"Default config <ul> <li>Read on GitHub</li> </ul> <pre><code>agent:\n  system_template: |\n    You are a helpful assistant that can interact with a computer.\n\n    Your response must contain exactly ONE bash code block with ONE command (or commands connected with &amp;&amp; or ||).\n    Include a THOUGHT section before your command where you explain your reasoning process.\n    Format your response as shown in &lt;format_example&gt;.\n\n    &lt;format_example&gt;\n    Your reasoning and analysis here. Explain why you want to perform the action.\n\n    ```bash\n    your_command_here\n    ```\n    &lt;/format_example&gt;\n\n    Failure to follow these rules will cause your response to be rejected.\n  instance_template: |\n    Please solve this issue: {{task}}\n\n    You can execute bash commands and edit files to implement the necessary changes.\n\n    ## Recommended Workflow\n\n    This workflows should be done step-by-step so that you can iterate on your changes and any possible problems.\n\n    1. Analyze the codebase by finding and reading relevant files\n    2. Create a script to reproduce the issue\n    3. Edit the source code to resolve the issue\n    4. Verify your fix works by running your script again\n    5. Test edge cases to ensure your fix is robust\n    6. Submit your changes and finish your work by issuing the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`.\n       Do not combine it with any other command. &lt;important&gt;After this command, you cannot continue working on this task.&lt;/important&gt;\n\n    ## Important Rules\n\n    1. Every response must contain exactly one action\n    2. The action must be enclosed in triple backticks\n    3. Directory or environment variable changes are not persistent. Every action is executed in a new subshell.\n       However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir &amp;&amp; ...` or write/load environment variables from files\n\n    &lt;system_information&gt;\n    {{system}} {{release}} {{version}} {{machine}}\n    &lt;/system_information&gt;\n\n    ## Formatting your response\n\n    Here is an example of a correct response:\n\n    &lt;example_response&gt;\n    THOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.\n\n    ```bash\n    ls -la\n    ```\n    &lt;/example_response&gt;\n\n    ## Useful command examples\n\n    ### Create a new file:\n\n    ```bash\n    cat &lt;&lt;'EOF' &gt; newfile.py\n    import numpy as np\n    hello = \"world\"\n    print(hello)\n    EOF\n    ```\n\n    ### Edit files with sed:\n\n    {%- if system == \"Darwin\" -%}\n    &lt;important&gt;\n    You are on MacOS. For all the below examples, you need to use `sed -i ''` instead of `sed -i`.\n    &lt;/important&gt;\n    {%- endif -%}\n\n    ```bash\n    # Replace all occurrences\n    sed -i 's/old_string/new_string/g' filename.py\n\n    # Replace only first occurrence\n    sed -i 's/old_string/new_string/' filename.py\n\n    # Replace first occurrence on line 1\n    sed -i '1s/old_string/new_string/' filename.py\n\n    # Replace all occurrences in lines 1-10\n    sed -i '1,10s/old_string/new_string/g' filename.py\n    ```\n\n    ### View file content:\n\n    ```bash\n    # View specific lines with numbers\n    nl -ba filename.py | sed -n '10,20p'\n    ```\n\n    ### Any other command you want to run\n\n    ```bash\n    anything\n    ```\n  action_observation_template: |\n    &lt;returncode&gt;{{output.returncode}}&lt;/returncode&gt;\n    {% if output.output | length &lt; 10000 -%}\n    &lt;output&gt;\n    {{ output.output -}}\n    &lt;/output&gt;\n    {%- else -%}\n    &lt;warning&gt;\n    The output of your last command was too long.\n    Please try a different command that produces less output.\n    If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.\n    If you're using grep or find and it produced too much output, you can use a more selective search pattern.\n    If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.\n    &lt;/warning&gt;\n    {%- set elided_chars = output.output | length - 10000 -%}\n    &lt;output_head&gt;\n    {{ output.output[:5000] }}\n    &lt;/output_head&gt;\n    &lt;elided_chars&gt;\n    {{ elided_chars }} characters elided\n    &lt;/elided_chars&gt;\n    &lt;output_tail&gt;\n    {{ output.output[-5000:] }}\n    &lt;/output_tail&gt;\n    {%- endif -%}\n  format_error_template: |\n    Please always provide EXACTLY ONE action in triple backticks, found {{actions|length}} actions.\n    If you want to end the task, please issue the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`\n    without any other command.\n    Else, please format your response exactly as follows:\n\n    &lt;response_example&gt;\n    Here are some thoughts about why you want to perform the action.\n\n    ```bash\n    &lt;action&gt;\n    ```\n    &lt;/response_example&gt;\n\n    Note: In rare cases, if you need to reference a similar format in your command, you might have\n    to proceed in two steps, first writing TRIPLEBACKTICKSBASH, then replacing them with ```bash.\n  step_limit: 0.\n  cost_limit: 3.\n  mode: confirm\nenvironment:\n  env:\n    PAGER: cat\n    MANPAGER: cat\n    LESS: -R\n    PIP_PROGRESS_BAR: 'off'\n    TQDM_DISABLE: '1'\nmodel:\n  model_kwargs:\n    temperature: 0.0\n    drop_params: true\n</code></pre> Run script <ul> <li>Read on GitHub</li> <li>API reference</li> </ul> <pre><code>#!/usr/bin/env python3\n\n\"\"\"Run mini-SWE-agent in your local environment. This is the default executable `mini`.\"\"\"\n# Read this first: https://mini-swe-agent.com/latest/usage/mini/  (usage)\n\nimport os\nimport traceback\nfrom pathlib import Path\nfrom typing import Any\n\nimport typer\nimport yaml\nfrom prompt_toolkit.formatted_text import HTML\nfrom prompt_toolkit.history import FileHistory\nfrom prompt_toolkit.shortcuts import PromptSession\nfrom rich.console import Console\n\nfrom minisweagent import global_config_dir\nfrom minisweagent.agents.interactive import InteractiveAgent\nfrom minisweagent.agents.interactive_textual import TextualAgent\nfrom minisweagent.config import builtin_config_dir, get_config_path\nfrom minisweagent.environments.local import LocalEnvironment\nfrom minisweagent.models import get_model\nfrom minisweagent.run.extra.config import configure_if_first_time\nfrom minisweagent.run.utils.save import save_traj\nfrom minisweagent.utils.log import logger\n\nDEFAULT_CONFIG = Path(os.getenv(\"MSWEA_MINI_CONFIG_PATH\", builtin_config_dir / \"mini.yaml\"))\nDEFAULT_OUTPUT = global_config_dir / \"last_mini_run.traj.json\"\nconsole = Console(highlight=False)\napp = typer.Typer(rich_markup_mode=\"rich\")\nprompt_session = PromptSession(history=FileHistory(global_config_dir / \"mini_task_history.txt\"))\n_HELP_TEXT = \"\"\"Run mini-SWE-agent in your local environment.\n\n[not dim]\nThere are two different user interfaces:\n\n[bold green]mini[/bold green] Simple REPL-style interface\n[bold green]mini -v[/bold green] Pager-style interface (Textual)\n\nMore information about the usage: [bold green]https://mini-swe-agent.com/latest/usage/mini/[/bold green]\n[/not dim]\n\"\"\"\n\n\n# fmt: off\n@app.command(help=_HELP_TEXT)\ndef main(\n    visual: bool = typer.Option(False, \"-v\", \"--visual\", help=\"Toggle (pager-style) UI (Textual) depending on the MSWEA_VISUAL_MODE_DEFAULT environment setting\",),\n    model_name: str | None = typer.Option( None, \"-m\", \"--model\", help=\"Model to use\",),\n    model_class: str | None = typer.Option(None, \"--model-class\", help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\", rich_help_panel=\"Advanced\"),\n    task: str | None = typer.Option(None, \"-t\", \"--task\", help=\"Task/problem statement\", show_default=False),\n    yolo: bool = typer.Option(False, \"-y\", \"--yolo\", help=\"Run without confirmation\"),\n    cost_limit: float | None = typer.Option(None, \"-l\", \"--cost-limit\", help=\"Cost limit. Set to 0 to disable.\"),\n    config_spec: Path = typer.Option(DEFAULT_CONFIG, \"-c\", \"--config\", help=\"Path to config file\"),\n    output: Path | None = typer.Option(DEFAULT_OUTPUT, \"-o\", \"--output\", help=\"Output trajectory file\"),\n    exit_immediately: bool = typer.Option( False, \"--exit-immediately\", help=\"Exit immediately when the agent wants to finish instead of prompting.\", rich_help_panel=\"Advanced\"),\n) -&gt; Any:\n    # fmt: on\n    configure_if_first_time()\n    config_path = get_config_path(config_spec)\n    console.print(f\"Loading agent config from [bold green]'{config_path}'[/bold green]\")\n    config = yaml.safe_load(config_path.read_text())\n\n    if not task:\n        console.print(\"[bold yellow]What do you want to do?\")\n        task = prompt_session.prompt(\n            \"\",\n            multiline=True,\n            bottom_toolbar=HTML(\n                \"Submit task: &lt;b fg='yellow' bg='black'&gt;Esc+Enter&lt;/b&gt; | \"\n                \"Navigate history: &lt;b fg='yellow' bg='black'&gt;Arrow Up/Down&lt;/b&gt; | \"\n                \"Search history: &lt;b fg='yellow' bg='black'&gt;Ctrl+R&lt;/b&gt;\"\n            ),\n        )\n        console.print(\"[bold green]Got that, thanks![/bold green]\")\n\n    if yolo:\n        config.setdefault(\"agent\", {})[\"mode\"] = \"yolo\"\n    if cost_limit:\n        config.setdefault(\"agent\", {})[\"cost_limit\"] = cost_limit\n    if exit_immediately:\n        config.setdefault(\"agent\", {})[\"confirm_exit\"] = False\n    if model_class is not None:\n        config.setdefault(\"model\", {})[\"model_class\"] = model_class\n    model = get_model(model_name, config.get(\"model\", {}))\n    env = LocalEnvironment(**config.get(\"env\", {}))\n\n    # Both visual flag and the MSWEA_VISUAL_MODE_DEFAULT flip the mode, so it's essentially a XOR\n    agent_class = InteractiveAgent\n    if visual == (os.getenv(\"MSWEA_VISUAL_MODE_DEFAULT\", \"false\") == \"false\"):\n        agent_class = TextualAgent\n\n    agent = agent_class(model, env, **config.get(\"agent\", {}))\n    exit_status, result, extra_info = None, None, None\n    try:\n        exit_status, result = agent.run(task)  # type: ignore[arg-type]\n    except Exception as e:\n        logger.error(f\"Error running agent: {e}\", exc_info=True)\n        exit_status, result = type(e).__name__, str(e)\n        extra_info = {\"traceback\": traceback.format_exc()}\n    finally:\n        if output:\n            save_traj(agent, output, exit_status=exit_status, result=result, extra_info=extra_info)  # type: ignore[arg-type]\n    return agent\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> Agent class <ul> <li>Read on GitHub</li> <li>API reference</li> </ul> <pre><code>\"\"\"A small generalization of the default agent that puts the user in the loop.\n\nThere are three modes:\n- human: commands issued by the user are executed immediately\n- confirm: commands issued by the LM but not whitelisted are confirmed by the user\n- yolo: commands issued by the LM are executed immediately without confirmation\n\"\"\"\n\nimport re\nfrom dataclasses import dataclass, field\nfrom typing import Literal\n\nfrom prompt_toolkit.history import FileHistory\nfrom prompt_toolkit.shortcuts import PromptSession\nfrom rich.console import Console\nfrom rich.rule import Rule\n\nfrom minisweagent import global_config_dir\nfrom minisweagent.agents.default import AgentConfig, DefaultAgent, LimitsExceeded, NonTerminatingException, Submitted\n\nconsole = Console(highlight=False)\nprompt_session = PromptSession(history=FileHistory(global_config_dir / \"interactive_history.txt\"))\n\n\n@dataclass\nclass InteractiveAgentConfig(AgentConfig):\n    mode: Literal[\"human\", \"confirm\", \"yolo\"] = \"confirm\"\n    \"\"\"Whether to confirm actions.\"\"\"\n    whitelist_actions: list[str] = field(default_factory=list)\n    \"\"\"Never confirm actions that match these regular expressions.\"\"\"\n    confirm_exit: bool = True\n    \"\"\"If the agent wants to finish, do we ask for confirmation from user?\"\"\"\n\n\nclass InteractiveAgent(DefaultAgent):\n    _MODE_COMMANDS_MAPPING = {\"/u\": \"human\", \"/c\": \"confirm\", \"/y\": \"yolo\"}\n\n    def __init__(self, *args, config_class=InteractiveAgentConfig, **kwargs):\n        super().__init__(*args, config_class=config_class, **kwargs)\n        self.cost_last_confirmed = 0.0\n\n    def add_message(self, role: str, content: str, **kwargs):\n        # Extend supermethod to print messages\n        super().add_message(role, content, **kwargs)\n        if role == \"assistant\":\n            console.print(\n                f\"\\n[red][bold]mini-swe-agent[/bold] (step [bold]{self.model.n_calls}[/bold], [bold]${self.model.cost:.2f}[/bold]):[/red]\\n\",\n                end=\"\",\n                highlight=False,\n            )\n        else:\n            console.print(f\"\\n[bold green]{role.capitalize()}[/bold green]:\\n\", end=\"\", highlight=False)\n        console.print(content, highlight=False, markup=False)\n\n    def query(self) -&gt; dict:\n        # Extend supermethod to handle human mode\n        if self.config.mode == \"human\":\n            match command := self._prompt_and_handle_special(\"[bold yellow]&gt;[/bold yellow] \"):\n                case \"/y\" | \"/c\":  # Just go to the super query, which queries the LM for the next action\n                    pass\n                case _:\n                    msg = {\"content\": f\"\\n```bash\\n{command}\\n```\"}\n                    self.add_message(\"assistant\", msg[\"content\"])\n                    return msg\n        try:\n            with console.status(\"Waiting for the LM to respond...\"):\n                return super().query()\n        except LimitsExceeded:\n            console.print(\n                f\"Limits exceeded. Limits: {self.config.step_limit} steps, ${self.config.cost_limit}.\\n\"\n                f\"Current spend: {self.model.n_calls} steps, ${self.model.cost:.2f}.\"\n            )\n            self.config.step_limit = int(input(\"New step limit: \"))\n            self.config.cost_limit = float(input(\"New cost limit: \"))\n            return super().query()\n\n    def step(self) -&gt; dict:\n        # Override the step method to handle user interruption\n        try:\n            console.print(Rule())\n            return super().step()\n        except KeyboardInterrupt:\n            # We always add a message about the interrupt and then just proceed to the next step\n            interruption_message = self._prompt_and_handle_special(\n                \"\\n\\n[bold yellow]Interrupted.[/bold yellow] \"\n                \"[green]Type a comment/command[/green] (/h for available commands)\"\n                \"\\n[bold yellow]&gt;[/bold yellow] \"\n            ).strip()\n            if not interruption_message or interruption_message in self._MODE_COMMANDS_MAPPING:\n                interruption_message = \"Temporary interruption caught.\"\n            raise NonTerminatingException(f\"Interrupted by user: {interruption_message}\")\n\n    def execute_action(self, action: dict) -&gt; dict:\n        # Override the execute_action method to handle user confirmation\n        if self.should_ask_confirmation(action[\"action\"]):\n            self.ask_confirmation()\n        return super().execute_action(action)\n\n    def should_ask_confirmation(self, action: str) -&gt; bool:\n        return self.config.mode == \"confirm\" and not any(re.match(r, action) for r in self.config.whitelist_actions)\n\n    def ask_confirmation(self) -&gt; None:\n        prompt = (\n            \"[bold yellow]Execute?[/bold yellow] [green][bold]Enter[/bold] to confirm[/green], \"\n            \"or [green]Type a comment/command[/green] (/h for available commands)\\n\"\n            \"[bold yellow]&gt;[/bold yellow] \"\n        )\n        match user_input := self._prompt_and_handle_special(prompt).strip():\n            case \"\" | \"/y\":\n                pass  # confirmed, do nothing\n            case \"/u\":  # Skip execution action and get back to query\n                raise NonTerminatingException(\"Command not executed. Switching to human mode\")\n            case _:\n                raise NonTerminatingException(\n                    f\"Command not executed. The user rejected your command with the following message: {user_input}\"\n                )\n\n    def _prompt_and_handle_special(self, prompt: str) -&gt; str:\n        \"\"\"Prompts the user, takes care of /h (followed by requery) and sets the mode. Returns the user input.\"\"\"\n        console.print(prompt, end=\"\")\n        user_input = prompt_session.prompt(\"\")\n        if user_input == \"/h\":\n            console.print(\n                f\"Current mode: [bold green]{self.config.mode}[/bold green]\\n\"\n                f\"[bold green]/y[/bold green] to switch to [bold yellow]yolo[/bold yellow] mode (execute LM commands without confirmation)\\n\"\n                f\"[bold green]/c[/bold green] to switch to [bold yellow]confirmation[/bold yellow] mode (ask for confirmation before executing LM commands)\\n\"\n                f\"[bold green]/u[/bold green] to switch to [bold yellow]human[/bold yellow] mode (execute commands issued by the user)\\n\"\n            )\n            return self._prompt_and_handle_special(prompt)\n        if user_input in self._MODE_COMMANDS_MAPPING:\n            if self.config.mode == self._MODE_COMMANDS_MAPPING[user_input]:\n                return self._prompt_and_handle_special(\n                    f\"[bold red]Already in {self.config.mode} mode.[/bold red]\\n{prompt}\"\n                )\n            self.config.mode = self._MODE_COMMANDS_MAPPING[user_input]\n            console.print(f\"Switched to [bold green]{self.config.mode}[/bold green] mode.\")\n            return user_input\n        return user_input\n\n    def has_finished(self, output: dict[str, str]):\n        try:\n            return super().has_finished(output)\n        except Submitted as e:\n            if self.config.confirm_exit:\n                console.print(\n                    \"[bold green]Agent wants to finish.[/bold green] \"\n                    \"[green]Type a comment to give it a new task or press enter to quit.\\n\"\n                    \"[bold yellow]&gt;[/bold yellow] \",\n                    end=\"\",\n                )\n                if new_task := self._prompt_and_handle_special(\"\").strip():\n                    raise NonTerminatingException(f\"The user added a new task: {new_task}\")\n            raise e\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"usage/mini_v/","title":"<code>mini -v</code>","text":""},{"location":"usage/mini_v/#mini-v","title":"<code>mini -v</code>","text":"<p>Overview</p> <ul> <li><code>mini -v</code> is a pager-style interactive command line interface for using mini-SWE-agent in the local requirement (as opposed for workflows that require sandboxing or large scale batch processing).</li> <li>Compared to <code>mini</code>, <code>mini -v</code> offers a more advanced UI based on Textual.</li> </ul> <p>Feedback wanted!</p> <p>Give feedback on the <code>mini</code> and <code>mini -v</code> interfaces at this github issue or in our Slack channel.</p>"},{"location":"usage/mini_v/#command-line-options","title":"Command line options","text":"<p>Invocation</p> <pre><code>mini -v [other options]\n</code></pre> <p>Default visual mode</p> <p>If you want to use the visual mode by default, you can set the <code>MSWEA_VISUAL_MODE_DEFAULT</code> environment variable to <code>true</code> (<code>mini-extra config set MSWEA_VISUAL_MODE_DEFAULT true</code>).</p> <p>Useful switches:</p> <ul> <li><code>-h</code>/<code>--help</code>: Show help</li> <li><code>-t</code>/<code>--task</code>: Specify a task to run (else you will be prompted)</li> <li><code>-c</code>/<code>--config</code>: Specify a config file to use, else we will use <code>mini.yaml</code> or the config <code>MSWEA_MINI_CONFIG_PATH</code> environment variable (see global configuration)   It's enough to specify the name of the config file, e.g., <code>-c mini.yaml</code> (see global configuration for how it is resolved).</li> <li><code>-m</code>/<code>--model</code>: Specify a model to use, else we will use the model <code>MSWEA_MODEL_NAME</code> environment variable (see global configuration)</li> <li><code>-y</code>/<code>--yolo</code>: Start in <code>yolo</code> mode (see below)</li> </ul>"},{"location":"usage/mini_v/#key-bindings","title":"Key bindings","text":"<p>Focused input fields</p> <p>Whenever you are prompted to enter text, the input field will be focused. You can use <code>Tab</code> or <code>Esc</code> to switch between the input field controls and the general controls below.</p> <ul> <li><code>f1</code> or <code>?</code>: Show keybinding help</li> <li><code>q</code> (or <code>ctrl+q</code>): Quit the agent</li> <li><code>c</code>: Switch to <code>confirm</code> mode</li> <li><code>y</code> (or <code>ctrl+y</code>): Switch to <code>yolo</code> mode</li> <li><code>h</code> or <code>LEFT</code>: Go to previous step of the agent</li> <li><code>l</code> or <code>RIGHT</code>: Go to next step of the agent</li> <li><code>0</code>: Go to first step of the agent</li> <li><code>$</code>: Go to last step of the agent</li> <li><code>j</code> or <code>DOWN</code>: Scroll down</li> <li><code>k</code> or <code>UP</code>: Scroll up</li> </ul>"},{"location":"usage/mini_v/#modes-of-operation","title":"Modes of operation","text":"<p><code>mini -v</code> provides two different modes of operation</p> <ul> <li><code>confirm</code> (<code>c</code>): The LM proposes an action and the user is prompted to confirm (press Enter)) or reject (enter a rejection message))</li> <li><code>yolo</code> (<code>y</code>): The action from the LM is executed immediately without confirmation</li> <li><code>human</code> (<code>u</code>): The user is prompted to enter a command directly</li> </ul> <p>You can switch between the modes at any time by pressing the <code>c</code>, <code>y</code>, or <code>u</code> keys.</p> <p><code>mini -v</code> starts in <code>confirm</code> mode by default. To start in <code>yolo</code> mode, you can add <code>-y</code>/<code>--yolo</code> to the command line.</p>"},{"location":"usage/mini_v/#faq","title":"FAQ","text":"<p>How can I select/copy text on the screen?</p> <p>Hold down the <code>Alt</code>/<code>Option</code> key and use the mouse to select the text.</p>"},{"location":"usage/mini_v/#miscellaneous-tips","title":"Miscellaneous tips","text":"<ul> <li><code>mini</code> saves the full history of your last run to your global config directory.   The path to the directory is printed when you start <code>mini</code>.</li> </ul>"},{"location":"usage/mini_v/#implementation","title":"Implementation","text":"Default config <ul> <li>Read on GitHub</li> </ul> <pre><code>agent:\n  system_template: |\n    You are a helpful assistant that can interact with a computer.\n\n    Your response must contain exactly ONE bash code block with ONE command (or commands connected with &amp;&amp; or ||).\n    Include a THOUGHT section before your command where you explain your reasoning process.\n    Format your response as shown in &lt;format_example&gt;.\n\n    &lt;format_example&gt;\n    Your reasoning and analysis here. Explain why you want to perform the action.\n\n    ```bash\n    your_command_here\n    ```\n    &lt;/format_example&gt;\n\n    Failure to follow these rules will cause your response to be rejected.\n  instance_template: |\n    Please solve this issue: {{task}}\n\n    You can execute bash commands and edit files to implement the necessary changes.\n\n    ## Recommended Workflow\n\n    This workflows should be done step-by-step so that you can iterate on your changes and any possible problems.\n\n    1. Analyze the codebase by finding and reading relevant files\n    2. Create a script to reproduce the issue\n    3. Edit the source code to resolve the issue\n    4. Verify your fix works by running your script again\n    5. Test edge cases to ensure your fix is robust\n    6. Submit your changes and finish your work by issuing the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`.\n       Do not combine it with any other command. &lt;important&gt;After this command, you cannot continue working on this task.&lt;/important&gt;\n\n    ## Important Rules\n\n    1. Every response must contain exactly one action\n    2. The action must be enclosed in triple backticks\n    3. Directory or environment variable changes are not persistent. Every action is executed in a new subshell.\n       However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir &amp;&amp; ...` or write/load environment variables from files\n\n    &lt;system_information&gt;\n    {{system}} {{release}} {{version}} {{machine}}\n    &lt;/system_information&gt;\n\n    ## Formatting your response\n\n    Here is an example of a correct response:\n\n    &lt;example_response&gt;\n    THOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.\n\n    ```bash\n    ls -la\n    ```\n    &lt;/example_response&gt;\n\n    ## Useful command examples\n\n    ### Create a new file:\n\n    ```bash\n    cat &lt;&lt;'EOF' &gt; newfile.py\n    import numpy as np\n    hello = \"world\"\n    print(hello)\n    EOF\n    ```\n\n    ### Edit files with sed:\n\n    {%- if system == \"Darwin\" -%}\n    &lt;important&gt;\n    You are on MacOS. For all the below examples, you need to use `sed -i ''` instead of `sed -i`.\n    &lt;/important&gt;\n    {%- endif -%}\n\n    ```bash\n    # Replace all occurrences\n    sed -i 's/old_string/new_string/g' filename.py\n\n    # Replace only first occurrence\n    sed -i 's/old_string/new_string/' filename.py\n\n    # Replace first occurrence on line 1\n    sed -i '1s/old_string/new_string/' filename.py\n\n    # Replace all occurrences in lines 1-10\n    sed -i '1,10s/old_string/new_string/g' filename.py\n    ```\n\n    ### View file content:\n\n    ```bash\n    # View specific lines with numbers\n    nl -ba filename.py | sed -n '10,20p'\n    ```\n\n    ### Any other command you want to run\n\n    ```bash\n    anything\n    ```\n  action_observation_template: |\n    &lt;returncode&gt;{{output.returncode}}&lt;/returncode&gt;\n    {% if output.output | length &lt; 10000 -%}\n    &lt;output&gt;\n    {{ output.output -}}\n    &lt;/output&gt;\n    {%- else -%}\n    &lt;warning&gt;\n    The output of your last command was too long.\n    Please try a different command that produces less output.\n    If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.\n    If you're using grep or find and it produced too much output, you can use a more selective search pattern.\n    If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.\n    &lt;/warning&gt;\n    {%- set elided_chars = output.output | length - 10000 -%}\n    &lt;output_head&gt;\n    {{ output.output[:5000] }}\n    &lt;/output_head&gt;\n    &lt;elided_chars&gt;\n    {{ elided_chars }} characters elided\n    &lt;/elided_chars&gt;\n    &lt;output_tail&gt;\n    {{ output.output[-5000:] }}\n    &lt;/output_tail&gt;\n    {%- endif -%}\n  format_error_template: |\n    Please always provide EXACTLY ONE action in triple backticks, found {{actions|length}} actions.\n    If you want to end the task, please issue the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`\n    without any other command.\n    Else, please format your response exactly as follows:\n\n    &lt;response_example&gt;\n    Here are some thoughts about why you want to perform the action.\n\n    ```bash\n    &lt;action&gt;\n    ```\n    &lt;/response_example&gt;\n\n    Note: In rare cases, if you need to reference a similar format in your command, you might have\n    to proceed in two steps, first writing TRIPLEBACKTICKSBASH, then replacing them with ```bash.\n  step_limit: 0.\n  cost_limit: 3.\n  mode: confirm\nenvironment:\n  env:\n    PAGER: cat\n    MANPAGER: cat\n    LESS: -R\n    PIP_PROGRESS_BAR: 'off'\n    TQDM_DISABLE: '1'\nmodel:\n  model_kwargs:\n    temperature: 0.0\n    drop_params: true\n</code></pre> Run script <ul> <li>Read on GitHub</li> <li>API reference</li> </ul> <pre><code>#!/usr/bin/env python3\n\n\"\"\"Run mini-SWE-agent in your local environment. This is the default executable `mini`.\"\"\"\n# Read this first: https://mini-swe-agent.com/latest/usage/mini/  (usage)\n\nimport os\nimport traceback\nfrom pathlib import Path\nfrom typing import Any\n\nimport typer\nimport yaml\nfrom prompt_toolkit.formatted_text import HTML\nfrom prompt_toolkit.history import FileHistory\nfrom prompt_toolkit.shortcuts import PromptSession\nfrom rich.console import Console\n\nfrom minisweagent import global_config_dir\nfrom minisweagent.agents.interactive import InteractiveAgent\nfrom minisweagent.agents.interactive_textual import TextualAgent\nfrom minisweagent.config import builtin_config_dir, get_config_path\nfrom minisweagent.environments.local import LocalEnvironment\nfrom minisweagent.models import get_model\nfrom minisweagent.run.extra.config import configure_if_first_time\nfrom minisweagent.run.utils.save import save_traj\nfrom minisweagent.utils.log import logger\n\nDEFAULT_CONFIG = Path(os.getenv(\"MSWEA_MINI_CONFIG_PATH\", builtin_config_dir / \"mini.yaml\"))\nDEFAULT_OUTPUT = global_config_dir / \"last_mini_run.traj.json\"\nconsole = Console(highlight=False)\napp = typer.Typer(rich_markup_mode=\"rich\")\nprompt_session = PromptSession(history=FileHistory(global_config_dir / \"mini_task_history.txt\"))\n_HELP_TEXT = \"\"\"Run mini-SWE-agent in your local environment.\n\n[not dim]\nThere are two different user interfaces:\n\n[bold green]mini[/bold green] Simple REPL-style interface\n[bold green]mini -v[/bold green] Pager-style interface (Textual)\n\nMore information about the usage: [bold green]https://mini-swe-agent.com/latest/usage/mini/[/bold green]\n[/not dim]\n\"\"\"\n\n\n# fmt: off\n@app.command(help=_HELP_TEXT)\ndef main(\n    visual: bool = typer.Option(False, \"-v\", \"--visual\", help=\"Toggle (pager-style) UI (Textual) depending on the MSWEA_VISUAL_MODE_DEFAULT environment setting\",),\n    model_name: str | None = typer.Option( None, \"-m\", \"--model\", help=\"Model to use\",),\n    model_class: str | None = typer.Option(None, \"--model-class\", help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\", rich_help_panel=\"Advanced\"),\n    task: str | None = typer.Option(None, \"-t\", \"--task\", help=\"Task/problem statement\", show_default=False),\n    yolo: bool = typer.Option(False, \"-y\", \"--yolo\", help=\"Run without confirmation\"),\n    cost_limit: float | None = typer.Option(None, \"-l\", \"--cost-limit\", help=\"Cost limit. Set to 0 to disable.\"),\n    config_spec: Path = typer.Option(DEFAULT_CONFIG, \"-c\", \"--config\", help=\"Path to config file\"),\n    output: Path | None = typer.Option(DEFAULT_OUTPUT, \"-o\", \"--output\", help=\"Output trajectory file\"),\n    exit_immediately: bool = typer.Option( False, \"--exit-immediately\", help=\"Exit immediately when the agent wants to finish instead of prompting.\", rich_help_panel=\"Advanced\"),\n) -&gt; Any:\n    # fmt: on\n    configure_if_first_time()\n    config_path = get_config_path(config_spec)\n    console.print(f\"Loading agent config from [bold green]'{config_path}'[/bold green]\")\n    config = yaml.safe_load(config_path.read_text())\n\n    if not task:\n        console.print(\"[bold yellow]What do you want to do?\")\n        task = prompt_session.prompt(\n            \"\",\n            multiline=True,\n            bottom_toolbar=HTML(\n                \"Submit task: &lt;b fg='yellow' bg='black'&gt;Esc+Enter&lt;/b&gt; | \"\n                \"Navigate history: &lt;b fg='yellow' bg='black'&gt;Arrow Up/Down&lt;/b&gt; | \"\n                \"Search history: &lt;b fg='yellow' bg='black'&gt;Ctrl+R&lt;/b&gt;\"\n            ),\n        )\n        console.print(\"[bold green]Got that, thanks![/bold green]\")\n\n    if yolo:\n        config.setdefault(\"agent\", {})[\"mode\"] = \"yolo\"\n    if cost_limit:\n        config.setdefault(\"agent\", {})[\"cost_limit\"] = cost_limit\n    if exit_immediately:\n        config.setdefault(\"agent\", {})[\"confirm_exit\"] = False\n    if model_class is not None:\n        config.setdefault(\"model\", {})[\"model_class\"] = model_class\n    model = get_model(model_name, config.get(\"model\", {}))\n    env = LocalEnvironment(**config.get(\"env\", {}))\n\n    # Both visual flag and the MSWEA_VISUAL_MODE_DEFAULT flip the mode, so it's essentially a XOR\n    agent_class = InteractiveAgent\n    if visual == (os.getenv(\"MSWEA_VISUAL_MODE_DEFAULT\", \"false\") == \"false\"):\n        agent_class = TextualAgent\n\n    agent = agent_class(model, env, **config.get(\"agent\", {}))\n    exit_status, result, extra_info = None, None, None\n    try:\n        exit_status, result = agent.run(task)  # type: ignore[arg-type]\n    except Exception as e:\n        logger.error(f\"Error running agent: {e}\", exc_info=True)\n        exit_status, result = type(e).__name__, str(e)\n        extra_info = {\"traceback\": traceback.format_exc()}\n    finally:\n        if output:\n            save_traj(agent, output, exit_status=exit_status, result=result, extra_info=extra_info)  # type: ignore[arg-type]\n    return agent\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> Agent class <ul> <li>Read on GitHub</li> <li>API reference</li> </ul> <pre><code>\"\"\"A small generalization of the default agent that puts the user in the loop.\n\nThere are three modes:\n- human: commands issued by the user are executed immediately\n- confirm: commands issued by the LM but not whitelisted are confirmed by the user\n- yolo: commands issued by the LM are executed immediately without confirmation\n\"\"\"\n\nimport re\nfrom dataclasses import dataclass, field\nfrom typing import Literal\n\nfrom prompt_toolkit.history import FileHistory\nfrom prompt_toolkit.shortcuts import PromptSession\nfrom rich.console import Console\nfrom rich.rule import Rule\n\nfrom minisweagent import global_config_dir\nfrom minisweagent.agents.default import AgentConfig, DefaultAgent, LimitsExceeded, NonTerminatingException, Submitted\n\nconsole = Console(highlight=False)\nprompt_session = PromptSession(history=FileHistory(global_config_dir / \"interactive_history.txt\"))\n\n\n@dataclass\nclass InteractiveAgentConfig(AgentConfig):\n    mode: Literal[\"human\", \"confirm\", \"yolo\"] = \"confirm\"\n    \"\"\"Whether to confirm actions.\"\"\"\n    whitelist_actions: list[str] = field(default_factory=list)\n    \"\"\"Never confirm actions that match these regular expressions.\"\"\"\n    confirm_exit: bool = True\n    \"\"\"If the agent wants to finish, do we ask for confirmation from user?\"\"\"\n\n\nclass InteractiveAgent(DefaultAgent):\n    _MODE_COMMANDS_MAPPING = {\"/u\": \"human\", \"/c\": \"confirm\", \"/y\": \"yolo\"}\n\n    def __init__(self, *args, config_class=InteractiveAgentConfig, **kwargs):\n        super().__init__(*args, config_class=config_class, **kwargs)\n        self.cost_last_confirmed = 0.0\n\n    def add_message(self, role: str, content: str, **kwargs):\n        # Extend supermethod to print messages\n        super().add_message(role, content, **kwargs)\n        if role == \"assistant\":\n            console.print(\n                f\"\\n[red][bold]mini-swe-agent[/bold] (step [bold]{self.model.n_calls}[/bold], [bold]${self.model.cost:.2f}[/bold]):[/red]\\n\",\n                end=\"\",\n                highlight=False,\n            )\n        else:\n            console.print(f\"\\n[bold green]{role.capitalize()}[/bold green]:\\n\", end=\"\", highlight=False)\n        console.print(content, highlight=False, markup=False)\n\n    def query(self) -&gt; dict:\n        # Extend supermethod to handle human mode\n        if self.config.mode == \"human\":\n            match command := self._prompt_and_handle_special(\"[bold yellow]&gt;[/bold yellow] \"):\n                case \"/y\" | \"/c\":  # Just go to the super query, which queries the LM for the next action\n                    pass\n                case _:\n                    msg = {\"content\": f\"\\n```bash\\n{command}\\n```\"}\n                    self.add_message(\"assistant\", msg[\"content\"])\n                    return msg\n        try:\n            with console.status(\"Waiting for the LM to respond...\"):\n                return super().query()\n        except LimitsExceeded:\n            console.print(\n                f\"Limits exceeded. Limits: {self.config.step_limit} steps, ${self.config.cost_limit}.\\n\"\n                f\"Current spend: {self.model.n_calls} steps, ${self.model.cost:.2f}.\"\n            )\n            self.config.step_limit = int(input(\"New step limit: \"))\n            self.config.cost_limit = float(input(\"New cost limit: \"))\n            return super().query()\n\n    def step(self) -&gt; dict:\n        # Override the step method to handle user interruption\n        try:\n            console.print(Rule())\n            return super().step()\n        except KeyboardInterrupt:\n            # We always add a message about the interrupt and then just proceed to the next step\n            interruption_message = self._prompt_and_handle_special(\n                \"\\n\\n[bold yellow]Interrupted.[/bold yellow] \"\n                \"[green]Type a comment/command[/green] (/h for available commands)\"\n                \"\\n[bold yellow]&gt;[/bold yellow] \"\n            ).strip()\n            if not interruption_message or interruption_message in self._MODE_COMMANDS_MAPPING:\n                interruption_message = \"Temporary interruption caught.\"\n            raise NonTerminatingException(f\"Interrupted by user: {interruption_message}\")\n\n    def execute_action(self, action: dict) -&gt; dict:\n        # Override the execute_action method to handle user confirmation\n        if self.should_ask_confirmation(action[\"action\"]):\n            self.ask_confirmation()\n        return super().execute_action(action)\n\n    def should_ask_confirmation(self, action: str) -&gt; bool:\n        return self.config.mode == \"confirm\" and not any(re.match(r, action) for r in self.config.whitelist_actions)\n\n    def ask_confirmation(self) -&gt; None:\n        prompt = (\n            \"[bold yellow]Execute?[/bold yellow] [green][bold]Enter[/bold] to confirm[/green], \"\n            \"or [green]Type a comment/command[/green] (/h for available commands)\\n\"\n            \"[bold yellow]&gt;[/bold yellow] \"\n        )\n        match user_input := self._prompt_and_handle_special(prompt).strip():\n            case \"\" | \"/y\":\n                pass  # confirmed, do nothing\n            case \"/u\":  # Skip execution action and get back to query\n                raise NonTerminatingException(\"Command not executed. Switching to human mode\")\n            case _:\n                raise NonTerminatingException(\n                    f\"Command not executed. The user rejected your command with the following message: {user_input}\"\n                )\n\n    def _prompt_and_handle_special(self, prompt: str) -&gt; str:\n        \"\"\"Prompts the user, takes care of /h (followed by requery) and sets the mode. Returns the user input.\"\"\"\n        console.print(prompt, end=\"\")\n        user_input = prompt_session.prompt(\"\")\n        if user_input == \"/h\":\n            console.print(\n                f\"Current mode: [bold green]{self.config.mode}[/bold green]\\n\"\n                f\"[bold green]/y[/bold green] to switch to [bold yellow]yolo[/bold yellow] mode (execute LM commands without confirmation)\\n\"\n                f\"[bold green]/c[/bold green] to switch to [bold yellow]confirmation[/bold yellow] mode (ask for confirmation before executing LM commands)\\n\"\n                f\"[bold green]/u[/bold green] to switch to [bold yellow]human[/bold yellow] mode (execute commands issued by the user)\\n\"\n            )\n            return self._prompt_and_handle_special(prompt)\n        if user_input in self._MODE_COMMANDS_MAPPING:\n            if self.config.mode == self._MODE_COMMANDS_MAPPING[user_input]:\n                return self._prompt_and_handle_special(\n                    f\"[bold red]Already in {self.config.mode} mode.[/bold red]\\n{prompt}\"\n                )\n            self.config.mode = self._MODE_COMMANDS_MAPPING[user_input]\n            console.print(f\"Switched to [bold green]{self.config.mode}[/bold green] mode.\")\n            return user_input\n        return user_input\n\n    def has_finished(self, output: dict[str, str]):\n        try:\n            return super().has_finished(output)\n        except Submitted as e:\n            if self.config.confirm_exit:\n                console.print(\n                    \"[bold green]Agent wants to finish.[/bold green] \"\n                    \"[green]Type a comment to give it a new task or press enter to quit.\\n\"\n                    \"[bold yellow]&gt;[/bold yellow] \",\n                    end=\"\",\n                )\n                if new_task := self._prompt_and_handle_special(\"\").strip():\n                    raise NonTerminatingException(f\"The user added a new task: {new_task}\")\n            raise e\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"},{"location":"usage/swebench/","title":"SWE-bench","text":""},{"location":"usage/swebench/#swe-bench","title":"SWE-bench","text":"<p>Overview</p> <ul> <li>We provide two scripts to run on the SWE-bench benchmark.</li> <li><code>mini-extra swebench</code> runs on all task instances in batch mode.</li> <li><code>mini-extra swebench-single</code> runs on a single task instance with interactivity (useful for debugging).</li> <li>You can also take a look at the runscripts to figure out how to build your own batch processing pipeline.</li> </ul>"},{"location":"usage/swebench/#usage","title":"Usage","text":"<p>Docker container availability</p> <p>The docker containers for Linux assume an x86 Linux architecture; you might not be able to run them on other architectures.</p> <p>Quickstart</p> <p>We provide two different scripts: <code>swebench</code> and <code>swebench-single</code>:</p> Batch modeSingle instance (for debugging) <p>Batch mode runs on all task instances in parallel.</p> <pre><code>mini-extra swebench --help\n# or\npython src/minisweagent/run/extra/swebench.py --help\n# Example:\nmini-extra swebench \\\n    --model anthropic/claude-sonnet-4-5-20250929 \\\n    --subset verified \\\n    --split test \\\n    --workers 4\n</code></pre> <p>Basic flags:</p> <ul> <li><code>-o</code>, <code>--output</code> - Output directory</li> <li><code>-m</code>, <code>--model</code> - Model to use</li> <li><code>-c</code>, <code>--config</code> - Path to a config file (default: <code>swebench.yaml</code> in the <code>config</code> directory)</li> <li><code>-w</code>, <code>--workers</code> - Number of worker threads for parallel processing (default: <code>1</code>)</li> </ul> <p>Data selection flags:</p> <ul> <li><code>--subset</code> - SWEBench subset to use or path to a dataset (default: <code>lite</code>)</li> <li><code>--split</code> - Dataset split (default: <code>dev</code>)</li> <li><code>--slice</code> - Slice specification (e.g., '0:5' for first 5 instances)</li> <li><code>--filter</code> - Filter instance IDs by regex</li> <li><code>--shuffle</code> - Shuffle instances (default: <code>False</code>)</li> <li><code>--redo-existing</code> - Redo existing instances (default: <code>False</code>)</li> </ul> <p>Advanced flags:</p> <ul> <li><code>--environment-class</code> - Environment type to use (recommended: <code>docker</code> or <code>singularity</code>)</li> </ul> <p>Single instance mode runs on a single task instance with interactivity (useful for debugging).</p> <pre><code>mini-extra swebench-single --help\n# or\npython src/minisweagent/run/extra/swebench_single.py --help\n# Example:\nmini-extra swebench-single \\\n    --subset verified \\\n    --split test \\\n    --model anthropic/claude-sonnet-4-5-20250929 \\\n    -i sympy__sympy-15599\n# or\nmini-extra swebench-single \\\n    --subset verified \\\n    --split test \\\n    -m anthropic/claude-sonnet-4-5-20250929 \\\n    -i 0  # instance index\n</code></pre> <p>Note: If you want to run the script without prompting for confirmation at exit, add the <code>--exit-immediately</code> flag.</p> <p>Basic flags:</p> <ul> <li><code>-m</code>, <code>--model</code> - Model to use</li> <li><code>-c</code>, <code>--config</code> - Path to a config file (default: <code>swebench.yaml</code> in the <code>config</code> directory)</li> <li><code>-o</code>, <code>--output</code> - Output trajectory file (default: saves to global config directory)</li> </ul> <p>Data selection flags:</p> <ul> <li><code>--subset</code> - SWEBench subset to use or path to a dataset (default: <code>lite</code>)</li> <li><code>--split</code> - Dataset split (default: <code>dev</code>)</li> <li><code>-i</code>, <code>--instance</code> - SWE-Bench instance ID (default: <code>0</code>)</li> </ul> <p>Advanced flags:</p> <ul> <li><code>--environment-class</code> - Environment type to use (recommended: <code>docker</code> or <code>singularity</code>)</li> <li><code>--exit-immediately</code> - Exit immediately when the agent wants to finish instead of prompting (default: <code>False</code>)</li> </ul> <p>Evaluating on SWE-bench</p> <p>You have two options to evaluate on SWE-bench: Our free cloud-based evaluation or the SWE-bench CLI.</p> Cloud-based evaluationLocal evaluation <p>You can use the sb-cli for extremely fast, cloud-based evaluations (and it's free!). After installing it and getting a token, simply run:</p> <pre><code>sb-cli submit swe-bench_verified test --predictions_path preds.json --run_id some-id-for-your-run\n</code></pre> <p>Typically you will have results within 20 minutes (this is not limited by how many instances you run, but by the slowest-to-evaluate instance in SWE-bench).</p> <p>You can also use a local installation of SWE-bench for evaluation:</p> <pre><code>python -m swebench.harness.run_evaluation \\\n    --dataset_name princeton-nlp/SWE-bench_Verified \\\n    --predictions_path all_preds.jsonl \\\n    --max_workers &lt;num_workers&gt; \\\n    --run_id &lt;run_id&gt;\n</code></pre>"},{"location":"usage/swebench/#faq","title":"FAQ","text":"<p>Can I set global cost limits?</p> <p>Yes, you can set global cost limits with the <code>MSWEA_GLOBAL_CALL_LIMIT</code> and <code>MSWEA_GLOBAL_COST_LIMIT</code> environment variables/global config. See global configuration for more details.</p> <p>What happens to uncompleted tasks when I abort with KeyboardInterrupt?</p> <p>Trajectories are only saved upon completion, so most likely, you can just rerun the script to complete the tasks next time. However, you should still check for <code>KeyboardInterrupt</code> in <code>preds.json</code> in case some tasks were aborted but saved.</p> <p>Certain tasks are being stuck even though I deleted the trajectories.</p> <p>The completed instances are inferred from <code>preds.json</code>. Remove the corresponding items from the file.</p> <p>How can I run on a different dataset?</p> <p>As long as it follows the SWE-bench format, you can use <code>--subset /path/to/your/dataset</code> to run on a custom dataset. The dataset needs to be loadable as <code>datasets.load_dataset(path, split=split)</code>.</p> <p>Some progress runners are stuck at 'initializing task' for a very long time / time out</p> <p>They might be pulling docker containers -- the run should start immediately the next time. If you see timeouts because of <code>docker pull</code> operations, you might want to increase <code>environment.pull_timeout</code> from the default of <code>120</code> (seconds).</p> <p>I have some docker issues</p> <p>Try running the docker command manually to see what's going on (it should be printed out in the console). Confirm that it's running with <code>docker ps</code>, and that you can use <code>docker exec -it &lt;container-id&gt; ls</code> to get some output.</p> <p>Docker isn't available on my HPC cluster.</p> <p>You can use the singularity/apptainer backend by setting <code>environment.environment_class</code> to <code>singularity</code> in your agent config file or specify <code>--environment-class singularity</code> from the command line</p> <p>Can I run a startup command in the environment?</p> <p>Yes, you can use the <code>run.env_startup_command</code> config option to run a command in the environment before the agent starts. For example:</p> <pre><code>run:\n  env_startup_command: \"apt-get update &amp;&amp; apt-get install -y python3-pip\"\n</code></pre> <p>The command is rendered with the instance variables as template variables using <code>jinja2</code>. For example, you could use</p> <pre><code>run:\n  env_startup_command: \"git clone {{ repo_url }} . --force\"\n</code></pre> <p>which might be particularly useful when running with environments like <code>bubblewrap</code>.</p> <p>What environment can I use for SWE-bench?</p> <p>See this guide for more details.</p>"},{"location":"usage/swebench/#implementation","title":"Implementation","text":"Default config <ul> <li>Read on GitHub</li> </ul> <pre><code>agent:\n  system_template: |\n    You are a helpful assistant that can interact multiple times with a computer shell to solve programming tasks.\n    Your response must contain exactly ONE bash code block with ONE command (or commands connected with &amp;&amp; or ||).\n\n    Include a THOUGHT section before your command where you explain your reasoning process.\n    Format your response as shown in &lt;format_example&gt;.\n\n    &lt;format_example&gt;\n    THOUGHT: Your reasoning and analysis here\n\n    ```bash\n    your_command_here\n    ```\n    &lt;/format_example&gt;\n\n    Failure to follow these rules will cause your response to be rejected.\n  instance_template: |\n    &lt;pr_description&gt;\n    Consider the following PR description:\n    {{task}}\n    &lt;/pr_description&gt;\n\n    &lt;instructions&gt;\n    # Task Instructions\n\n    ## Overview\n    You're a software engineer interacting continuously with a computer by submitting commands.\n    You'll be helping implement necessary changes to meet requirements in the PR description.\n    Your task is specifically to make changes to non-test files in the current directory in order to fix the issue described in the PR description in a way that is general and consistent with the codebase.\n\n    IMPORTANT: This is an interactive process where you will think and issue ONE command, see its result, then think and issue your next command.\n\n    For each response:\n    1. Include a THOUGHT section explaining your reasoning and what you're trying to accomplish\n    2. Provide exactly ONE bash command to execute\n\n    ## Important Boundaries\n    - MODIFY: Regular source code files in /testbed (this is the working directory for all your subsequent commands)\n    - DO NOT MODIFY: Tests, configuration files (pyproject.toml, setup.cfg, etc.)\n\n    ## Recommended Workflow\n    1. Analyze the codebase by finding and reading relevant files\n    2. Create a script to reproduce the issue\n    3. Edit the source code to resolve the issue\n    4. Verify your fix works by running your script again\n    5. Test edge cases to ensure your fix is robust\n\n    ## Command Execution Rules\n    You are operating in an environment where\n    1. You write a single command\n    2. The system executes that command in a subshell\n    3. You see the result\n    4. You write your next command\n\n    Each response should include:\n    1. A **THOUGHT** section where you explain your reasoning and plan\n    2. A single bash code block with your command\n\n    Format your responses like this:\n\n    &lt;format_example&gt;\n    THOUGHT: Here I explain my reasoning process, analysis of the current situation,\n    and what I'm trying to accomplish with the command below.\n\n    ```bash\n    your_command_here\n    ```\n    &lt;/format_example&gt;\n\n    Commands must be specified in a single bash code block:\n\n    ```bash\n    your_command_here\n    ```\n\n    **CRITICAL REQUIREMENTS:**\n    - Your response SHOULD include a THOUGHT section explaining your reasoning\n    - Your response MUST include EXACTLY ONE bash code block\n    - This bash block MUST contain EXACTLY ONE command (or a set of commands connected with &amp;&amp; or ||)\n    - If you include zero or multiple bash blocks, or no command at all, YOUR RESPONSE WILL FAIL\n    - Do NOT try to run multiple independent commands in separate blocks in one response\n    - Directory or environment variable changes are not persistent. Every action is executed in a new subshell.\n    - However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir &amp;&amp; ...` or write/load environment variables from files\n\n    Example of a CORRECT response:\n    &lt;example_response&gt;\n    THOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.\n\n    ```bash\n    ls -la\n    ```\n    &lt;/example_response&gt;\n\n    Example of an INCORRECT response:\n    &lt;example_response&gt;\n    THOUGHT: I need to examine the codebase and then look at a specific file. I'll run multiple commands to do this.\n\n    ```bash\n    ls -la\n    ```\n\n    Now I'll read the file:\n\n    ```bash\n    cat file.txt\n    ```\n    &lt;/example_response&gt;\n\n    If you need to run multiple commands, either:\n    1. Combine them in one block using &amp;&amp; or ||\n    ```bash\n    command1 &amp;&amp; command2 || echo \"Error occurred\"\n    ```\n\n    2. Wait for the first command to complete, see its output, then issue the next command in your following response.\n\n    ## Environment Details\n    - You have a full Linux shell environment\n    - Always use non-interactive flags (-y, -f) for commands\n    - Avoid interactive tools like vi, nano, or any that require user input\n    - If a command isn't available, you can install it\n\n    ## Useful Command Examples\n\n    ### Create a new file:\n    ```bash\n    cat &lt;&lt;'EOF' &gt; newfile.py\n    import numpy as np\n    hello = \"world\"\n    print(hello)\n    EOF\n    ```\n\n    ### Edit files with sed:\n    ```bash\n    # Replace all occurrences\n    sed -i 's/old_string/new_string/g' filename.py\n\n    # Replace only first occurrence\n    sed -i 's/old_string/new_string/' filename.py\n\n    # Replace first occurrence on line 1\n    sed -i '1s/old_string/new_string/' filename.py\n\n    # Replace all occurrences in lines 1-10\n    sed -i '1,10s/old_string/new_string/g' filename.py\n    ```\n\n    ### View file content:\n    ```bash\n    # View specific lines with numbers\n    nl -ba filename.py | sed -n '10,20p'\n    ```\n\n    ### Any other command you want to run\n    ```bash\n    anything\n    ```\n\n    ## Submission\n    When you've completed your work (reading, editing, testing), and cannot make further progress\n    issue exactly the following command:\n\n    ```bash\n    echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT &amp;&amp; git add -A &amp;&amp; git diff --cached\n    ```\n\n    This command will submit your work.\n    You cannot continue working (reading, editing, testing) in any way on this task after submitting.\n    &lt;/instructions&gt;\n  action_observation_template: |\n    &lt;returncode&gt;{{output.returncode}}&lt;/returncode&gt;\n    {% if output.output | length &lt; 10000 -%}\n    &lt;output&gt;\n    {{ output.output -}}\n    &lt;/output&gt;\n    {%- else -%}\n    &lt;warning&gt;\n    The output of your last command was too long.\n    Please try a different command that produces less output.\n    If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.\n    If you're using grep or find and it produced too much output, you can use a more selective search pattern.\n    If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.\n    &lt;/warning&gt;\n    {%- set elided_chars = output.output | length - 10000 -%}\n    &lt;output_head&gt;\n    {{ output.output[:5000] }}\n    &lt;/output_head&gt;\n    &lt;elided_chars&gt;\n    {{ elided_chars }} characters elided\n    &lt;/elided_chars&gt;\n    &lt;output_tail&gt;\n    {{ output.output[-5000:] }}\n    &lt;/output_tail&gt;\n    {%- endif -%}\n  format_error_template: |\n    Please always provide EXACTLY ONE action in triple backticks, found {{actions|length}} actions.\n\n    Please format your action in triple backticks as shown in &lt;response_example&gt;.\n\n    &lt;response_example&gt;\n    Here are some thoughts about why you want to perform the action.\n\n    ```bash\n    &lt;action&gt;\n    ```\n    &lt;/response_example&gt;\n\n    If you have completed your assignment, please consult the first message about how to\n    submit your solution (you will not be able to continue working on this task after that).\n  step_limit: 250\n  cost_limit: 3.\n\nenvironment:\n  cwd: \"/testbed\"\n  timeout: 60\n  env:\n    PAGER: cat\n    MANPAGER: cat\n    LESS: -R\n    PIP_PROGRESS_BAR: 'off'\n    TQDM_DISABLE: '1'\n  environment_class: docker\n\nmodel:\n  model_name: \"anthropic/claude-sonnet-4-5-20250929\"\n  model_kwargs:\n    drop_params: true\n    temperature: 0.0\n</code></pre> <code>swebench.py</code> run script <ul> <li>Read on GitHub</li> <li>API reference</li> </ul> <pre><code>#!/usr/bin/env python3\n\n\"\"\"Run mini-SWE-agent on SWE-bench instances in batch mode.\"\"\"\n# Read this first: https://mini-swe-agent.com/latest/usage/swebench/  (usage docs)\n\nimport concurrent.futures\nimport json\nimport random\nimport re\nimport threading\nimport time\nimport traceback\nfrom pathlib import Path\n\nimport typer\nimport yaml\nfrom datasets import load_dataset\nfrom jinja2 import StrictUndefined, Template\nfrom rich.live import Live\n\nfrom minisweagent import Environment\nfrom minisweagent.agents.default import DefaultAgent\nfrom minisweagent.config import builtin_config_dir, get_config_path\nfrom minisweagent.environments import get_environment\nfrom minisweagent.models import get_model\nfrom minisweagent.run.extra.utils.batch_progress import RunBatchProgressManager\nfrom minisweagent.run.utils.save import save_traj\nfrom minisweagent.utils.log import add_file_handler, logger\n\n_HELP_TEXT = \"\"\"Run mini-SWE-agent on SWEBench instances.\n\n[not dim]\nMore information about the usage: [bold green]https://mini-swe-agent.com/latest/usage/swebench/[/bold green]\n[/not dim]\n\"\"\"\n\napp = typer.Typer(rich_markup_mode=\"rich\", add_completion=False)\n\nDATASET_MAPPING = {\n    \"full\": \"princeton-nlp/SWE-Bench\",\n    \"verified\": \"princeton-nlp/SWE-Bench_Verified\",\n    \"lite\": \"princeton-nlp/SWE-Bench_Lite\",\n    \"multimodal\": \"princeton-nlp/SWE-Bench_Multimodal\",\n    \"multilingual\": \"swe-bench/SWE-Bench_Multilingual\",\n    \"smith\": \"SWE-bench/SWE-smith\",\n    \"_test\": \"klieret/swe-bench-dummy-test-dataset\",\n}\n\n\n_OUTPUT_FILE_LOCK = threading.Lock()\n\n\nclass ProgressTrackingAgent(DefaultAgent):\n    \"\"\"Simple wrapper around DefaultAgent that provides progress updates.\"\"\"\n\n    def __init__(self, *args, progress_manager: RunBatchProgressManager, instance_id: str = \"\", **kwargs):\n        super().__init__(*args, **kwargs)\n        self.progress_manager: RunBatchProgressManager = progress_manager\n        self.instance_id = instance_id\n\n    def step(self) -&gt; dict:\n        \"\"\"Override step to provide progress updates.\"\"\"\n        self.progress_manager.update_instance_status(\n            self.instance_id, f\"Step {self.model.n_calls + 1:3d} (${self.model.cost:.2f})\"\n        )\n        return super().step()\n\n\ndef get_swebench_docker_image_name(instance: dict) -&gt; str:\n    \"\"\"Get the image name for a SWEBench instance.\"\"\"\n    image_name = instance.get(\"image_name\", None)\n    if image_name is None:\n        # Docker doesn't allow double underscore, so we replace them with a magic token\n        iid = instance[\"instance_id\"]\n        id_docker_compatible = iid.replace(\"__\", \"_1776_\")\n        image_name = f\"docker.io/swebench/sweb.eval.x86_64.{id_docker_compatible}:latest\".lower()\n    return image_name\n\n\ndef get_sb_environment(config: dict, instance: dict) -&gt; Environment:\n    env_config = config.setdefault(\"environment\", {})\n    env_config[\"environment_class\"] = env_config.get(\"environment_class\", \"docker\")\n    image_name = get_swebench_docker_image_name(instance)\n    if env_config[\"environment_class\"] == \"docker\":\n        env_config[\"image\"] = image_name\n    elif env_config[\"environment_class\"] == \"singularity\":\n        env_config[\"image\"] = \"docker://\" + image_name\n    env = get_environment(env_config)\n    if startup_command := config.get(\"run\", {}).get(\"env_startup_command\"):\n        startup_command = Template(startup_command, undefined=StrictUndefined).render(**instance)\n        out = env.execute(startup_command)\n        if out[\"returncode\"] != 0:\n            raise RuntimeError(f\"Error executing startup command: {out}\")\n    return env\n\n\ndef update_preds_file(output_path: Path, instance_id: str, model_name: str, result: str):\n    \"\"\"Update the output JSON file with results from a single instance.\"\"\"\n    with _OUTPUT_FILE_LOCK:\n        output_data = {}\n        if output_path.exists():\n            output_data = json.loads(output_path.read_text())\n        output_data[instance_id] = {\n            \"model_name_or_path\": model_name,\n            \"instance_id\": instance_id,\n            \"model_patch\": result,\n        }\n        output_path.write_text(json.dumps(output_data, indent=2))\n\n\ndef remove_from_preds_file(output_path: Path, instance_id: str):\n    \"\"\"Remove an instance from the predictions file.\"\"\"\n    if not output_path.exists():\n        return\n    with _OUTPUT_FILE_LOCK:\n        output_data = json.loads(output_path.read_text())\n        if instance_id in output_data:\n            del output_data[instance_id]\n            output_path.write_text(json.dumps(output_data, indent=2))\n\n\ndef process_instance(\n    instance: dict,\n    output_dir: Path,\n    config: dict,\n    progress_manager: RunBatchProgressManager,\n) -&gt; None:\n    \"\"\"Process a single SWEBench instance.\"\"\"\n    instance_id = instance[\"instance_id\"]\n    instance_dir = output_dir / instance_id\n    # avoid inconsistent state if something here fails and there's leftover previous files\n    remove_from_preds_file(output_dir / \"preds.json\", instance_id)\n    (instance_dir / f\"{instance_id}.traj.json\").unlink(missing_ok=True)\n    model = get_model(config=config.get(\"model\", {}))\n    task = instance[\"problem_statement\"]\n\n    progress_manager.on_instance_start(instance_id)\n    progress_manager.update_instance_status(instance_id, \"Pulling/starting docker\")\n\n    agent = None\n    extra_info = None\n\n    try:\n        env = get_sb_environment(config, instance)\n        agent = ProgressTrackingAgent(\n            model,\n            env,\n            progress_manager=progress_manager,\n            instance_id=instance_id,\n            **config.get(\"agent\", {}),\n        )\n        exit_status, result = agent.run(task)\n    except Exception as e:\n        logger.error(f\"Error processing instance {instance_id}: {e}\", exc_info=True)\n        exit_status, result = type(e).__name__, str(e)\n        extra_info = {\"traceback\": traceback.format_exc()}\n    finally:\n        save_traj(\n            agent,\n            instance_dir / f\"{instance_id}.traj.json\",\n            exit_status=exit_status,\n            result=result,\n            extra_info=extra_info,\n            instance_id=instance_id,\n            print_fct=logger.info,\n        )\n        update_preds_file(output_dir / \"preds.json\", instance_id, model.config.model_name, result)\n        progress_manager.on_instance_end(instance_id, exit_status)\n\n\ndef filter_instances(\n    instances: list[dict], *, filter_spec: str, slice_spec: str = \"\", shuffle: bool = False\n) -&gt; list[dict]:\n    \"\"\"Filter and slice a list of SWEBench instances.\"\"\"\n    if shuffle:\n        instances = sorted(instances.copy(), key=lambda x: x[\"instance_id\"])\n        random.seed(42)\n        random.shuffle(instances)\n    before_filter = len(instances)\n    instances = [instance for instance in instances if re.match(filter_spec, instance[\"instance_id\"])]\n    if (after_filter := len(instances)) != before_filter:\n        logger.info(f\"Instance filter: {before_filter} -&gt; {after_filter} instances\")\n    if slice_spec:\n        values = [int(x) if x else None for x in slice_spec.split(\":\")]\n        instances = instances[slice(*values)]\n        if (after_slice := len(instances)) != before_filter:\n            logger.info(f\"Instance slice: {before_filter} -&gt; {after_slice} instances\")\n    return instances\n\n\n# fmt: off\n@app.command(help=_HELP_TEXT)\ndef main(\n    subset: str = typer.Option(\"lite\", \"--subset\", help=\"SWEBench subset to use or path to a dataset\", rich_help_panel=\"Data selection\"),\n    split: str = typer.Option(\"dev\", \"--split\", help=\"Dataset split\", rich_help_panel=\"Data selection\"),\n    slice_spec: str = typer.Option(\"\", \"--slice\", help=\"Slice specification (e.g., '0:5' for first 5 instances)\", rich_help_panel=\"Data selection\"),\n    filter_spec: str = typer.Option(\"\", \"--filter\", help=\"Filter instance IDs by regex\", rich_help_panel=\"Data selection\"),\n    shuffle: bool = typer.Option(False, \"--shuffle\", help=\"Shuffle instances\", rich_help_panel=\"Data selection\"),\n    output: str = typer.Option(\"\", \"-o\", \"--output\", help=\"Output directory\", rich_help_panel=\"Basic\"),\n    workers: int = typer.Option(1, \"-w\", \"--workers\", help=\"Number of worker threads for parallel processing\", rich_help_panel=\"Basic\"),\n    model: str | None = typer.Option(None, \"-m\", \"--model\", help=\"Model to use\", rich_help_panel=\"Basic\"),\n    model_class: str | None = typer.Option(None, \"-c\", \"--model-class\", help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\", rich_help_panel=\"Advanced\"),\n    redo_existing: bool = typer.Option(False, \"--redo-existing\", help=\"Redo existing instances\", rich_help_panel=\"Data selection\"),\n    config_spec: Path = typer.Option( builtin_config_dir / \"extra\" / \"swebench.yaml\", \"-c\", \"--config\", help=\"Path to a config file\", rich_help_panel=\"Basic\"),\n    environment_class: str | None = typer.Option( None, \"--environment-class\", help=\"Environment type to use. Recommended are docker or singularity\", rich_help_panel=\"Advanced\"),\n) -&gt; None:\n    # fmt: on\n    output_path = Path(output)\n    output_path.mkdir(parents=True, exist_ok=True)\n    logger.info(f\"Results will be saved to {output_path}\")\n    add_file_handler(output_path / \"minisweagent.log\")\n\n    dataset_path = DATASET_MAPPING.get(subset, subset)\n    logger.info(f\"Loading dataset {dataset_path}, split {split}...\")\n    instances = list(load_dataset(dataset_path, split=split))\n\n    instances = filter_instances(instances, filter_spec=filter_spec, slice_spec=slice_spec, shuffle=shuffle)\n    if not redo_existing and (output_path / \"preds.json\").exists():\n        existing_instances = list(json.loads((output_path / \"preds.json\").read_text()).keys())\n        logger.info(f\"Skipping {len(existing_instances)} existing instances\")\n        instances = [instance for instance in instances if instance[\"instance_id\"] not in existing_instances]\n    logger.info(f\"Running on {len(instances)} instances...\")\n\n    config_path = get_config_path(config_spec)\n    logger.info(f\"Loading agent config from '{config_path}'\")\n    config = yaml.safe_load(config_path.read_text())\n    if environment_class is not None:\n        config.setdefault(\"environment\", {})[\"environment_class\"] = environment_class\n    if model is not None:\n        config.setdefault(\"model\", {})[\"model_name\"] = model\n    if model_class is not None:\n        config.setdefault(\"model\", {})[\"model_class\"] = model_class\n\n    progress_manager = RunBatchProgressManager(len(instances), output_path / f\"exit_statuses_{time.time()}.yaml\")\n\n    def process_futures(futures: dict[concurrent.futures.Future, str]):\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                future.result()\n            except concurrent.futures.CancelledError:\n                pass\n            except Exception as e:\n                instance_id = futures[future]\n                logger.error(f\"Error in future for instance {instance_id}: {e}\", exc_info=True)\n                progress_manager.on_uncaught_exception(instance_id, e)\n\n    with Live(progress_manager.render_group, refresh_per_second=4):\n        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n            futures = {\n                executor.submit(process_instance, instance, output_path, config, progress_manager): instance[\n                    \"instance_id\"\n                ]\n                for instance in instances\n            }\n            try:\n                process_futures(futures)\n            except KeyboardInterrupt:\n                logger.info(\"Cancelling all pending jobs. Press ^C again to exit immediately.\")\n                for future in futures:\n                    if not future.running() and not future.done():\n                        future.cancel()\n                process_futures(futures)\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <code>swebench_single.py</code> run script <ul> <li>Read on GitHub</li> <li>API reference</li> </ul> <pre><code>\"\"\"Run on a single SWE-Bench instance.\"\"\"\n\nimport traceback\nfrom pathlib import Path\n\nimport typer\nimport yaml\nfrom datasets import load_dataset\n\nfrom minisweagent import global_config_dir\nfrom minisweagent.agents.interactive import InteractiveAgent\nfrom minisweagent.config import builtin_config_dir, get_config_path\nfrom minisweagent.models import get_model\nfrom minisweagent.run.extra.swebench import (\n    DATASET_MAPPING,\n    get_sb_environment,\n)\nfrom minisweagent.run.utils.save import save_traj\nfrom minisweagent.utils.log import logger\n\napp = typer.Typer(add_completion=False)\n\nDEFAULT_OUTPUT = global_config_dir / \"last_swebench_single_run.traj.json\"\n\n\n# fmt: off\n@app.command()\ndef main(\n    subset: str = typer.Option(\"lite\", \"--subset\", help=\"SWEBench subset to use or path to a dataset\", rich_help_panel=\"Data selection\"),\n    split: str = typer.Option(\"dev\", \"--split\", help=\"Dataset split\", rich_help_panel=\"Data selection\"),\n    instance_spec: str = typer.Option(0, \"-i\", \"--instance\", help=\"SWE-Bench instance ID or index\", rich_help_panel=\"Data selection\"),\n    model_name: str | None = typer.Option(None, \"-m\", \"--model\", help=\"Model to use\", rich_help_panel=\"Basic\"),\n    model_class: str | None = typer.Option(None, \"-c\", \"--model-class\", help=\"Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')\", rich_help_panel=\"Advanced\"),\n    config_path: Path = typer.Option( builtin_config_dir / \"extra\" / \"swebench.yaml\", \"-c\", \"--config\", help=\"Path to a config file\", rich_help_panel=\"Basic\"),\n    environment_class: str | None = typer.Option(None, \"--environment-class\", rich_help_panel=\"Advanced\"),\n    exit_immediately: bool = typer.Option( False, \"--exit-immediately\", help=\"Exit immediately when the agent wants to finish instead of prompting.\", rich_help_panel=\"Basic\"),\n    output: Path = typer.Option(DEFAULT_OUTPUT, \"-o\", \"--output\", help=\"Output trajectory file\", rich_help_panel=\"Basic\"),\n) -&gt; None:\n    # fmt: on\n    \"\"\"Run on a single SWE-Bench instance.\"\"\"\n    dataset_path = DATASET_MAPPING.get(subset, subset)\n    logger.info(f\"Loading dataset from {dataset_path}, split {split}...\")\n    instances = {\n        inst[\"instance_id\"]: inst  # type: ignore\n        for inst in load_dataset(dataset_path, split=split)\n    }\n    if instance_spec.isnumeric():\n        instance_spec = sorted(instances.keys())[int(instance_spec)]\n    instance: dict = instances[instance_spec]  # type: ignore\n\n    config_path = get_config_path(config_path)\n    logger.info(f\"Loading agent config from '{config_path}'\")\n    config = yaml.safe_load(config_path.read_text())\n    if environment_class is not None:\n        config.setdefault(\"environment\", {})[\"environment_class\"] = environment_class\n    if model_class is not None:\n        config.setdefault(\"model\", {})[\"model_class\"] = model_class\n    if exit_immediately:\n        config.setdefault(\"agent\", {})[\"confirm_exit\"] = False\n    env = get_sb_environment(config, instance)\n    agent = InteractiveAgent(\n        get_model(model_name, config.get(\"model\", {})),\n        env,\n        **({\"mode\": \"yolo\"} | config.get(\"agent\", {})),\n    )\n\n    exit_status, result, extra_info = None, None, None\n    try:\n        exit_status, result = agent.run(instance[\"problem_statement\"])  # type: ignore[arg-type]\n    except Exception as e:\n        logger.error(f\"Error processing instance {instance_spec}: {e}\", exc_info=True)\n        exit_status, result = type(e).__name__, str(e)\n        extra_info = {\"traceback\": traceback.format_exc()}\n    finally:\n        save_traj(agent, output, exit_status=exit_status, result=result, extra_info=extra_info)  # type: ignore[arg-type]\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> bug_report Something broken/unclear? <p>Open an issue on GitHub!</p> help Open-ended discussions <p>Join our Slack!</p>"}]}